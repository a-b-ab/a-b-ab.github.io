[{"content":"前言 最近参与工作室开源项目的开发，需要自己Fork公司项目到自己的项目然后发起PR(Pull Request)，这其中让我对Git管理有了更深的理解\nFork项目的自己的Github仓库 点击Code复制仓库的URL 我最开始是复制HTTP类型的，不过可能因为Github的原因，我一直push不上去，后面找了资料，利用SSH解决的，所以我们复制SSH类型\nGit全局配置 查看自己的git配置\ngit config -l\n配置自己的邮箱\ngit config \u0026ndash;global user.name \u0026ldquo;username\u0026rdquo; git config \u0026ndash;global user.email \u0026ldquo;email\u0026rdquo;\n记住你的用户名和邮箱\n生成ssh私钥和公钥 ssh-keygen -t rsa -C \u0026ldquo;email\u0026rdquo;\n直接两个enter，一个yes覆盖，不设置密码\n配置密钥 去生成的id.rsa.pub文件复制密码，放到gitub项目\n使用ssh拉去代码 修改本地项目连接方式为ssh（针对之前用http连接拉取的）\ngit remote set-url origin git@github.com:xxxx/xxxx.git\ngit clone ssh项目地址 git init git add . git commit -m \u0026ldquo;123\u0026rdquo; git push\n这样就可以push上去了\n发起PR 这个在github页面操作很简单 点击上方页面的Pull Request，设置好目标url和你的url就可以 ","date":"2025-01-21T00:00:00Z","image":"https://a-b-ab.github.io/b2df4789359238a9c06909a4b06c2c45.jpg","permalink":"https://a-b-ab.github.io/p/%E5%8F%91%E8%B5%B7%E4%B8%80%E4%B8%AApr/","title":"发起一个PR"},{"content":"变量与注释 变量解包 允许我们把一个可迭代对象的所有成员，一次性赋值给多个变量\n假如在赋值语句左侧添加⼩括号 (\u0026hellip;)，甚⾄可以⼀次展开多层嵌套数据\n1 2 3 4 5 6 7 8 attrs = [1,[\u0026#39;piglei\u0026#39;,100]] user_id,(username,score) = attrs \u0026gt;\u0026gt; user_id 1 \u0026gt;\u0026gt; username \u0026#39;piglei\u0026#39; 除了上⾯的普通解包外，Python 还⽀持更灵活的动态解包语法。只要用星号表达式(*variables)作为变量名,她会贪婪地捕获多个值对象，并将捕获到的内容作为列表赋值给variables\n⽐如，下⾯ data 列表⾥的数据就分为三段：头为⽤⼾，尾为分数，中间的都是⽔果名称。通 过把 *fruits 设置为中间的解包变量，我们就能⼀次性解包所有变量——fruits 会捕获 data 去头去尾后的所有成员\n1 2 3 4 5 6 7 8 9 data = [\u0026#39;piglei\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;banana\u0026#39;, 100] username,*fruits,score = data \u0026gt;\u0026gt;username \u0026#39;piglei\u0026#39; \u0026gt;\u0026gt;fruits [\u0026#39;apple\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;banana\u0026#39;] \u0026gt;\u0026gt;score 100 和常规的切⽚赋值语句⽐起来，动态解包语法要直观许多\n1 2 3 4 5 # 动态解包 \u0026gt;\u0026gt; username,*fruits,score = data # 切片赋值 \u0026gt;\u0026gt; username,fruits,score = data[0],data[1:-1],data[-1] # 两种完全等价 上⾯的变量解包操作也可以在任何循环语句⾥使⽤\n1 2 3 4 for username,score in [(\u0026#39;piglei\u0026#39;,100),(\u0026#39;raymonmd\u0026#39;,60)]: print(username) \u0026gt;\u0026gt;glei \u0026gt;\u0026gt;ymond 单下划线变量名 _ 在常⽤的诸多变量名中，单下划线 _ 是⽐较特殊的⼀个。它常作为⼀个⽆意义的占位符出现 在赋值语句中。_ 这个名字本⾝没什么特别之处，这算是⼤家约定俗成的⼀种⽤法。 举个例⼦，假如你想在解包赋值时忽略某些变量，就可以使⽤ _ 作为变量名：\n1 2 3 4 5 # 忽略展开时的第二个变量 author,_ = usernames # 忽略第一个和最后一个变量之间的所有变量 username,*_,score = data ⽽在 Python 交互式命令⾏（直接执⾏ python 命令进⼊的交互环境）⾥，_ 变量还有⼀ 层特殊含义——默认保存我们输⼊的上个表达式的返回值\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; \u0026#39;foo\u0026#39;.upper() \u0026#39;FOO\u0026#39; \u0026gt;\u0026gt;\u0026gt; print(_) FOO 此时的_变量保存着上一个.upper()表达式的结果\n下⾯是增加了 Python 官⽅推荐的 Sphinx 格式⽂档后的效果\n1 2 3 4 5 6 def remove_invalid(items): \u0026#34;\u0026#34;\u0026#34;剔除items里面无效的元素 :param items:待剔除对象 :type items:包含整数的列表,[int,...] \u0026#34;\u0026#34;\u0026#34; 当然，标注类型的办法肯定不⽌上⾯这⼀种。在 Python 3.5 版本 以后，你可以⽤类型注 解功能来直接注明变量类型。相⽐编写 Sphinx 格式⽂档，我其实更推荐使⽤类型注解，因为它是 Python 的内置功能，⽽且正在变得越来越流⾏\n1 2 3 4 from typing import List def remove_invalid(items:List[int]): \u0026#34;\u0026#34;\u0026#34;剔除items里无效的元素\u0026#34;\u0026#34;\u0026#34; List 表⽰参数为列表类型，[int] 表⽰⾥⾯的成员是整型\n对于普通变量，使⽤蛇形命名法，⽐如 max_value； 对于常量，采⽤全⼤写字⺟，使⽤下划线连接，⽐如 MAX_VALUE； 如果变量标记为“仅内部使⽤”，为其增加下划线前缀，⽐如 _local_var； 当名字与 Python 关键字冲突时，在变量末尾追加下划线，⽐如 class_ 布尔值（bool）是⼀种很简单的类型，它只有两个可能的值：“是”（True）或“不是” （False）。因此，给布尔值变量起名有⼀个原则：⼀定要让读到变量的⼈觉得它只有 “肯定”和“否定”两种可能。举例来说，is、has 这些⾮⿊即⽩的词就很适合⽤来修饰这 类名字。\n最好别拿⼀个名词的复数形式来作为 int 类型的变量名，⽐如 apples、 trips 等，因为这类名字容易与那些装着 Apple 和 Trip 的普通容器对象 （List[Apple]、List[Trip]）混淆，建议⽤ number_of_apples 或 trips_count 这类复合词来作为 int 类型的名字。\n超短命名 数组索引三剑客 i,j,k 某个整数 n 某个字符串 s 文件对象 fp 另⼀类短名字，则是对⼀些其他常⽤名的缩写。⽐如，在使⽤ Django 框架做国际化内容翻 译时，常常会⽤到 gettext ⽅法。为了⽅便，我们常把 gettext 缩写成 _：\n1 2 from django.utils.translation import gettext as _ print(_(\u0026#39;待翻译文字\u0026#39;)) 除使⽤ # 的注释外，另⼀种注释则是我们前⾯看到过的函数（类）⽂档（docstring），这些 ⽂档也称接⼝注释（interface comment）。\n1 2 3 4 5 6 7 class Person: \u0026#34;\u0026#34;\u0026#34;人 :param name:姓名 :param age:年龄 :param favorite_color:最喜欢的颜色 \u0026#34;\u0026#34;\u0026#34; 注释作为代码之外的说明性⽂字，应该尽量提供那些读者⽆法从代码⾥读出来的信息。描述代 码为什么要这么做，⽽不是简单复述代码本⾝。\n除了描述“为什么”的解释性注释外，还有⼀种注释也很常⻅：指引性注释。这种注释并不直接 复述代码，⽽是简明扼要地概括代码功能，起到“代码导读”的作⽤。\n在编写指引性注释时，有⼀点需要注意，那就是你得判断何时该写注释，何时该将代码提炼为 独⽴的函数（或⽅法）。⽐如上⾯的代码，其实可以通过抽象两个新函数改成下⾯这样：\n在编写接⼝⽂档时，我们应该站在函数设计者的⻆度，着重描述函数的功能、参数说明等。⽽ 函数⾃⾝的实现细节，⽐如调⽤了哪个第三⽅模块、为何有性能问题等，⽆须放在接⼝⽂档 ⾥。\n1 2 3 4 5 6 7 8 9 def resize_image(image,size): \u0026#34;\u0026#34;\u0026#34;将图片缩放到指定尺寸，并返回新的图片 注意：当⽂件超过 5MB 时，请使⽤ resize_big_image( :param image:图片文件对象 :param size:包含宽高的元组:(width,height) :return:新图片对象 \u0026#34;\u0026#34;\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def magic_bubble_sort(numbers:List[int]): \u0026#34;\u0026#34;\u0026#34;有魔力的冒泡排序算法，默认所有的偶数都比奇数大 :param numbers:需要排序的列表，函数会直接修改原始列表 \u0026#34;\u0026#34;\u0026#34; stop_position = len(numbers) - 1 while stop_position \u0026gt; 0: for i in range(stop_position): cuurent,next_ = numbers[i],number[i+1] current_is_even,next_is_even = current % 2 == 0,next_ % 2 == 0 should_swap = False # 交换位置的两个条件 # - 前面是偶数，后面是奇数 # - 前面和后面同为奇数或者偶数，但是前面比后面大 if current_is_even and not next_is_even: should_swap = True elif current_is_even == next_is_even and current \u0026gt; next_: should_swap = True if should_swap: numbers[i],numbers[i+1] = numbers[i+1],numbers[i] stop_position -= 1 return numbers 保持变量的一致性 名字⼀致性是指在同⼀个项⽬（或者模块、函数）中，对⼀类事物的称呼不要变来变去。 类型⼀致性则是指不要把同⼀个变量重复指向不同类型的值 通过把变量定义移动到每段“各司其职”的代码头部，⼤⼤缩短了变量从初始化到被使⽤的“距 离”。当读者阅读代码时，可以更容易理解代码的逻辑，⽽不是来回翻阅代码，⼼想：“这个变量是什 么时候定义的？是⼲什么⽤的？”\n定义临时变量提升可读性 1 2 3 4 # 为所有性别为⼥或者级别⼤于 3 的活跃⽤⼾发放 10 000 个⾦币 if user.is_active and (user.sex == \u0026#39;female\u0026#39; or user.level \u0026gt; 3): user.add_coins(10000) return 逻辑虽然如此，不代表我们就得把代码直⽩地写成这样。如果把后⾯的复杂表达式赋值为⼀个临 时变量，代码可以变得更易读：\n1 2 3 4 5 6 # 为所有性别为⼥或者级别⼤于 3 的活跃⽤⼾发放 10 000 个⾦币 user_is_eligible = user.is_active and (user.sex == \u0026#39;female\u0026#39; or user.level \u0026gt; 3) if user_is_eligible: user.add_coins(10000) return 在新代码⾥，“计算⽤⼾合规的表达式”和“判断合规发送⾦币的条件分⽀”这两段代码不再直接 杂糅在⼀起，⽽是添加了⼀个可读性强的变量 user_is_elegible 作为缓冲。不论是代码的可读 性还是可维护性，都因为这个变量⽽增强了\n直接翻译业务逻辑的代码，⼤多不是好代码。优秀的程序设计需要在理解原需求的基础 上，恰到好处地抽象，只有这样才能同时满⾜可读性和可扩展性⽅⾯的需求。抽象有许多种⽅ 式，比如定义新函数，定义新类型，“定义⼀个临时变量”是诸多⽅式⾥不太起眼的⼀个，但⽤得 恰当的话效果也很巧妙。\n同一作用域不要有太多变量 对局部变量分组并建模 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class ImportedSummary: \u0026#34;\u0026#34;\u0026#34;保存导入结果摘要的数据类\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.succeeded_count = 0 self.failed_count = 0 class ImportingUserGroup: \u0026#34;\u0026#34;\u0026#34;用于暂存用户导入处理的数据类\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.duplicated = [] self.banned = [] self.normal = [] def import_users_from_file(fp): \u0026#34;\u0026#34;\u0026#34;尝试从文件对象读取用户，然后导入数据库 :param fp:可读文件对象 :return: 成功与失败的数量 \u0026#34;\u0026#34;\u0026#34; importing_user_group = ImportingUserGroup() for line in fp: parsed_user = parse_user(line) # …… 进⾏判断处理，修改上⾯定义的 importing_user_group 变量 summary = ImportedSummary() # …… 读取 importing_user_group，写⼊数据库并修改成功与失败的数量 return summary.succeeded_count,summary.failed_count 通过增加两个数据类，函数内的变量被更有逻辑地组织了起来，数量变少了许多。 需要说明的⼀点是，⼤多数情况下，只是执⾏上⾯这样的操作是远远不够的。函数内变量的数量 太多，通常意味着函数过于复杂，承担了太多职责。只有把复杂函数拆分为多个⼩函数，代码的整体 复杂度才可能实现根本性的降低\n能不定义变量就别定义 1 2 3 4 5 def get_best_trip_by_user_id(user_id): return { \u0026#39;user\u0026#39;: get_user(user_id), \u0026#39;trip\u0026#39;: get_best_trip(user_id) } 这样的代码就像删掉赘语的句⼦，变得更精练、更易读。所以，不必为了那些未来可能出现的变 动，牺牲代码此时此刻的可读性。如果以后需要定义变量，那就以后再做吧\n不要使用locals() locals() 是 Python 的⼀个内置函数，调⽤它会返回当前作⽤域中的所有局部变量：\n在有些场景下，我们需要⼀次性拿到当前作⽤域下的所有（或绝⼤部分）变量，⽐如在渲染 Django 模板时：\n1 2 3 4 5 6 7 8 9 10 def render_trip_page(request,user_id,trip_id): \u0026#34;\u0026#34;\u0026#34;渲染旅程页面\u0026#34;\u0026#34;\u0026#34; user = User.objects.get(id=user_id) trip = get_object_or_404(Trip,pk=trip_id) is_suggested = check_if_suggested(user,trip) return render(request,\u0026#39;trip.html\u0026#39;,{ \u0026#39;user\u0026#39;:user, \u0026#39;trip\u0026#39;:trip, \u0026#39;is_suggested\u0026#39;:is_suggested }) 看上去使⽤ locals() 函数正合适，假如调⽤ locals()，上⾯的代码会简化许多：\n1 2 3 4 def render_trip_page(request,user_id) # 利⽤ locals() 把当前所有变量作为模板渲染参数返回 # 节约了三⾏代码，我简直是个天才！ return render(request,\u0026#39;trip.html\u0026#39;,locals()) 第⼀眼看上去⾮常“简洁”，但是，这样的代码真的更好吗？ 答案并⾮如此。locals() 看似简洁，但其他⼈在阅读代码时，为了搞明⽩模板渲染到底⽤了 哪些变量，必须记住当前作⽤域⾥的所有变量。假如函数⾮常复杂，“记住所有局部变量”简直是个不 可能完成的任务\n使⽤ locals() 还有⼀个缺点，那就是它会把⼀些并没有真正使⽤的变量也⼀并暴露\nPython 之禅：显式优于隐式 “Python 之禅”中有⼀句“Explicit is better than implicit”（显式优于隐 式），这条原则完全可以套⽤到 locals() 的例⼦上——locals() 实在是太隐晦了，直接写 出变量名显然更好\n空行也是一种注释 在写代码时，我们可以适当地在代码中插⼊空⾏，把代码按不同的逻辑块分隔开，这样能有效提 升代码的可读性\n先写注释，后写代码 每个函数的名称与接⼝注释（也就是 docstring），其实是⼀种⽐函数内部代码更为抽象的东 西。你需要在函数名和短短⼏⾏注释⾥，把函数内代码所做的事情，⾼度浓缩地表达清楚。\n举个例⼦，你在编辑器⾥写下了 def process_user(\u0026hellip;):，准备实现⼀个名为 process_user 的新函数。在编写函数注释时，你发现在写了好⼏⾏⽂字后，仍然没法把 process_user() 的职责描述清楚，因为它可以同时完成好多件不同的事情。 这时你就应该意识到，process_user() 函数承担了太多职责，解决办法就是直接删掉它，设 计更多单⼀职责的⼦函数来替代之。 先写注释的另⼀个好处是：不会漏掉任何应该写的注释。 我常常在审查代码时发现，⼀些关键函数的 docstring 位置⼀⽚空⽩，⽽那⾥本该备注详尽 的接⼝注释。每当遇到这种情况，我都会不厌其烦地请代码提交者补充和完善接⼝注释。 为什么⼤家总会漏掉注释？我的⼀个猜测是：程序员在编写函数时，总是跳过接⼝注释直接开始 写代码。⽽当写完代码，实现函数的所有功能后，他就对这个函数失去了兴趣。这时，他最不愿意做 的事，就是回过头去补写函数的接⼝注释，即便写了，也只是草草对付了事。 如果遵守“先写注释，后写代码”的习惯，我们就能完全避免上⾯的问题。要养成这个习惯其实很 简单：在写出⼀句有说服⼒的接⼝注释前，别写任何函数代码\n要点总结 变量和注释决定“第⼀印象” 变量和注释是代码⾥最接近⾃然语⾔的东西，它们的可读性⾮常重要 即使是实现同⼀个算法，变量和注释不⼀样，给⼈的感觉也会截然不同 基础知识 Python 的变量赋值语法⾮常灵活，可以使⽤ *variables 星号表达式灵活赋值 编写注释的两个要点：不要⽤来屏蔽代码，⽽是⽤来解释“为什么” 接⼝注释是为使⽤者⽽写，因此应该简明扼要地描述函数职责，⽽不必包含太多内部细节 可以⽤ Sphinx 格式⽂档或类型注解给变量标明类型 变量名字很重要 给变量起名要遵循 PEP 8 原则，代码的其他部分也同样如此 尽量给变量起描述性强的名字，但评价描述性也需要结合场景 在保证描述性的前提下，变量名要尽量短 变量名要匹配它所表达的类型 可以使⽤⼀两个字⺟的超短名字，但注意不要过度使⽤ 代码组织技巧 按照代码的职责来组织代码：让变量定义靠近使⽤ 适当定义临时变量可以提升代码的可读性 不必要的变量会让代码显得冗⻓、啰唆 同⼀个作⽤域内不要有太多变量，解决办法：提炼数据类、拆分函数 空⾏也是⼀种特殊的“注释”，适当的空⾏可以让代码更易 代码可维护性技巧 保持变量在两个⽅⾯的⼀致性：名字⼀致性与类型⼀致性 显式优于隐式：不要使⽤ locals() 批量获取变量 把接⼝注释当成⼀种函数设计⼯具：先写注释，后写代码 数值与字符串 为了解决这个问题，Python 提供了⼀个内置模块：decimal。假如你的程序需要精确的浮点数 计算，请考虑使⽤ decimal.Decimal 对象来替代普通浮点数，它在做四则运算时不会损失任何精 度\n1 2 3 4 from decimal import Decimal # 这里的\u0026#39;0.1\u0026#39;和\u0026#39;0.2\u0026#39;必须是字符串 Decimal(\u0026#39;0.1\u0026#39;) + Decimal(\u0026#39;0.2\u0026#39;) Decimal(\u0026#39;0.3\u0026#39;) 在使⽤ Decimal 的过程中，⼤家需要注意：必须使⽤字符串来表⽰数字。如果你提供的是普通 浮点数⽽⾮字符串，在转换为 Decimal 对象前就会损失精度，掉进所谓的“浮点数陷阱”\n1 2 Decimal(0.1) Decimal(\u0026#39;0.1000000000000000055511151231257827021181583404541015625\u0026#39;) 布尔值其实也是数字 布尔（bool）类型是 Python ⾥⽤来表⽰“真假”的数据类型。你肯定知道它只有两个可选值： True 和 False。不过，你可能不知道的是：布尔类型其实是整型的⼦类型，在绝⼤多数情况下， True 和 False 这两个布尔值可以直接当作 1 和 0 来使⽤。\n1 2 3 4 5 6 7 8 9 10 11 int(True),int(False) (1,0) \u0026gt;\u0026gt; True + 1 2 # 把False当除数的效果和0一样 1/False Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; ZeroDivisionError: division by zero 布尔值的这个特点，常用来简化统计总数操作 假设有⼀个包含整数的列表，我需要计算列表⾥⼀共有多少个偶数。正常来说，我得写⼀个循环 加分⽀结构才能完成统计\n1 2 3 4 5 6 numbers = [1, 2, 4, 5, 7] count = 0 for i in numbers: if i % 2 == 0: count += 1 print(count) 利用布尔值可以作为整型使用的特性，一个简单的表达式就能完成同样的事情：\n1 count = sum(i % 2 == 0 for i in numbers) 此处的表达式 i % 2 == 0 会返回⼀个布尔值结果，该结果随后会被当成数字 0 或 1 由 sum() 函数累加求和\n字符串常用操作 字符串是⼀种序列类型，这意味着你可以对它进⾏遍历、切⽚等操作，就像访问⼀个列表对象⼀ 样\n第⼀种字符串格式化⽅式历史最为悠久，但现在已经很少使⽤。相⽐之下，后两种⽅式正变得越 来越流⾏。从个⼈体验来说，f-string 格式化⽅式⽤起来最⽅便，是我的⾸选。和其他两种 ⽅式⽐起来，使⽤ f-string 的代码多数情况下更简洁、更直观。\nstr.format 与 f-string 共享了同⼀种复杂的“字符串格式化微语⾔”。通过这种微语⾔， 我们可以⽅便地对字符串进⾏⼆次加⼯，然后输出。⽐如：\n1 2 3 4 5 6 # 将 username 靠右对⻬，左侧补空格到⼀共 20 个字符 # 以下两种⽅式将输出同样的内容 print(\u0026#39;{:\u0026gt;20}\u0026#39;.format(username)) print(f\u0026#39;{username:\u0026gt;20}\u0026#39;) # 输出： # piglei 虽然年轻的 f-string 抢⾛了 str.format 的⼤部分⻛头，但后者仍有着⾃⼰的独到之 处。⽐如 str.format ⽀持⽤位置参数来格式化字符串，实现对参数的重复使⽤：\n1 2 3 print(\u0026#39;{0}:name={0} score={1}\u0026#39;.format(username,score)) # 输出： piglei: name=piglei score=100 综上所述，⽇常编码中推荐优先使⽤ f-string，搭配 str.format 作为补充，想必能满⾜ ⼤家绝⼤多数的字符串格式化需求\n拼接多个字符串 假如要拼接多个字符串，⽐较常⻅的 Python 式做法是：⾸先创建⼀个空列表，然后把需要拼 接的字符串都放进列表，最后调⽤ str.join 来获得⼤字符串。⽰例如下：\n1 2 3 4 5 6 7 8 9 words = [\u0026#39;Number(1-10):\u0026#39;] for i in range(10): words.append(f\u0026#39;Value:{i+1}\u0026#39;) print(\u0026#39;\\n\u0026#39;.join(words)) Number(1-10): Value:1 ... Value:10 除了使⽤ join，也可以直接⽤ words_str += f\u0026rsquo;Value: {i + 1}\u0026rsquo; 这种⽅式来拼接字 符串。但也许有⼈告诫过你：“千万别这么⼲！这样操作字符串很慢很不专业！”这个说法也许 曾经正确，但现在看其实有些危⾔耸听。我在 2.3.5 节会向你证明：在拼接字符串时，+= 和 join 同样好⽤。\n不常用但特别好用的字符串方法 为了⽅便，Python 为字符串类型实现了⾮常多内置⽅法。在对字符串执⾏某种操作前，请⼀定 先查查某个内置⽅法是不是已经实现了该操作，否则⼀不留神就会重复发明轮⼦。\n⽇常编程中，我们最常⽤到的字符串⽅法有 .join()、.split()、.startswith()，等等。 虽然这些常⽤⽅法能满⾜⼤部分的字符串处理需求，但要成为真正的字符串⾼⼿，除了掌握常⽤⽅ 法，了解⼀些不那么常⽤的⽅法也很重要。在这⽅⾯，.partition() 和 .translate() ⽅法就 是两个很好的例⼦\nstr.partition(sep)的功能是按照分隔符sep切分字符串 返回一个包含三个成员的元组(part_before,sep,part_after)，它们分别代表分隔符前的内容，分隔符以及分隔符后的内容\n第⼀眼看上去，partition 的功能和 split 的功能似乎是重复的——两个⽅法都是分割字符 串，只是结果稍有不同。但在某些场景下，使⽤ partition 可以写出⽐⽤ split 更优雅的代 码\n举个例⼦，我有⼀个字符串 s，它的值可能会是以下两种格式。 (1)\u0026rsquo;{key}:{value}\u0026rsquo;，键值对标准格式，此时我需要拿到 value 部分。 (2)\u0026rsquo;{key}\u0026rsquo;，只有 key，没有冒号 : 分隔符，此时我需要拿到空字符串 \u0026lsquo;\u0026rsquo;。 如果⽤ split ⽅法来实现需求，我需要写出下⾯这样的代码\n1 2 3 4 5 6 7 def extract_value(s): items = s.split(\u0026#39;:\u0026#39;) # 因为s不一定会包含\u0026#39;:\u0026#39;,所以需要对结果长度进行判断 if len(items) == 2: return items[1] else: return \u0026#39;\u0026#39; 这个函数的逻辑虽算不上复杂，但由于 split 的特点，函数内的分⽀判断基本⽆法避免。这 时，如果使⽤ partition 函数来替代 split，原本的分⽀判断逻辑就可以消失——⼀⾏代码就能完 成任务：\n1 2 3 4 def extract_value_v2(s): # 当s包含分隔符 ： 时，元组最后一个成员刚好是value # 若是没有分隔符，最后一共成员默认时空字符串\u0026#39;\u0026#39; return s.partition(\u0026#39;:\u0026#39;)[-1] 除了 partition ⽅法，str.translate(table) ⽅法有时也⾮常有⽤。它可以按规则⼀次 性替换多个字符，使⽤它⽐调⽤多次 replace ⽅法更快也更简单：\n1 2 3 4 5 s = \u0026#39;明明是中文，却使用了英文标点.\u0026#39; # 创建替换规则表：\u0026#39;,\u0026#39; -\u0026gt; \u0026#39;，\u0026#39;,\u0026#39;.\u0026#39;-\u0026gt;\u0026#39;。\u0026#39; table = s.maketrans(\u0026#39;,.\u0026#39;,\u0026#39;，。\u0026#39;) s.translate(table) 字符串与字节符 字符串：我们最常挂在嘴边的“普通字符串”，有时也被称为⽂本（text），是给⼈看的， 对应 Python 中的字符串（str）类型。str 使⽤ Unicode 标准，可通过 .encode() ⽅法编 码为字节串\n字节串：有时也称“⼆进制字符串”（binary string），是给计算机看的，对应 Python 中的字节串（bytes）类型。bytes ⼀定包含某种真正的字符串编码格式（默认为 UTF-8），可通 过 .decode() 解码为字符串。\n要创建⼀个字节串字⾯量，可以在字符串前加⼀个字⺟ b 作为前缀\nbin_obj = b\u0026rsquo;Hello\u0026rsquo;\nbytes 和 str 是两种数据类型，即便有时看上去“⼀样”，但做⽐较时永不相等\n\u0026lsquo;Hello\u0026rsquo; == b\u0026rsquo;Hello\u0026rsquo; False\n1 2 3 4 5 6 7 \u0026gt;\u0026gt;\u0026gt; \u0026#39;Hello\u0026#39;.split(\u0026#39;e\u0026#39;) [\u0026#39;H\u0026#39;, \u0026#39;llo\u0026#39;] # str 不能使⽤ bytes 来调⽤任何内置⽅法，反之亦然 \u0026gt;\u0026gt;\u0026gt; \u0026#39;Hello\u0026#39;.split(b\u0026#39;e\u0026#39;) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; TypeError: must be str or None, not bytes 正因为字符串⾯向的是⼈，⽽⼆进制的字节串⾯向的是计算机，因此，在使⽤体验⽅⾯，前者要 好得多。在我们的程序中，应该尽量保证总是操作普通字符串，⽽⾮字节串。必须操作处理字节串的 场景，⼀般来说只有两种：\n程序从文件或其他外部存储读取字节串内容，将其解码为字符串，然后再在内部使⽤； 程序完成处理，要把字符串写⼊⽂件或其他外部存储，将其编码为字节串，然后继续执⾏其 他操作 当接收的输⼊是字节串时，你需要先将其转换为普通字符串，再调⽤函数：\n反之，当你要把字符串写⼊⽂件（进⼊计算机的领域）时，请谨记：普通字符串采⽤的是⽂本格 式，没法直接存放于外部存储，⼀定要将其编码为字节串——也就是“⼆进制字符串”——才⾏\n如果不指定 encoding，Python 将通过 locale 模块获取系统偏好的编码格式\n说到有意义的数字，⼤家最先想到的⼀般是“常量”（constant）。但 Python ⾥没有真正 的常量类型，⼈们⼀般会把⼤写字⺟全局变量当“常量”来⽤。\n⽐如把积分数量定义为常量：\n1 2 3 4 # ⽤⼾每⽇奖励积分数量 DAILY_POINTS_REWARDS = 100 # VIP ⽤⼾每⽇额外奖励积分数量 VIP_EXTRA_POINTS = 20 除了常量以外，我们还可以使⽤枚举类型（enum.Enum）。\n1 2 3 4 5 6 7 8 9 from enum import Enum # 在定义枚举类型时，如果同时继承⼀些基础类型，⽐如 int、str， # 枚举类型就能同时充当该基础类型使⽤。⽐如在这⾥，UserType 就可以当作int 使⽤ class UserType(int,Enum): # VIP用户 VIP = 3 # 小黑屋用户 BANNED = 13 有了这些常量和枚举类型后，⼀开始那段满是“密码”的代码就可以重写成这样：\n1 2 3 4 5 6 7 8 9 def add_daily_points(user): \u0026#34;\u0026#34;\u0026#34;用户每天完成第一次登录后，为其增加积分\u0026#34;\u0026#34;\u0026#34; if user.type == UserType.BANNED: return if user.type == UserType.VIP: user.points += DAILY_POINTS_REWARDS + VIP_EXTRA_POINTS return user.points += DAILY_POINTS_REWARDS return 把那些神奇的数字定义成常量和枚举类型后，代码的可读性得到了可观的提升。不仅如此，代 码出现 bug 的概率其实也降低了。\n最后，总结⼀下⽤常量和枚举类型来代替字⾯量的好处。\n更易读：所有⼈都不需要记忆某个数字代表什么。 更健壮：降低输错数字或字⺟产⽣ bug 的可能性 别轻易成为SQL语句大师 在这个⼤公司的核⼼项⽬⾥，所有的数据库操作代码，都是⽤下⾯这样的“裸字符串处理”逻辑拼 接 SQL 语句⽽成的，⽐如⼀个根据条件查询⽤⼾列表的函数如下所⽰：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def fetch_users( conn, min_level=None, gender=None, has_membership=False, sort_field=\u0026#34;created\u0026#34;, ): \u0026#34;\u0026#34;\u0026#34;获取⽤⼾列表 :param min_level: 要求的最低⽤⼾级别，默认为所有级别 :type min_level: int, optional :param gender: 筛选⽤⼾性别，默认为所有性别 :type gender: int, optional :param has_membership: 筛选会员或⾮会员⽤⼾，默认为 False，代表⾮会员 :type has_membership: bool, optional :param sort_field: 排序字段，默认为 \u0026#34;created\u0026#34;，代表按⽤⼾创建⽇期排序 :type sort_field: str, optional :return: ⼀个包含⽤⼾信息的列表：[(User ID, User Name), ...] \u0026#34;\u0026#34;\u0026#34; # ⼀种古⽼的 SQL 拼接技巧，使⽤“WHERE 1=1”来简化字符串拼接操作 statement = \u0026#34;SELECT id, name FROM users WHERE 1=1\u0026#34; params = [] if min_level is not None: statement += \u0026#34; AND level \u0026gt;= ?\u0026#34; params.append(min_level) if gender is not None: statement += \u0026#34; AND gender \u0026gt;= ?\u0026#34; params.append(gender) if has_membership: statement += \u0026#34; AND has_membership = true\u0026#34; else: statement += \u0026#34; AND has_membership = false\u0026#34; statement += \u0026#34; ORDER BY ?\u0026#34; params.append(sort_field) # 将查询参数 params 作为位置参数传递，避免 SQL 注⼊问题 return list(conn.execute(statement, params)) 但令⼈遗憾的是，这样的代码只是看上去简单，实际上有⼀个⾮常⼤的问题：⽆法有效表达更复杂的业务逻辑。假如未来查询逻辑要增加⼀些复合条件、连表查询，⼈们很难在现有代码的基础上扩展，修改也容易出错。\n使用SQLAlchmey模块改写代码 上述函数所做的事情，我习惯称之为“裸字符串处理”。这种处理⼀般只使⽤基本的加减乘除和循环，配合 .split() 等内置⽅法来不断操作字符串，获得想要的结果。\n它的优点显⽽易⻅：⼀开始业务逻辑⽐较简单，操作字符串代码符合思维习惯，写起来容易。但随着业务逻辑逐渐变得复杂，这类裸处理就会显得越来越⼒不从⼼。 其实，对于 SQL 语句这种结构化、有规则的特殊字符串，⽤对象化的⽅式构建和编辑才是更好的做法\n下⾯这段代码引⼊了 SQLAlchemy 模块，⽤更少的代码量完成了同样的功能\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def fetch_users_v2( conn, min_level=None, gender=None, has_membership=False, sort_field=\u0026#34;created\u0026#34;, ): \u0026#34;\u0026#34;\u0026#34;获取用户列表\u0026#34;\u0026#34;\u0026#34; query = select([users.c.id,users.c.name]) if mini_level != None: query = query.where(users.c.level \u0026gt;= min_level) if gender != None: query = query.where(users.c.gender == gender) query = query.where(users.c.has_membership == has_membership).order_by( users.c[sort_field] ) return list(conn.execute(query)) 新的 fetch_users_v2() 函数不光更短、更好维护，⽽且根本不需要担⼼ SQL 注⼊问 题。它最⼤的缺点在于引⼊了⼀个额外依赖：sqlalchemy，但同 sqlalchemy 带来的种种 好处相⽐，这点复杂度成本微不⾜道。\n使用Jinja2模板处理字符串 除了 SQL 语句，我们⽇常接触最多的还是⼀些普通字符串拼接任务。⽐如，有⼀份电影评分 数据列表，我需要把它渲染成⼀段⽂字并输出\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def render_movies(username,movies): \u0026#34;\u0026#34;\u0026#34; 以文本方式展开电源列表信息 \u0026#34;\u0026#34;\u0026#34; welcome_test = \u0026#39;Welcome,{}.\\n\u0026#39;.format(username) text_parts = [welcome_text] for name,rating in movies: # 没有提供评分的电影，以[NOT RATED]代替 rating_text = rating if rating else \u0026#39;[NOT RATED]\u0026#39; movie_item = \u0026#39;* {},Rating:{}\u0026#39;.format(name,rating_text) text_parts.append(movie_item) return \u0026#39;\\n\u0026#39;.join(text_parts) movies = [ (\u0026#39;The Shawshank Redemption\u0026#39;, \u0026#39;9.3\u0026#39;), (\u0026#39;The Prestige\u0026#39;, \u0026#39;8.5\u0026#39;), (\u0026#39;Mulan\u0026#39;, None), ] print(render_movies(\u0026#39;piglei\u0026#39;,movies)) Welcome, piglei.\nThe Shawshank Redemption, Rating: 9.3 The Prestige, Rating: 8.5 Mulan, Rating: [NOT RATED] 或许你觉得，这样的字符串拼接代码没什么问题。但如果使⽤ Jinja2 模板引擎处理，代码 可以变得更简单：\n1 2 3 4 5 6 7 from jinja2 import Template _MOVIES_TMPL = \u0026#39;\u0026#39;\u0026#39;\\ Welcome,{{ username }}. {%for name,rating in movies %} * {{ name }}, Rating: {{ rating|default(\u0026#34;[NOT RATED]\u0026#34;,True)}} {%- endfor %} 和之前的代码相⽐，新代码少了列表拼接、默认值处理，所有的逻辑都通过模板语⾔来表达。 假如我们的渲染逻辑以后变得更复杂，第⼆份代码也能更好地随之进化。 总结⼀下，当你的代码⾥出现复杂的裸字符串处理逻辑时，请试着问⾃⼰⼀个问题：“⽬标/源 字符串是结构化的且遵循某种格式吗？”如果答案是肯定的，那么请先寻找是否有对应的开源专 有模块，⽐如处理 SQL 语句的 SQLAlchemy、处理 XML 的 lxml 模块等。 如果你要拼接⾮结构化字符串，也请先考虑使⽤ Jinja2 等模板引擎，⽽不是⼿动拼接，因 为⽤模板引擎处理字符串之后，代码写起来更⾼效，也更容易维护。\n编程建议 不必预计算字面量表达式 但事实是，即便我们把代码改写成 if delta_seconds \u0026lt; 11 * 24 * 3600:，函数也不会 多出任何额外开销。为了展⽰这⼀点，我们需要⽤到两个知识点：字节码与 dis 模块\n使⽤ dis 模块反编译字节码 虽然 Python 是⼀⻔解释型语⾔，但在解释器真正运⾏ Python 代码前，其实仍然有⼀个类 似“编译”的加速过程：将代码编译为⼆进制的字节码。我们没法直接读取字节码，但利⽤内置的 dis 模块 ，可以将它们反汇编成⼈类可读的内容——类似⼀⾏⾏的汇编代码\ndis 的全称是 disassembler for Python bytecode，翻译过来就是 Python 字节码的反汇编器。\n先举⼀个简单的例⼦。⽐如，⼀个简单的加法函数的反汇编结果是这样的：\n1 2 3 4 5 6 7 8 9 10 def add(x,y): return x+y # 导⼊ dis 模块，使⽤它打印 add() 函数的字节码，也就是解释器如何理解 add() 函数 import dis dis.dis(add) 2 0 LOAD_FAST 0 (x) 2 LOAD_FAST 1 (y) 4 BINARY_ADD 6 RETURN_VALUE 在上⾯的输出中，add() 函数的反汇编结果主要展⽰了下⾯⼏种操作。 (1) 两次 LOAD_FAST：分别把局部变量 x 和 y 的值放⼊栈顶。 (2) BINARY_ADD：从栈顶取出两个值（也就是 x 和 y 的值），执⾏加法操作，将结果放回 栈顶。 (3) RETURN_VALUE：返回栈顶的结果。\n注意到 2 LOAD_CONST 1 (950400) 那⼀⾏了吗？这表⽰ Python 解释器在将源码编译 成字节码时，会主动计算 11 * 24 * 3600 表达式的结果，并⽤ 950400 替换它。也就是说， ⽆论你调⽤ do_something 多少次，其中的算式 11 * 24 * 3600 都只会在编译期被执⾏ 1 次。\n因此，当我们需要⽤到复杂计算的数字字⾯量时，请保留整个算式吧。这样做对性能没有任何影 响，⽽且会让代码更容易阅读。\n使用特殊数字：无穷大 答案是“有的”，它们就是 float(\u0026ldquo;inf\u0026rdquo;) 和 float(\u0026quot;-inf\u0026quot;)。这两个值分别对应数学世界 ⾥的正负⽆穷⼤。当它们和任意数值做⽐较时，满⾜这样的规律：float(\u0026quot;-inf\u0026quot;) \u0026lt; 任意数值 \u0026lt; float(\u0026ldquo;inf\u0026rdquo;)。\n⽐如有⼀个包含⽤⼾名和年龄的字典，我需要把⾥⾯的⽤⼾名按照年龄升序排序，没有提供年龄 的放在最后。使⽤ float(\u0026lsquo;inf\u0026rsquo;)，代码可以这么写：\n1 2 3 4 5 6 7 8 9 10 11 12 13 def sort_users_inf(users): def key_func(username): age = users[username] # 当年龄为空时,返回正无穷大作为key，因此就会被排到最后 return age if age is not None else float(\u0026#39;inf\u0026#39;) return sorted(users.keys(),key=key_func) users = {\u0026#34;tom\u0026#34;: 19, \u0026#34;jenny\u0026#34;: 13, \u0026#34;jack\u0026#34;: None, \u0026#34;andrew\u0026#34;: 43} print(sort_users_inf(users)) # 输出 [\u0026#39;jenny\u0026#39;, \u0026#39;tom\u0026#39;, \u0026#39;andrew\u0026#39;, \u0026#39;jack\u0026#39;] 改善超长字符串的可读性 为了保证可读性，单⾏代码的⻓度不宜太⻓。⽐如 PEP 8 规范就建议每⾏字符数不超过 79。 在现实世界⾥，⼤部分⼈遵循的单⾏最⼤字符数通常会⽐ 79 稍⼤⼀点⼉，但⼀般不会超过 119 个字符\n这时，除了⽤斜杠 \\ 和加号 + 将⻓字符串拆分为⼏段，还有⼀种更简单的办法，那就是拿括 号将⻓字符串包起来，之后就可以随意折⾏了\n多级缩进里出现多行字符串 但假如你不想那么做，也可以⽤标准库 textwrap 来解决这个问题：\n1 2 3 4 5 6 7 8 9 from textwrap import dedent def main(): if user.is_active: message = dedent(\u0026#34;\u0026#34;\u0026#34;\\ Welcome,today\u0026#39;s move list: - Jaw (1975) - The Shining (1980) - Saw (2004)\u0026#34;\u0026#34;\u0026#34;) dedent ⽅法会删除整段字符串左侧的空⽩缩进。使⽤它来处理多⾏字符串以后，整段代码的缩 进视觉效果就能保持正常了。\n别忘了以r开头的字符串内置方法 但除了这些“正序”⽅法，字符串其实还有⼀些执⾏从右往左处理的“逆序”⽅法。这些⽅法都以 字符 r 开头，⽐如 rsplit() 就是 split() 的镜像“逆序”⽅法。在处理某些特定任务时，使 ⽤“逆序”⽅法也许能事半功倍\n举个例⼦，假设我需要解析⼀些访问⽇志，⽇志格式为 \u0026lsquo;\u0026quot;{user_agent}\u0026quot; {content_length}\u0026rsquo;：\nlog_line = \u0026lsquo;\u0026ldquo;AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/ 537.36\u0026rdquo; 47632\u0026rsquo;\n如果使⽤ .split() 将⽇志拆分为 (user_agent, content_length)，我们需要这么 写：\n1 2 3 4 5 l = log_line.split() # 因为UserqAgent里面有空格,所以切完后得把它们再连接起来 \u0026gt;\u0026gt;\u0026gt; \u0026#34; \u0026#34;.join(l[:-1]),l[-1] (\u0026#39;\u0026#34;AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\u0026#34;\u0026#39;, \u0026#39;47632\u0026#39;) 但假如利⽤ .rsplit()，处理逻辑就可以变得更直接：\n1 2 3 # 从右往左切割，None 表⽰以所有的空⽩字符串切割 \u0026gt;\u0026gt;\u0026gt; log_line.rsplit(None,maxsplit=1) [\u0026#39;\u0026#34;AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\u0026#34;\u0026#39;, \u0026#39;47632\u0026#39;] 不要害怕字符串拼接 Python 有⼀个内置模块 timeit，利⽤它，我们可以⾮常⽅便地测试代码的执⾏效率。⾸ 先，定义需要测试的两个函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 定义⼀个⻓度为 100 的词汇列表 WORDS = [\u0026#39;Hello\u0026#39;, \u0026#39;string\u0026#39;, \u0026#39;performance\u0026#39;, \u0026#39;test\u0026#39;] * 25 def str_cat(): \u0026#34;\u0026#34;\u0026#34;使用字符串拼接\u0026#34;\u0026#34;\u0026#34; s = \u0026#39;\u0026#39; for word in WORDS: s += word return s def str_join(): \u0026#34;\u0026#34;\u0026#34;使用列表配合join产生字符串\u0026#34;\u0026#34;\u0026#34; l = [] for word in WORDS: l.append(word) return \u0026#39;\u0026#39;.join(l) 然后，导⼊ timeit 模块，定义性能测试：\n1 2 3 4 5 6 7 8 import timeit # 默认执行100万次 cat_spent = timeit.timeit(setup=\u0026#39;from __main__ import str_cat\u0026#39;,stmt=\u0026#39;str_cat()\u0026#39;) pirnt(\u0026#34;cat_spent:\u0026#34;,cat_spent) join_spent = timeit.timeit(setup=\u0026#39;from __main__ import str_join\u0026#39;,stmt=\u0026#39;str_join()\u0026#39;) print(\u0026#34;join_spent\u0026#34;,join_spent) cat_spent: 7.844882188 join_spent 7.310863505\n如今，使⽤ += 拼接字符串基本已经和 \u0026ldquo;\u0026quot;.join(str_list) ⼀样快了。所以，该拼接时就 拼接吧，少量的字符串拼接根本不会带来任何性能问题，反⽽会让代码更直观。\n总结 数值基础知识 Python 的浮点数有精度问题，请使⽤ Decimal 对象做精确的⼩数运算 布尔类型是整型的⼦类型，布尔值可以当作 0 和 1 来使⽤ 使⽤ float(\u0026lsquo;inf\u0026rsquo;) ⽆穷⼤可以简化边界处理逻辑 字符串基础知识 字符串分为两类：str（给⼈阅读的⽂本类型）和 bytes（给计算机阅读的⼆进制类型） 通过 .encode() 与 .decode() 可以在两种字符串之间做转换 优先推荐的字符串格式化⽅式（从前往后）：f-string、str.format()、C 语⾔⻛格格式 化 使⽤以 r 开头的字符串内置⽅法可以从右往左处理字符串，特定场景下可以派上⽤场 字符串拼接并不慢，不要因为性能原因害怕使⽤它 代码可读性技巧 在定义数值字⾯量时，可以通过插⼊ _ 字符来提升可读性 不要出现“神奇”的字⾯量，使⽤常量或者枚举类型替换它们 保留数学算式表达式不会影响性能，并且可以提升可读性 使⽤ textwrap.dedent() 可以让多⾏字符串更好地融⼊代码 代码可维护性技巧 当操作 SQL 语句等结构化字符串时，使⽤专有模块⽐裸处理的代码更易于维护 使⽤ Jinja2 模板来替代字符串拼接操作 语言内部知识 使⽤ dis 模块可以查看 Python 字节码，帮助我们理解内部原理 使⽤ timeit 模块可以对 Python 代码⽅便地进⾏性能测试 Python 语⾔进化得很快，不要轻易被旧版本的“经验”所左右 容器类型 ⽐如每个类实例的所有属性，就都存放在⼀个名为 dict 的字典⾥\n在遍历列表时获取下标 当你使⽤ for 循环遍历列表时，默认会逐个拿到列表的所有成员。假如你想在遍历的同时，获 取当前循环下标，可以选择⽤内置函数 enumerate() 包裹列表对象\n列表推导式 理解列表的可变性 可变（mutable）：列表、字典、集合。 不可变（immutable）：整数、浮点数、字符串、字节串、元组。 值传递（pass-by-value）：调⽤函数时，传过去的是变量所指向对象（值）的拷⻉，因 此对函数内变量的任何修改，都不会影响原始变量——对应 orig_obj 是字符串时的⾏为。 引⽤传递（pass-by-reference）：调⽤函数时，传过去的是变量⾃⾝的引⽤（内存地 址），因此，修改函数内的变量会直接影响原始变量——对应 orig_obj 是列表时的⾏为。\n看了上⾯的解释，你也许会发出灵魂拷问：为什么 Python 的函数调⽤要同时使⽤两套不同的 机制，把事情搞得这么复杂呢？\n答案其实没有你想得那么“复杂”——Python 在进⾏函数调⽤传参时，采⽤的既不是值传递，也不 是引⽤传递，⽽是传递了“变量所指对象的引⽤”（pass-by-object-reference）。\n换个⻆度说，当你调⽤ func(orig_obj) 后，Python 只是新建了⼀个函数内部变量 in_func_obj，然后让它和外部变量 orig_obj 指向同⼀个对象，相当于做了⼀次变量赋值 ⼀次函数调⽤基本等于执⾏了 in_func_obj = orig_obj。 所以，当我们在函数内部执⾏ in_func_obj += \u0026hellip; 等修改操作时，是否会影响外部变量， 只取决于 in_func_obj 所指向的对象本⾝是否可变\n在对字符串进⾏ += 操作时，因为字符串是不可变类型，所以程序会⽣成⼀个新对象 （值）：\u0026lsquo;foo suffix\u0026rsquo;，并让 in_func_obj 变量指向这个新对象；旧值（原始变量 orig_obj 指向的对象）则不受任何影响，如图 3-2 右侧所⽰ 但如果对象是可变的（⽐如列表），+= 操作就会直接原地修改 in_func_obj 变量所指向的 值，⽽它同时也是原始变量 orig_obj 所指向的内容；待修改完成后，两个变量所指向的值（同⼀ 个）肯定就都受到了影响。如图 3-3 所⽰，右边的列表在操作后直接多了⼀个成员：\u0026lsquo;bar\u0026rsquo; 由此可⻅，Python 的函数调⽤不能简单归类为“值传递”或者“引⽤传递”，⼀切⾏为取决于对象 的可变性\n返回多个结果，其实就是返回元组 1 2 3 4 5 6 7 8 9 def get_rectangle(): \u0026#34;\u0026#34;\u0026#34;返回长方形的宽和高\u0026#34;\u0026#34;\u0026#34; width = 100 height = 20 return width,height # 获取函数的多个返回值 result = get_rectangle() print(result,type(result)) 将函数返回值⼀次赋值给多个变量时，其实就是对元组做了⼀次解包操作：\n1 2 3 results = (n*100 for n in range(10) if n%2 == 0) results \u0026gt;\u0026gt;\u0026gt; \u0026lt;generator object \u0026lt;genexpr\u0026gt; at 0x10e94e2e0\u0026gt; 很遗憾，上⾯的表达式并没有⽣成元组，⽽是返回了⼀个⽣成器（generator）对象。因此它 是⽣成器推导式，⽽⾮元组推导式。 不过幸运的是，虽然⽆法通过推导式直接拿到元组，但⽣成器仍然是⼀种可迭代类型，所以我们 还是可以对它调⽤ tuple() 函数，获得元组\n1 2 3 result = tuple((n*100 for n in range(10) if n%2==0)) results (0,200,400,600,800) 具名元组 具名元组在保留普通元组功能的基础上，允许为元组的每个成员命名，这样你便能通过名称⽽不⽌是数字索引访问 成员\n创建具名元组需要⽤到 namedtuple() 函数，它位于标准库的 collections 模块⾥，使⽤ 前需要先导⼊：\n1 2 3 from collections import namedtuple Rectangle = namedtuple(\u0026#39;Rectangle\u0026#39;,\u0026#39;width,height\u0026#39;) 除了⽤逗号来分隔具名元组的字段名称以外，还可以⽤空格分隔：\u0026lsquo;width height\u0026rsquo;，或是 直接使⽤⼀个字符串列表：[\u0026lsquo;width\u0026rsquo;, \u0026lsquo;height\u0026rsquo;]\n1 2 3 4 5 6 7 8 9 \u0026gt;\u0026gt;\u0026gt; rect = Rectangle(100, 20) \u0026gt;\u0026gt;\u0026gt; rect = Rectangle(width=100, height=20) \u0026gt;\u0026gt;\u0026gt; print(rect[0]) 100 \u0026gt;\u0026gt;\u0026gt; print(rect.width) 100 \u0026gt;\u0026gt;\u0026gt; rect.width += 1 ... AttributeError: can\u0026#39;t set attribute 在 Python 3.6 版本以后，除了使⽤ namedtuple() 函数以外，你还可以⽤ typing.NamedTuple 和类型注解语法来定义具名元组类型。这种⽅式在可读性上更胜⼀筹：\n1 2 3 4 5 class Rectangle(NamedTuple): width:int height:int rect = Rectangle(100,20) 但需要注意的是，上⾯的写法虽然给 width 和 height 加了类型注解，但 Python 在执⾏ 时并不会做真正的类型校验。也就是说，下⾯这段代码也能正常执⾏：\n1 2 # 提供错误的类型来初始化 rect_wrong_type = Rectangle(\u0026#39;string\u0026#39;, \u0026#39;not_a_number\u0026#39;) 遍历字典 当我们直接遍历⼀个字典对象时，会逐个拿到字典所有的 key。如果你想在遍历字典时同时获 取 key 和 value，需要使⽤字典的 .items() ⽅法：\n访问不存在的字典键 当⽤不存在的键访问字典内容时，程序会抛出 KeyError 异常，我们通常称之为程序⾥的边界 情况（edge case）。针对这种边界情况，⽐较常⻅的处理⽅式有两种：\n直接操作，但是捕获 KeyError 异常。 读取内容前先做⼀次条件判断，只有判断通过的情况下才继续执⾏其他操作； 1 2 3 4 5 6 7 8 9 10 11 # 第一种写法 if \u0026#39;rating\u0026#39; in movie: rating = movie[\u0026#39;rating\u0026#39;] else: rating = 0 # 第二种写法 try: rating = movie[\u0026#39;rating\u0026#39;] except KeyError: rating = 0 在 Python 中，⼈们⽐较推崇第⼆种写法，因为它看起来更简洁，执⾏效率也更⾼。不过，如 果只是“提供默认值的读取操作”，其实可以直接使⽤字典的 .get() ⽅法。\ndict.get(key, default) ⽅法接收⼀个 default 参数，当访问的键不存在时，⽅法会 返回 default 作为默认值：\n1 movie.get(\u0026#39;rating\u0026#39;,0) 使用setdefault取值并修改 有时，我们需要修改字典中某个可能不存在的键，⽐如在下⾯的代码⾥，我需要往字典 d 的 items 键⾥追加新值，但 d[\u0026lsquo;items\u0026rsquo;] 可能根本就不存在。因此我写了⼀段异常捕获逻辑—— 假如 d[\u0026lsquo;items\u0026rsquo;] 不存在，就以列表来初始化它\n针对上⾯这种情况，其实有⼀个更适合的⼯具：d.setdefault(key, default=None) ⽅ 法。使⽤它，可以直接删掉上⾯的异常捕获，代码逻辑会变得更简单。\n视条件的不同，调⽤ dict.setdefault(key, default) 会产⽣两种结果：当 key 不存 在时，该⽅法会把 default 值写⼊字典的 key 位置，并返回该值；假如 key 已经存在， 该⽅法就会直接返回它在字典中的对应值。代码如下：\n1 2 3 4 5 6 7 d = {\u0026#39;title\u0026#39;:\u0026#39;foobar\u0026#39;} d.setdefault(\u0026#39;items\u0026#39;,[]).append(\u0026#39;foo\u0026#39;) d {\u0026#39;title\u0026#39;: \u0026#39;foobar\u0026#39;, \u0026#39;items\u0026#39;: [\u0026#39;foo\u0026#39;]} d.setdefault(\u0026#39;items\u0026#39;,[]).append(\u0026#39;bar\u0026#39;) d {\u0026#39;title\u0026#39;: \u0026#39;foobar\u0026#39;, \u0026#39;items\u0026#39;: [\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;]} 使⽤ pop ⽅法删除不存在的键 但假设你只是单纯地想去掉某个键，并不关⼼它存在与否、删除有没有成功，那么使⽤ dict.pop(key, default) ⽅法就够了。\n只要在调⽤ pop ⽅法时传⼊默认值 None，在键不存在的情况下也不会产⽣任何异常\nd.pop(key,None)\n严格说来，pop ⽅法的主要⽤途并不是删除某个键，⽽是取出这个键对应的值。但 我个⼈觉得，偶尔⽤它来执⾏删除操作也⽆伤⼤雅。\n字典推导式 和列表类似，字典同样有⾃⼰的字典推导式。（⽐元组待遇好多啦！）你可以⽤它来⽅便地过滤 和处理字典成员：\n1 2 3 d1 = {\u0026#39;foo\u0026#39;:3,\u0026#39;bar\u0026#39;:4} {key:value * 10 for key,value in d1.items() if key == \u0026#39;foo\u0026#39;} \u0026gt;\u0026gt;\u0026gt;{\u0026#39;foo\u0026#39;,30} 但 Python 语⾔在不断进化。Python 3.6 为字典类型引⼊了⼀个改进：优化了底层实现，同 样的字典相⽐ 3.5 版本可节约多达 25% 的内存。⽽这个改进同时带来了⼀个有趣的副作⽤：字典 变得有序了。\n但如果你使⽤的 Python 版本没有那么新，也可以从 collections 模块⾥⽅便地拿到另⼀个 有序字典对象 OrderedDict，它可以在 Python 3.7 以前的版本⾥保证字典有序\n1 2 3 4 5 6 7 8 9 10 from collections import OrderedDict d = OrderedDict() d[\u0026#39;FIRST_KEY\u0026#39;] = 1 D[\u0026#39;SECOND_KEY\u0026#39;] = 2 for key in d: print(key) FIRST_KEY SECOND_KEY 但在我看来，OrderedDict ⽐起普通字典仍然有⼀些优势。最直接的⼀点是，OrderedDict 把“有序”放在了⾃⼰的名字⾥，因此当你在代码中使⽤它时，其实⽐普通字典更清晰地表达了“此处 会依赖字典的有序特性”这⼀点。\n内容⼀致⽽顺序不同的字典被视作相等，因为解释器只对⽐字典的键和值是否⼀致\n同样的 OrderedDict 则被视作不相等，因为“键的顺序”也会作为对⽐条件\n除此之外，OrderedDict 还有 .move_to_end() 等普通字典没有的⼀些⽅法。所以，即便 Python 3.7 及之后的版本已经提供了内置的“有序字典”，但 OrderedDict 仍然有着⾃⼰的⼀席 之地。\n集合常用操作 集合也有⾃⼰的推导式语法：\n1 2 3 nums = [1,2,2,4,1] {n for n in nums if n\u0026lt;3} {1,2} 集合是⼀种可变类型，使⽤ .add() ⽅法可以向集合追加新成员：\n假如你想要⼀个不可变的集合，可使⽤内置类型 frozenset，它和普通 set ⾮常像，只是少 了所有的修改类⽅法：\n集合运算 对两个集合求交集，也就是获取两个集合中同时存在的东西：\u0026amp; intersection\n对集合求并集，把两个集合⾥的东西合起来：| union\n对集合求差集，获得前⼀个集合有、后⼀个集合没有的东西： - difference\n正如上⾯的报错信息所⽰，集合⾥只能存放“可哈希”（hashable）的对象。假如把不可哈希的 对象（⽐如上⾯的列表）放⼊集合，程序就会抛出 TypeError 异常\n了解对象的可哈希性 在介绍字典类型时，我们说过字典底层使⽤了哈希表数据结构，其实集合也⼀样。当我们把某个 对象放进集合或者作为字典的键使⽤时，解释器都需要对该对象进⾏⼀次哈希运算，得到哈希值，然 后再进⾏后⾯的操作。\n这个计算哈希值的过程，是通过调⽤内置函数 hash(obj) 完成的。如果对象是可哈希的， hash 函数会返回⼀个整型结果，否则将会报 TypeError 错误。\n因此，要把某个对象放进集合，那它就必须是“可哈希”的。话说到这⾥，到底哪些类型是可哈希 的？哪些⼜是不可哈希的呢？我们来试试看\n⾸先，那些不可变的内置类型都是可哈希的：\n⽽可变的内置类型都⽆法正常计算哈希值：\n可变类型的不可哈希特点有⼀定的“传染性”。⽐如在⼀个原本可哈希的元组⾥放⼊可变的列表对 象后，它也会⻢上变得不可哈希\n1 2 hash((1,2,3,[\u0026#39;foo\u0026#39;,\u0026#39;bar\u0026#39;])) TypeError: unhashable type: \u0026#39;list\u0026#39; 由⽤⼾定义的所有对象默认都是可哈希的：\n1 2 3 4 5 6 class Foo: pass foo = Foo() has(foo) \u0026gt;\u0026gt;\u0026gt; 273594269 总结⼀下，某种类型是否可哈希遵循下⾯的规则：\n所有的不可变内置类型，都是可哈希的，⽐如 str、int、tuple、frozenset 等； 所有的可变内置类型，都是不可哈希的，⽐如 dict、list 等； 对于不可变容器类型 (tuple, frozenset)，仅当它的所有成员都不可变时，它⾃⾝才 是可哈希的 ⽤⼾定义的类型默认都是可哈希的。 谨记，只有可哈希的对象，才能放进集合或作为字典的键使⽤\n深拷贝与浅拷贝 在操作这些可变对象时，如果不拷⻉原始对象就修改，可能会产⽣我们并不期待的结果\n假如我们想让两个变量的修改操作互不影响，就需要拷⻉变量所指向的可变对象，做到让不同变 量指向不同对象。按拷⻉的深度，常⽤的拷⻉操作可分为两种：浅拷⻉与深拷⻉\n浅拷贝 要进⾏浅拷⻉，最通⽤的办法是使⽤ copy 模块下的 copy() ⽅法：\n1 2 3 4 5 6 7 import copy nums_copy = copy.copy(nums) nums[2] = 30 # 修改不再相互影响 \u0026gt;\u0026gt;\u0026gt; nums, nums_copy ([1, 2, 30, 4], [1, 2, 3, 4]) 除了使⽤ copy() 函数外，对于那些⽀持推导式的类型，⽤推导式也可以产⽣⼀个浅拷⻉对 象：\n1 2 3 4 5 d = {\u0026#39;foo\u0026#39;:1} d2 = {key:value for key,value in d.items()} d[\u0026#39;foo\u0026#39;] = 2 d,d2 ({\u0026#39;foo\u0026#39;: 2}, {\u0026#39;foo\u0026#39;: 1}) 使⽤各容器类型的内置构造函数，同样能实现浅拷⻉效果：\n1 2 d2 = dict(d.items()) nums_copy = list(nums) 以字典 d 的内容构建⼀个新字典 以列表 nums 的成员构建⼀个新列表\n对于⽀持切⽚操作的容器类型——⽐如列表、元组，对其进⾏全切⽚也可以实现浅拷⻉效果：\n1 2 # nums_copy 会变成 nums 的浅拷⻉ nums_copy = nums[:] 除了上⾯这些办法，有些类型⾃⾝就提供了浅拷⻉⽅法，可以直接使⽤：\n1 2 3 4 5 6 7 8 9 # 列表有copy方法 num = [1,2,3,4] nums.copy() [1,2,3,4] # 字典也有copy方法 d = {\u0026#39;foo\u0026#39;:\u0026#39;bar\u0026#39;} d.copy() {\u0026#39;foo\u0026#39;:\u0026#39;bar\u0026#39;} 深拷贝 ⼤部分情况下，上⾯的浅拷⻉操作⾜以满⾜我们对可变类型的复制需求。但对于⼀些层层嵌套的 复杂数据来说，浅拷⻉仍然⽆法解决嵌套对象被修改的问题。\n⽐如，下⾯的 items 是⼀个嵌套了⼦列表的多级列表：\nitems = [1, [\u0026lsquo;foo\u0026rsquo;, \u0026lsquo;bar\u0026rsquo;], 2, 3]\n1 2 3 4 5 6 7 8 \u0026gt;\u0026gt;\u0026gt; import copy \u0026gt;\u0026gt;\u0026gt; items_copy = copy.copy(items) \u0026gt;\u0026gt;\u0026gt; items[0] = 100 ❶ \u0026gt;\u0026gt;\u0026gt; items[1].append(\u0026#39;xxx\u0026#39;) ❷ \u0026gt;\u0026gt;\u0026gt; items [100, [\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;, \u0026#39;xxx\u0026#39;], 2, 3] \u0026gt;\u0026gt;\u0026gt; items_copy ❸ [1, [\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;, \u0026#39;xxx\u0026#39;], 2, 3] ❶ 修改 items 的第⼀层成员 ❷ 修改 items 的第⼆层成员，往⼦列表内追加元素 ❸ 对 items[1] 的第⼀层修改没有影响浅拷⻉对象，items_copy[0] 仍然是 1，但 对嵌套⼦列表 items[1] 的修改已经影响了 items_copy[1] 的值，列表内多出了 \u0026lsquo;xxx\u0026rsquo;\n之所以会出现这样的结果，是因为即便对 items 做了浅拷⻉，items[1] 和 items_copy[1] 指向的仍旧是同⼀个列表。如果使⽤ id() 函数查看它们的对象 ID，会发 现它们其实是同⼀个对象：\n要解决这个问题，可以⽤ copy.deepcopy() 函数来进⾏深拷⻉操作：\nitems_deep = copy.deepcopy(items)\n深拷⻉会遍历并拷⻉ items ⾥的所有内容——包括它所嵌套的⼦列表。做完深拷⻉后，items 和 items_deep 的⼦列表不再是同⼀个对象，它们的修改操作⾃然也不会再相互影响：\n要优化性能，第⼀步永远是找到性能瓶颈。刚好，我把⽹站所有⻚⾯的访问耗时都记录在了⼀个 访问⽇志⾥。因此，我准备先分析访问⽇志，看看究竟是哪些⻚⾯在“拖后腿”\n访问⽇志⽂件格式如下： ⽇志⾥记录了每次请求的路径与耗时。基于这些⽇志，我决定先写⼀个访问分析脚本，把请求数 据按路径分组，然后再依据耗时将其划为不同的性能等级，从⽽找到迫切需要优化的⻚⾯。\n基于我的设计，响应时间被分为四个性能等级。 (1) ⾮常快：⼩于 100 毫秒。 (2) 较快：100 到 300 毫秒之间。 (3) 较慢：300 毫秒到 1 秒之间。 (4) 慢：⼤于 1 秒\n代码清单 3-1⽇志分析脚本 analyzer_v1.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from enum import Enum class PagePerfLevel(str,Enum): LT_100 = \u0026#39;Less than 100ms\u0026#39; LT_300 = \u0026#39;Between 100 and 300 ms\u0026#39; LT_1000 = \u0026#39;Between 300 ms and 1s\u0026#39; GT_1000 = \u0026#39;Greater than 1s\u0026#39; def analyze_v1() path_groups = {} with open(\u0026#34;logs.txt\u0026#34;,\u0026#34;r\u0026#34;) as fp: for line in fp: path,time_costs_str = line.strip().split() # 根据页面耗时计算性能等级 time_cost = int(time_cost_str) if time_cost \u0026lt; 100: level = PagePerfLevel.LT_100 elif time_cost \u0026lt; 300: level = PagePerfLevel.LT_300 elif time_cost \u0026lt; 1000: level = PagePerfLevel.LT_1000 else: level = PagePerfLevel.GT_1000 # 如果路径第一次出现，存入初始值 if path not in path_groups: path_groups[path] = {} # 如果性能level第一次出现，存入初始值1 try: path_groups[path][level] += 1 except KeyError: path_groups[path][level] = 1 for path,result in path_groups.iems(): print(f\u0026#39;== Path:{path}\u0026#39;) total = sum(result.values()) print(f\u0026#39; Total requests:{total}\u0026#39;) print(f\u0026#39; Performance\u0026#39;) # 在输出结果前，按照“性能等级”在 PagePerfLevel ⾥⾯的顺序排列，⼩于 100 毫秒的在最前⾯ sorted_items = sorted( result.items(),key=lambda pair:list(PagePerfLevel).index(pair[0]) ) for level_name,count in sorted_items: print(f\u0026#39; - {level_name}:{count}\u0026#39;) 在上⾯的代码⾥，我⾸先在最外层定义了枚举类型 PagePerfLevel，⽤于表⽰不同的请求性 能等级，随后在 analyze_v1() 内实现了所有的主逻辑。其中的关键步骤有： (1) 遍历整个⽇志⽂件，逐⾏解析请求路径（path）与耗时（time_cost）； (2) 根据耗时计算请求属于哪个性能等级； (3) 判断请求路径是否初次出现，如果是，以⼦字典初始化 path_groups ⾥的对应值； (4) 对⼦字典的对应性能等级 key，执⾏请求数加 1 操作。 经以上步骤完成数据统计后，在输出每组路径的结果时，函数不能直接遍历 result.items()，⽽是要先参照 PagePerfLevel 枚举类按性能等级排好序，然后再输出。 在线上测试试⽤这个脚本后，我发现它可以正常分析请求、输出性能分组信息，达到了我的预 期。 不过，虽然脚本功能正常，但我总觉得它的代码写得不太好。⼀个最直观的感受是： analyze_v1() 函数⾥的逻辑特别复杂，耗时转级别、请求数累加的逻辑，全都被糅在了⼀块，整 个函数读起来很困难。 另⼀个问题是，代码⾥分布着太多零碎的字典操作，⽐如 if path not in path_groups、 try: \u0026hellip; except KeyError:，等等，看上去⾮常不利落。 于是我决定花点⼉时间重构⼀下这份脚本，解决上述两个问题。\ndefaultdict(default_factory, \u0026hellip;) 是⼀种特殊的字典类型。它在被初始化时，接收 ⼀个可调⽤对象 default_factory 作为参数。之后每次进⾏ d[key] 操作时，如果访问 的 key 不存在，defaultdict 对象会⾃动调⽤ default_factory() 并将结果作为值保 存在对应的 key ⾥。\n在 Python 中定义⼀个字典类型，可通过继承 MutableMapping 抽象类来实现，\n使用MutableMapping创建自定义字典类型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from collections.abc import MutableMapping class PerfLevelDict(MutableMapping): \u0026#34;\u0026#34;\u0026#34;⽤于存储响应时间的⾃定义字典\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.data = defaultdict(int) def __getitem__(self,key): \u0026#34;\u0026#34;\u0026#34;当某个级别不存在时，默认返回0\u0026#34;\u0026#34;\u0026#34; return self.data[self.compute_level(key)] def __setitem__(self,key,value): \u0026#34;\u0026#34;\u0026#34;将key转换为对应的性能等级，然后设置值\u0026#34;\u0026#34;\u0026#34; self.data[self.compute_level(key)] = value def __delitem__(self,key): del self.data[key] def __iter__(self): return iter(self.data) def __len__(self): return len(self.data) @staticmethod def compute_level(timg_cost_str): \u0026#34;\u0026#34;\u0026#34;根据响应时间计算性能等级\u0026#34;\u0026#34;\u0026#34; # 假如已经时性能等级，不做转换直接返回 if time_cost_str in list(PagePerfLevel): return time_cost_str time_cost = int(time_cost_str): if time_cost \u0026lt; 100: return PagePerfLevel.LT_100 elif time_cost \u0026lt; 300: return PagePerfLevel.LT_300 elif time_cost \u0026lt; 1000: return PagePerfLevel.LT_1000 return PagePerfLevel.GT_1000 在上⾯的代码中，我编写了⼀个继承了 MutableMapping 的字典类 PerfLevelDict。但 光继承还不够，要让这个类变得像字典⼀样，还需要重写包括 getitem____setitem 在内的 6 个魔法⽅法。\n我们来试⽤⼀下 PerfLevelDict 类\n有了 PerfLevelDict 类以后，我们不需要再去⼿动做“耗时→级别”转换了，⼀切都可以由 ⾃定义字典的内部逻辑处理好。 创建⾃定义字典类还带来了⼀个额外的好处。在之前的代码⾥，有许多有关字典的零碎操作， ⽐如求和、对 .items() 排序等，现在它们全都可以封装到 PerfLevelDict 类⾥，代码 逻辑不再是东⼀块、西⼀块，⽽是全部由⼀个数据类搞定\n代码重构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 from enum import Enum from collections import defaultdict from collections.abc import MutableMapping class PagePerfLevel(str,Enum): LT_100 = \u0026#39;Less than 100 ms\u0026#39; LT_300 = \u0026#39;Between 100 and 300 ms\u0026#39; LT_1000 = \u0026#39;Between 300 ms and 1 s\u0026#39; GT_1000 = \u0026#39;Greater than 1 s\u0026#39; class PerfLevelDict(MutableMapping): \u0026#34;\u0026#34;\u0026#34;存储响应时间性能等级的字典\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.data = defaultdict(int) def __getitem__(self,key): \u0026#34;\u0026#34;\u0026#34;当某个性能等级不存在时，默认返回 0\u0026#34;\u0026#34;\u0026#34; return self.data[self.compute_level(key)] def __delitem__(self,key): del self.data[key] def __iter__(self): return iter(self.data) def __len__(self): return len(self.data) def items(self): \u0026#34;\u0026#34;\u0026#34;按照顺序返回性能等级数据\u0026#34;\u0026#34;\u0026#34; return sorted( self.data.items(), key = lambda pair:list(PagePerfLevel).index(pair[0]), ) def total_request(self): \u0026#34;\u0026#34;\u0026#34;返回总请求数\u0026#34;\u0026#34;\u0026#34; return sum(self.values()) @staticmethod def compute_level(time_cost_str): \u0026#34;\u0026#34;\u0026#34;根据响应时间计算性能等级\u0026#34;\u0026#34;\u0026#34; if time_cost_str in list(PagePerfLevel): return time_cost_str time_cost = int(time_cost_str) if time_cost \u0026lt; 100: return PagePerfLevel.LT_100 elif time_cost \u0026lt; 300: return PagePerfLevel.LT_300 elif time_cost \u0026lt; 1000: return PagePerfLevel.LT_1000 return PagePerfLevel.GT_1000 def analyze_v2(): path_groups = defaultdict(PerfLevelDict) with open(\u0026#34;log.txt\u0026#34;,\u0026#34;r\u0026#34;) as fp: for line in fp: path,time_cost = line.strip().split() path_groups[path][time_cost] += 1 for path,result in path_groups.items(): print(f\u0026#39;== Path: {path}\u0026#39;) print(f\u0026#39; Total requests: {result.total_requests()}\u0026#39;) print(f\u0026#39; Performance:\u0026#39;) for level_name,count in result.items(): print(f\u0026#39; - {level_name}:{count}\u0026#39;) 在实现⾃定义字典时，我让 PerfLevelDict 继承了 collections.abc 下的MutableMapping 抽象类，⽽不是内置字典 dict。这看起来有点⼉奇怪，因为从直觉 上说，假如你想实现某个⾃定义类型，最⽅便的选择就是继承原类型。但是，如果真的继承 dict 来创建⾃定义字典类型，你会碰到很多问题。 拿⼀个最常⻅的场景来说，假如你继承了 dict，通过 setitem ⽅法重写了它的键赋值操作。此时，虽然常规的 d[key] = value ⾏为会被重写；但假如调⽤⽅使 ⽤ d.update(\u0026hellip;) 来更新字典内容，就根本不会触发重写后的键赋值逻辑。这最终会导致⾃定义类型的⾏为不⼀致。\n用按需返回替代容器 Python 3，调⽤ range(100000000) 瞬间就会返回结果。因为它不再返回列表，⽽ 是返回⼀个类型为 range 的惰性计算对象。\n当序列过⼤时，新的 range() 函数不再会⼀次性耗费⼤量内存和时间，⽣成⼀个巨⼤的列 表，⽽是仅在被迭代时按需返回数字。range() 的进化过程虽然简单，但它其实代表了⼀种重要的 编程思维——按需⽣成，⽽不是⼀次性返回。\n生成器 ⽣成器（generator）是 Python ⾥的⼀种特殊的数据类型。顾名思义，它是⼀个不断给调 ⽤⽅“⽣成”内容的类型。定义⼀个⽣成器，需要⽤到⽣成器函数与 yield 关键字。\n1 2 3 4 5 6 7 8 def generate_even(max_number): \u0026#34;\u0026#34;\u0026#34;一个简单生成器，返回 0 到 max_number 之间的所有偶数\u0026#34;\u0026#34;\u0026#34; for i in range(0,max_number): if i % 2 == 0: yield i for i in generate_even(10): print(i) 虽然都是返回结果，但 yield 和 return 的最⼤不同之处在于，return 的返回是⼀次性 的，使⽤它会直接中断整个函数执⾏，⽽ yield 可以逐步给调⽤⽅⽣成结果\n1 2 3 4 5 i = generate_even(10) next(i) \u0026gt;\u0026gt; 0 next(i) \u0026gt;\u0026gt; 2 调⽤ next() 可以逐步从⽣成器对象⾥拿到结果 因为⽣成器是可迭代对象，所以你可以使⽤ list() 等函数⽅便地把它转换为各种其他容器 类型：\n1 2 list(generate_even(10)) [0,2,4,6,8] 用生成器替代列表 1 2 3 4 5 6 7 8 9 10 11 12 def batch_process(items): \u0026#34;\u0026#34;\u0026#34; 批量处理多个items对象 \u0026#34;\u0026#34;\u0026#34; # 初始化空结果列表 results = [] for item in items: # 处理 item，可能需要耗费⼤量时间…… # processed_item = ... results.append(processed_item) # 将拼装后的结果列表返回 return results 这样的函数遵循同⼀种模式：初始化容器-\u0026gt;处理-\u0026gt;将结果存入容器-\u0026gt;返回容器\n⼀个问题是，如果需要处理的对象 items 过⼤，batch_process() 函数就会像 Python 2 ⾥的 range() 函数⼀样，每次执⾏都特别慢，存放结果的对象 results 也会占⽤⼤量 内存。 另⼀个问题是，如果函数调⽤⽅想在某个 processed_item 对象满⾜特定条件时中断，不再 继续处理后⾯的对象，现在的 batch_process() 函数也做不到——它每次都得⼀次性处理完 所有 items 才会返回。\n1 2 3 4 for processed_item in batch_process(items): # 如果某个已处理对象过期了，就中断当前的所有处理 if processed_item.has_expired(): break 避开列表的性能陷阱 可以看到，同样是构建⼀个⻓度为 5000 的列表，不断往头部插⼊的 insert ⽅式的耗时是 从尾部追加的 append ⽅式的 16 倍还多。\n这个性能差距与列表的底层实现有关。Python 在实现列表时，底层使⽤了数组（array）数 据结构。这种结构最⼤的⼀个特点是，当你在数组中间插⼊新成员时，该成员之后的其他成员 都需要移动位置，该操作的平均时间复杂度是 O(n)。因此，在列表的头部插⼊成员，⽐在尾 部追加要慢得多（后者的时间复杂度为 O(1)）。 如果你经常需要往列表头部插⼊数据，请考虑使⽤ collections.deque 类型来替代列表 （代码如下）。因为 deque 底层使⽤了双端队列，⽆论在头部还是尾部追加成员，时间复杂 度都是 O(1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 from collections import deque def deque_append(): \u0026#34;\u0026#34;\u0026#34;不断往尾部追加\u0026#34;\u0026#34;\u0026#34; l = deque() for i in range(5000): l.append(i) def deque_appendleft(): \u0026#34;\u0026#34;\u0026#34;不断往头部插入\u0026#34;\u0026#34;\u0026#34; l = deque() for i in range(5000): l.appendleft(i) 因为列表在底层使⽤了数组结构，所以要判断某个成员是否存在，唯⼀的办法是从前往后遍历 所有成员，执⾏该操作的时间复杂度是 O(n)。如果列表内容很多，这种 in 操作耗时就会很 久。 对于这类判断成员是否存在的场景，我们有更好的选择。\n使用集合判断成员是否存在 要判断某个容器是否包含特定成员，⽤集合⽐⽤列表更合适\n⽽在集合⾥搜索，就像通过字典查字。我们先按照字的拼⾳从索引找到它所在的⻚码，然后直 接翻到那⼀⻚。完成这种操作需要的时间复杂度是 O(1)。 在集合⾥搜索之所以这么快，是因为其底层使⽤了哈希表数据结构。要判断集合中是否存在某 个对象 obj，Python 只需先⽤ hash(obj) 算出它的哈希值，然后直接去哈希表对应位置 检查 obj 是否存在即可，根本不需要关⼼哈希表的其他部分，⼀步到位\n除了集合，对字典进⾏ key in \u0026hellip; 查询同样⾮常快，因为⼆者都是基于哈希表结构实现 的。\n要实现合并功能，需要⽤到双星号 ** 运算符来做解包操作。在字典中使⽤ **dict_obj 表 达式，可以动态解包 dict_obj 字典的所有内容，并与当前字典合并：\n因为解包过程会默认进⾏浅拷⻉操作，所以我们可以⽤它⽅便地合并两个字典\n除了使⽤ ** 解包字典，你还可以使⽤单星号 * 运算符来解包任何可迭代对象：\n字典的I运算符 字典类型新增了对 | 运算符的⽀持。只要执⾏ d1 | d2，就能快速拿到两个字典合并后的结果：\n使用有序字典去重 前⾯提到过，集合⾥的成员不会重复，因此它经常⽤来去重。但是，使⽤集合去重有⼀个很⼤的 缺点：得到的结果会丢失集合内成员原有的顺序：\n这种⽆序性是由集合所使⽤的哈希表结构所决定的，⽆法避免。如果你既需要去重，⼜想要保留 原有顺序，怎么办？可以使⽤前⽂提到过的有序字典 OrderedDict 来完成这件事。因为 OrderedDict 同时满⾜两个条件： (1) 它的键是有序的； (2) 它的键绝对不会重复。 因此，只要根据列表构建⼀个字典，字典的所有键就是有序去重的结果\n1 2 3 from collections import OrderedDict list(OrderedDict.fromkeys(nums).keys()) \u0026gt;\u0026gt;\u0026gt; [10,2,3,21] 调⽤ fromkeys ⽅法会创建⼀个有序字典对象。字典的键来⾃⽅法的第⼀个参数：可迭代 对象（此处为 nums 列表），字典的值默认为 None\n别在遍历列表是同步修改 1 2 3 4 5 6 7 def remove_even(numbers): \u0026#34;\u0026#34;\u0026#34;去掉列表里所有的偶数\u0026#34;\u0026#34;\u0026#34; for number in numbers: if number % 2 == 0: numbers.remove(number) \u0026gt;\u0026gt;\u0026gt; [1,7,8,11] 注意到那个本不该出现的数字 8 了吗？遍历列表的同时修改列表就会发⽣这样的怪事。 之所以会出现这样的结果，是因为：在遍历过程中，循环所使⽤的索引值不断增加，⽽被遍历对 象 numbers ⾥的成员⼜同时在被删除，⻓度不断缩短——这最终导致列表⾥的⼀些成员其实根本就 没被遍历到。 因此，要修改列表，请不要在遍历时直接修改。只需选择启⽤⼀个新列表来保存修改后的成员， 就不会碰到这种奇怪的问题。\n别写太复杂的推导式 别把推导式当作代码量更少的循环 但这样做其实并不合适。推导式的核⼼意义在于它会返回值——⼀个全新构建的列表，如果你不 需要这个新列表，就失去了使⽤表达式的意义。\n让函数返回NamedTuple 对于这种未来可能会变动的多返回值函数来说，如果⼀开始就使⽤ NamedTuple 类型对返回结 果进⾏建模，上⾯的改动会变得简单许多：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from typing import NamedTuple class Address(NamedTuple): \u0026#34;\u0026#34;\u0026#34;地址信息结果\u0026#34;\u0026#34;\u0026#34; country:str province:str city:str def latlon_to_address(lat,lon): return Address( country=country, province=province, city=city, ) addr = latlon_to_address(lat,lon) # 通过属性名来使用addr addr.counrty/addr.province/addr.city 假如我们在 Address ⾥增加了新的返回值 district，已有的函数调⽤代码也不⽤进⾏任何 适配性修改，因为函数结果只是多了⼀个新属性，没有任何破坏性影响\n总结 基础知识 在进⾏函数调⽤时，传递的不是变量的值或者引⽤，⽽是变量所指对象的引⽤ Python 内置类型分为可变与不可变两种，可变性会影响⼀些操作的⾏为，⽐如 += 对于可变类型，必要时对其进⾏拷⻉操作，能避免产⽣意料之外的影响 常⻅的浅拷⻉⽅式：copy.copy、推导式、切⽚操作 使⽤ copy.deepcopy 可以进⾏深拷⻉操作 列表与元组 使⽤ enumerate 可以在遍历列表的同时获取下标 函数的多返回值其实是⼀个元组 不存在元组推导式，但可以使⽤ tuple 来将⽣成器表达式转换为元组 元组经常⽤来表⽰⼀些结构化的数据 字典与集合 在 Python 3.7 版本前，字典类型是⽆序的，之后变为保留数据的插⼊顺序 使⽤ OrderedDict 可以在 Python 3.7 以前的版本⾥获得有序字典 只有可哈希的对象才能存⼊集合，或者作为字典的键使⽤ 使⽤有序字典 OrderedDict 可以快速实现有序去重 使⽤ fronzenset 可以获得⼀个不可变的集合对象 集合可以⽅便地进⾏集合运算，计算交集、并集 不要通过继承 dict 来创建⾃定义字典类型\\ 代码可读性技巧 具名元组⽐普通元组可读性更强 列表推导式可以更快速地完成遍历、过滤、处理以及构建新列表操作 不要编写过于复杂的推导式，⽤朴实的代码替代就好 不要把推导式当作代码量更少的循环，写普通循环就好 代码可维护性技巧 当访问的字典键不存在时，可以选择捕获异常或先做判断，优先推荐捕获异常 使⽤ get、setdefault、带参数的 pop ⽅法可以简化边界处理逻辑 使⽤具名元组作为返回值，⽐普通元组更好扩展 当字典键不存在时，使⽤ defaultdict 可以简化处理 继承 MutableMapping 可以⽅便地创建⾃定义字典类，封装处理逻辑 ⽤⽣成器按需返回成员，⽐直接返回⼀个结果列表更灵活，也更省内存 使⽤动态解包语法可以⽅便地合并字典 不要在遍历列表的同时修改，否则会出现不可预期的结果 代码性能要点 列表的底层实现决定了它的头部操作很慢，deque 类型则没有这个问题 当需要判断某个成员在容器中是否存在时，使⽤字典 / 集合更快 条件分支控制流 当我们编写分⽀时，第⼀件要注意的事情，就是不要显式地和布尔值做⽐较\n绝⼤多数情况下，在分⽀判断语句⾥写 == True 都没有必要，删掉它代码会更短也更易读。 但这条原则也有例外，⽐如你确实想让分⽀仅当值是 True 时才执⾏。不过即便这样，写 if == True 仍然是有问题的，我会在 4.1.3 节解释这⼀点。\n省略0值判断 这种判断语句其实可以变得更简单，因为当某个对象作为主⻆出现在 if 分⽀⾥时，解释器会 主动对它进⾏“真值测试”，也就是调⽤ bool() 函数获取它的布尔值。⽽在计算布尔值时， 每类对象都有着各⾃的规则，⽐如整型和列表的规则如下：\n布尔值为假：None、0、False、[]、()、{}、set()、frozenset()，等等。 布尔值为真：⾮ 0 的数值、True，⾮空的序列、元组、字典，⽤⼾定义的类和实例，等 等。 把否定逻辑移入表达式内 尽可能让三元表达式保持简单 修改对象的布尔值 但其实，上⾯的分⽀判断语句可以变得更简单。只要给 UserCollection 类实现 len 魔法⽅法，users 对象就可以直接⽤于“真值测试”\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class UserCollection: \u0026#34;\u0026#34;\u0026#34;用于保存多个用户的集合工具类\u0026#34;\u0026#34;\u0026#34; def __init__(self,users): self.items = users def __len__(self): return len(self.items) users = UserCollection([\u0026#39;piglei\u0026#39;,\u0026#39;raymond\u0026#39;]) # 不再需要手动判断对象内部items的长度 if users: print(\u0026#34;There\u0026#39;s some users in collection!\u0026#34;) 为对象定义 bool ⽅法后，对它进⾏布尔值运算会直接返回该⽅法的调⽤结果。举个例 ⼦\n1 2 3 4 5 6 7 8 class ScoreJudger: \u0026#34;\u0026#34;\u0026#34;仅当分数大于60时为真\u0026#34;\u0026#34;\u0026#34; def __init__(self,score): self.score = score def __bool__(self): return self.score \u0026gt;= 60 假如⼀个类同时定义了 len 和 bool 两个⽅法，解释器会优先使⽤ bool ⽅法的执⾏结果。\n与None比较时使用is运算符 1 2 3 4 5 6 7 class EqualWithAnything: \u0026#34;\u0026#34;\u0026#34;与任何对象相等\u0026#34;\u0026#34;\u0026#34; def __eq__(self,other): # ⽅法⾥的 other ⽅法代表 == 操作时右边的对象，⽐如 # x == y 会调⽤ x 的 __eq__ ⽅法，other 的参数为 y return True 上⾯定义的 EqualWithAnything 对象，在和任何东西做 == 计算时都会返回 True：\n既然 == 的⾏为可被魔法⽅法改变，那我们如何严格检查某个对象是否为 None 呢？答案是使 ⽤ is 运算符。虽然⼆者看上去差不多，但有着本质上的区别： (1) == 对⽐两个对象的值是否相等，⾏为可被 eq ⽅法重载； (2) is 判断两个对象是否是内存⾥的同⼀个东西，⽆法被重载。\n因此，当你想要判断某个对象是否为 None 时，应该使⽤ is 运算符：\n这是因为，除了 None、True 和 False 这三个内置对象以外，其他类型的对象在 Python 中并不是严格以单例模式存在的。换句话说，即便值⼀致，它们在内存中仍然是完全不同的两个对 象\n因此，仅当你需要判断某个对象是否是 None、True、False 时，使⽤ is，其他情况下，请 使⽤ ==。\n令人迷惑的整型驻留技术 为什么会这样？这是因为 Python 语⾔使⽤了⼀种名为“整型驻留”（integer interning）的底层优化技术。 对于从 -5 到 256 的这些常⽤⼩整数，Python 会将它们缓存在内存⾥的⼀个数组中。 当你的程序需要⽤到这些数字时，Python 不会创建任何新的整型对象，⽽是会返回缓存中的对 象。这样能为程序节约可观的内存。\n除了整型以外，Python 对字符串也有类似的“驻留”操作。如果你对这⽅⾯感兴趣，可⾃ ⾏搜索“Python integer/string interning”关键字了解更多内容\n案例 现在的电影数据是字典（dict）格式的，处理起来不是很⽅便。于是，我⾸先创建了⼀个类： Movie，⽤来存放与电影数据和封装电影有关的操作。有了 Movie 类后，我在⾥⾯定义了 rank 属性对象，并在 rank 内实现了按评分计算级别的逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Movie: \u0026#34;\u0026#34;\u0026#34;电源对象数据类\u0026#34;\u0026#34;\u0026#34; def __init__(slef,name,year,rating): self.name = name self.year = year self.rating = rating @property def rank(self): \u0026#34;\u0026#34;\u0026#34;按照评分对电影分级 - S: 8.5 分及以上 - A：8 ~ 8.5 分 - B：7 ~ 8 分 - C：6 ~ 7 分 - D：6 分以下 \u0026#34;\u0026#34;\u0026#34; rating_num = floay(self.rating) if rating_num \u0026gt;= 8.5: return \u0026#39;S\u0026#39; elif rating_num \u0026gt;= 8: return \u0026#39;A\u0026#39; elif rating_num \u0026gt;= 7: return \u0026#39;B\u0026#39; elif rating_num \u0026gt;= 6: return \u0026#39;C\u0026#39; else: return \u0026#39;D\u0026#39; 对电影列表排序，这件事乍听上去很难，但好在 Python 为我们提供了⼀个好⽤的内置函数： sorted()。借助它，我可以很便捷地完成排序操作。我新建了⼀个名为 get_sorted_movies() 的排序函数，它接收两个参数：电影列表（movies）和排序选项（sorting_type），返回排序后 的电影列表作为结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_sorted_movies(movies,sorting_type): \u0026#34;\u0026#34;\u0026#34;对电影列表进行排序并返回 :param movies:Movie 对象列表 :param sorting_type:排序选项，可选值 name（名称）、rating（评分）、year（年份）、random（随机乱序） \u0026#34;\u0026#34;\u0026#34; if sorting_type == \u0026#39;name\u0026#39;: sorted_movies = sorted(movies,key=lambda movie:movie.name.lower()) elif sorting_type == \u0026#39;rating\u0026#39;: sorted_movies = sorted(movies,key=lambda movie:float(movie.rating),reverse=True) elif sorting_type == \u0026#39;year\u0026#39;: sorted_movies = sorted( movies,key=lambda movie:movie.year,reverse=True ) elif sorting_type == \u0026#39;random\u0026#39;: sorted_movies = sorted(movies,key=lambda movie:random.random()) else: raise RuntimeError(f\u0026#39;Unkown sorting type:{sorting_type}\u0026#39;) return sorted_movies 为了把上⾯这些代码串起来，我在 main() 函数⾥实现了接收排序选项、解析电影数据、排序 并打印电影列表等功能\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def main(): # 接收用户输入的排序选项 sorting_type = input(\u0026#39;Please input sorting type\u0026#39;) if sorting_type not in all_sorting_types: print( \u0026#39;Sorry, \u0026#34;{}\u0026#34; is not a valid sorting type, please choose from \u0026#39; \u0026#39;\u0026#34;{}\u0026#34;, exit now\u0026#39;.format( sorting_type, \u0026#39;/\u0026#39;.join(all_sorting_types), ) ) return # 初始化电影数据对象 movie_items = [] for movie_json in movies: movie = Movie(**movie_json) movie_items.append(movie) # 排序并输出电影列表 sorted_movies = get_sorted_movies(movie_items,sorting_type) for movie in sorted_movies: print(f\u0026#39;- [{movie,rank}] {movie.name} ({movie.year}) | rating: {movie.rating}\u0026#39;) 看上去还不错，对吧？只要短短的 100 ⾏不到的代码，⼀个⼩⼯具就完成了。不过，虽然这个⼯具实现了我最初设想的功能，在它的源码⾥却藏着两⼤段可以简化的条件分⽀代码。如果使⽤恰当的⽅式，这些分⽀语句可以彻底从代码中消失。\n使用bisect优化范围类分支判断 第一个需要优化的分支，藏在Movie类的rank方法属性中\n1 2 3 4 5 6 7 8 9 10 11 12 13 @property def rank(self): rating_num = float(self.rating) if rating_num \u0026gt;= 8.5: return \u0026#39;S\u0026#39; elif rating_num \u0026gt;= 8: return \u0026#39;A\u0026#39; elif rating_num \u0026gt;= 7: return \u0026#39;B\u0026#39; elif rating_num \u0026gt;= 6: return \u0026#39;C\u0026#39; else: return \u0026#39;D\u0026#39; 有优化这段代码，我们得先把所有分界点收集起来，放在一个元组里\n1 2 # 已经排好序的评级分界点 breakpoints = (6,7,8,8.5) 接下来要做的事，就是根据rating的值，判断它在breakpoints里的位置\n要实现这个功能，最直接的做法是编写⼀个循环——通过遍历元组 breakpoints ⾥的所有分界点，我们就能找到 rating 在其中的位置。但除此之外，其实还有更简单的办法。因为breakpoints 已经是⼀个排好序的元组，所以我们可以直接使⽤ bisect 模块来实现查找功能\nbisect 是 Python 内置的⼆分算法模块，它有⼀个同名函数 bisect，可以⽤来在有序列表⾥做⼆分查找\n1 2 3 4 5 6 7 import bisect # 注意：用来做二分查找的容器必须事已经排好序的 breakpoints = [10,20,30] # bisect函数会返回值在列表中的位置，0代表相应的值位于第一个元素10之前 bisect.bisect(breakpoints,1) 0 将分界点定义成元组，并引⼊ bisect 模块后，之前的⼗⼏⾏分⽀代码可以简化成下⾯这 样：\n1 2 3 4 5 6 7 8 9 @property def rank(self): # 已经排序号的评级分界点 breakpoints = (6,7,8,8.5) # 各评分区间级别名 grades = (\u0026#39;D\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;A\u0026#39;,\u0026#39;S\u0026#39;) index = bisect.bisect(breakpoints,float(slef.rating)) return grades[index] 优化完 rank ⽅法后，程序中还有另⼀段待优化的条件分⽀代码——get_sorted_movies() 函数⾥的排序⽅式选择逻辑\n使用字典优化分支代码 在 get_sorted_movies() 函数⾥，同样有⼀⼤段条件分⽀代码。它们负责根据 sorting_type 的值，为函数选择不同的排序⽅式：\n1 2 3 4 5 6 7 8 9 10 11 12 def get_sorted_movies(movies,sorting_type): if sorting_type == \u0026#39;name\u0026#39;: sorted_movies = sorted(movies,key=lambda movie:movie.name.lower()) elif sorting_type == \u0026#39;rating\u0026#39;: sorted_movies = sorted(movies,key=lambda movie:float(movie.rating),reverse=True) elif sorting_type == \u0026#39;year\u0026#39;: sorted_movies = sorted(movies,key=lambda movie:movie.year,reserve=True) elif sorting_type == \u0026#39;random\u0026#39;: sorted_type = sorted(movies,key=lambda movie:random.random()) else: raise RuntimeError(f\u0026#39;Unknown sorting type:{sorting_type}\u0026#39;) return sorted_movies 这段代码有两个⾮常明显的特点。 (1) 它⽤到的条件表达式都⾮常类似，都是对 sorting_type 做等值判断（sorting_type == \u0026rsquo;name\u0026rsquo;）。 (2) 它的每个分⽀的内部逻辑也⼤同⼩异——都是调⽤ sorted() 函数，只是 key 和reverse 参数略有不同\n如果⼀段条件分⽀代码同时满⾜这两个特点，我们就可以⽤字典类型来简化它。因为 Python的字典可以装下任何对象，所以我们可以把各个分⽀下不同的东西——排序的 key 函数和 reverse 参数，直接放进字典⾥\n1 2 3 4 5 6 7 sorting_algos = { # sorting_type:(key_func,reverse) \u0026#39;name\u0026#39;:(lambda movie:movie.name.lower(),False) \u0026#39;rating\u0026#39;:(lambda movie:float(movie.rating),True) \u0026#39;year\u0026#39;:(lambda movie:movie.year,True) \u0026#39;random\u0026#39;:(lambda movie:random.random(),False) } 有了这份字典以后，我们的 get_sorted_movies() 函数就可以改写成下⾯这样:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def get_sorted_movies(movies,sorting_type): \u0026#34;\u0026#34;\u0026#34;对电影列表进行排序并返回 :param movie:Movie 对象列表 :param sorting_type 排序选项，可选值 name(名称),rating(评分)、year（年份）、random（随机乱序） \u0026#34;\u0026#34;\u0026#34; sorting_algos = { # sorting_type:(key_func,reverse) \u0026#39;name\u0026#39;:(lambda movie:movie.name.lower(),False), \u0026#39;rating\u0026#39;:(lambda movie:float(movie.rating),True), \u0026#39;year\u0026#39;:(lambda movie:movie_year,True), \u0026#39;random\u0026#39;:(lambda movie:random.random(),False) } try: key_func,reverse = sorting_algos[sorting_type] except KeyError: raise RuntimeError(f\u0026#39;Unknown sorting type: {sorting_type}\u0026#39;) sorted_movies = sorted(movies,key=key_func,reverse=reverse) return sorted_movie 相⽐之前的⼤段 if/elif，新代码变得整⻬了许多，扩展性也更强。如果要增加新的排序算 法，我们只需要在 sorting_algos 字典⾥增加新成员即可\n优化成果 通过引入bisect模块和算法字典，案例开头的⼩⼯具代码最终优化成了代码清单 4-5\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import bisect import random class Movie: \u0026#34;\u0026#34;\u0026#34;电影对象数据类\u0026#34;\u0026#34;\u0026#34; def __init__(self,name,year,rating): self.name = name self.year = year self.rating = rating @property def rank(self): \u0026#34;\u0026#34;\u0026#34; 按照评分对电影分级 \u0026#34;\u0026#34;\u0026#34; # 已经排序好的评级分界点 breakpoints = (6,7,8,8.5) # 各评分区间级别名 grades = (\u0026#39;D\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;A\u0026#39;,\u0026#39;S\u0026#39;) index = bisect.bisect(breakpoints,float(self.rating)) return grades[index] def get_sorted_movies(movies,sorting_type): \u0026#34;\u0026#34;\u0026#34;对电影列表进⾏排序并返回 :param movies: Movie 对象列表 :param sorting_type: 排序选项，可选值 name（名称）、rating（评分）、year（年份）、random（随机乱序） \u0026#34;\u0026#34;\u0026#34; sorting_algos = { # sorting_type: (key_func, reverse) \u0026#39;name\u0026#39;:(lambda movie:movie.name.lower(),False) \u0026#39;rating\u0026#39;:(lambda movie:float(movie.rating),True) \u0026#39;year\u0026#39;:(lambda movie:movie.year,True) \u0026#39;random\u0026#39;:(lambda movie:random.random(),False) } try: key_func,reverse = sorting_algos[sorting_type] except KeyError: raise RuntimeError(f\u0026#39;Unknown sorting type: {sorting_type}\u0026#39;) sorted_movies = sorted(movies,key=key_func,reverse=reverse) return sorted_movies 在这个案例中，我们⼀共⽤到了两种优化分⽀的⽅法。虽然它们看上去不太⼀样，但代表的思想其实是类似的。当我们编写代码时，有时会下意识地编写⼀段段⼤同⼩异的条件分⽀语句。多数情况下，它们 只是对业务逻辑的⼀种“直译”，是我们对业务逻辑的理解尚处在第⼀层的某种拙劣表现。如果进⼀步深⼊业务逻辑，尝试从中总结规律，那么这些条件分⽀代码也许就可以被另⼀种更 精简、更易扩展的⽅式替代。当你在编写条件分⽀时，请多多思考这些分⽀背后所代表的深层需求，寻找简化它们的办法，进⽽写出更好的代码。\n尽量避免多层分支嵌套 幸运的是，这些多层嵌套可以⽤⼀个简单的技巧来优化——“提前返回”。“提前返回”指的是：当你在编写分⽀时，⾸先找到那些会中断执⾏的条件，把它们移到函数的最前⾯，然后在分⽀⾥直接使⽤ return 或 raise 结束执⾏。\n使⽤这个技巧，前⾯的代码可以优化成下⾯这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 def buy_fruit(nerd,store): if not store.is_open(): raise MadAtNoFruit(\u0026#34;store is closed\u0026#34;) if not store.has_stocks(\u0026#34;apple\u0026#34;) raise MadAtNoFruit(\u0026#34;no apple in store!\u0026#34;) if nerd.can_afford(store.price(\u0026#34;apple\u0026#34;,amount=1)): nerd.buy(store,\u0026#34;apple\u0026#34;,amount=1) return else: nerd.go_home_and_get_money() return buy_fruit(nerd,store) 实践“提前返回”后，buy_fruit() 函数变得更扁平了，整个逻辑也变得更直接、更容易理解了\n在“Python 之禅”⾥有⼀句：“扁平优于嵌套”（Flat is better than nested），这刚好说明了把嵌套分⽀改为扁平的重要性\n别写太复杂的条件表达式 针对这种代码，我们需要对条件表达式进⾏简化，把它们封装成函数或者对应的类⽅法，这样才能提升分⽀代码的可读性\n尽量降低分支内代码的相似性 我们可以把重复代码移到分⽀外，尽量降低分⽀内代码的相似性：\n像上⾯这种重复的语句很容易发现，下⾯是⼀个隐蔽性更强的例⼦：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 创建或更新⽤⼾资料数据 # 如果是新⽤⼾，创建新 Profile 数据，否则更新已有数据 if user.no_profile_exists: create_user_profile( username=data.username, gender=data.gender, email=data.email, age=data.age, address=data.address, points=0, created=now(), ) else: update_user_profile( username=data.username, gender=data.gender, email=data.email, age=data.age, address=data.address, updated=now(), ) 在上⾯这段代码⾥，我们可以⼀眼看出，程序在两个分⽀下调⽤了不同的函数，做了不⼀样的事情。但因为那些重复的函数参数，我们很难⼀下看出⼆者的核⼼不同点到底是什么.为了降低这种相似性，我们可以使⽤ Python 函数的动态关键字参数（**kwargs）特性，简单优化⼀下上⾯的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 if user.no_profile_exists: _update_or_create = create_user_profile extra_args = {\u0026#39;points\u0026#39;:0,\u0026#39;created\u0026#39;:now()} else: _update_or_created = update_user_profile extra_args = {\u0026#39;updated\u0026#39;:now()} _update_or_create( username=user.name, gender=user.gender, email=user.email, age=user.age, address=user.address, **extra_args, ) 降低不同分⽀内代码的相似性，可以帮助读者更快地领会它们之间的差异，进⽽更容易理解分⽀的存在意义。\n使用德摩根定律 相⽐之前，新代码少了⼀个 not 关键字，变得好理解了不少。当你的代码出现太多“否定”时，请尝试⽤“德摩根定律”来化繁为简吧。\n使用all()/any()函数构建条件表达式 留意and和or的运算优先级 避开or运算符的陷阱 or 运算符是构建逻辑表达式时的常客。or 最有趣的地⽅是它的“短路求值”特性。⽐如在下⾯的例⼦⾥，1 / 0 永远不会被执⾏，也就意味着不会抛出 ZeroDivisionError 异常：\n因为 a or b or c or \u0026hellip; 这样的表达式，会返回这些变量⾥第⼀个布尔值为真的对象，直到最末⼀个为⽌，所以 extra_context or {} 表达式在对象不为空时就是 extra_context、⾃⾝，⽽当 extra_context 为 None 时就变成 {}。\n总结 条件分支语句惯用写法 不要显式地和布尔值作比较 利用类型本身的布尔值规则，省略0值判断 把not代表的否定逻辑移入表达式内部 仅在需要判断某个对象是否是None，True，False时，使用is运算符 python数据模型 定义__len__和__bool__魔术方法，可以自定义对象的布尔值规则 定义__eq__方法，可以修改对象在进行 == 运算时的行为 代码可读性技巧 不同分⽀内容易出现重复或类似的代码，把它们抽到分⽀外可提升代码的可读性 使⽤“德摩根定律”可以让有多重否定的表达式变得更容易理解 代码可维护性技巧 尽可能让三元表达式保持简单 扁平优于嵌套：使⽤“提前返回”优化代码⾥的多层分⽀嵌套 当条件表达式变得特别复杂时，可以尝试封装新的函数和⽅法来简化 and 的优先级⽐ or ⾼，不要忘记使⽤括号来让逻辑更清晰 在使⽤ or 运算符替代条件分⽀时，请注意避开因布尔值运算导致的陷阱 代码组织技巧 bisect模块可以用来优化范围类分支判断 字典类型可以用来替代简单的条件分支语句 尝试总结条件分支代码里的规律，用更精简，更易扩展的方式改写它们 使⽤ any() 和 all() 内置函数可以让条件表达式变得更精简 异常与错误处理 基础知识 优先使用异常捕获 ⼀种通⽤的编程⻛格：LBYL（look before you leap）。LBYL 常被翻译成“三思⽽后⾏”。通俗点⼉说，就是在执⾏⼀个可能会出错的操作时，先做⼀些关键的条件判断，仅当条件满⾜时才进⾏操作。\n获取原谅比许可简单 EAFP“获取原谅⽐许可简单”是⼀种和 LBYL“三思⽽后⾏”截然不同的编程⻛格。\n在 Python 世界⾥，EAFP 指不做任何事前检查，直接执⾏操作，但在外层⽤ try 来捕获可能发⽣的异常。如果还⽤下⾬举例，这种做法类似于“出⻔前不看天⽓预报，如果淋⾬了，就回家后洗澡吃感冒药”。\n和 LBYL 相⽐，EAFP 编程⻛格更为简单直接，它总是直奔主流程⽽去，把意外情况都放在异常处理 try/except 块内消化掉。\n如果你问我：这两种编程⻛格哪个更好？我只能说，整个 Python 社区明显偏爱基于异常捕获的 EAFP⻛格。这⾥⾯的原因很多。⼀个显⽽易⻅的原因是，EAFP ⻛格的代码通常会更精简。因为它不要求开发者⽤分⽀完全覆盖 各种可能出错的情况，只需要捕获可能发⽣的异常即可。另外，EAFP ⻛格的代码通常性能也更好。⽐如在这个例⼦⾥，假如你每次都⽤字符串 \u0026lsquo;73\u0026rsquo; 来调⽤函数，这两种⻛格的代码在操作流程上会有如下区别\ntry语句常用知识 把更精确的except语句放在前面 使用else分支 异常捕获语句⾥的 else 表⽰：仅当 try 语句块⾥没抛出任何异常时，才执⾏ else 分⽀下的内容，效果就像在 try 最后增加⼀个标记变量⼀样。\n使用空raise语句 当⼀个空 raise 语句出现在 except 块⾥时，它会原封不动地重新抛出当前异常\n抛出异常，而不是返回错误 我们知道，Python ⾥的函数可以⼀次返回多个值（通过返回⼀个元组实现）。所以，当我们要表明函数执⾏出错时，可以让它同时返回结果与错误信息。\n使用上下文管理器 with 是⼀个神奇的关键字，它可以在代码中开辟⼀段由它管理的上下⽂，并控制程序在进⼊和退出这段上下⽂时的⾏为。⽐如在上⾯的代码⾥，这段上下⽂所附加的主要⾏为就是：进⼊时打开某 个⽂件并返回⽂件对象，退出时关闭该⽂件对象。并⾮所有对象都能像 open(\u0026lsquo;foo.txt\u0026rsquo;) ⼀样配合 with 使⽤，只有满⾜上下⽂管理器（context manager）协议的对象才⾏。\n上下⽂管理器是⼀种定义了“进⼊”和“退出”动作的特殊对象。要创建⼀个上下⽂管理器，只要实现 enter 和 exit 两个魔法⽅法即可。\n下⾯这段代码实现了⼀个简单的上下⽂管理器：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class DummyContext: def __init__(self,name): self.name = name def __enter__(self): # __enter__ 会在进入管理器时被调用，同时可以返回结果 # 这个结果可以通过as关键字被调用方获取 # 此处返回一个增加了随机后缀的name return f\u0026#39;{self.name}-{random.random()}\u0026#39; def __exit__(self,exc_type,exc_val,exc_tb): # __exit__ 会在退出管理器被调用 print(\u0026#39;Exiting DummyContext\u0026#39;) return False 用于替代finally语句清理资源 因此，我们完全可以⽤上下⽂管理器来替代 finally 语句。做起来很简单，只要在 exit ⾥增加需要的回收语句即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 class create_conn_obj: \u0026#34;\u0026#34;\u0026#34;创建连接对象，并在退出上下⽂时⾃动关闭\u0026#34;\u0026#34;\u0026#34; def __init__(self,host,port,timeout=None): self.conn = create_conn(host,port,timeout=timeout) def __enter__(self): return self.conn def __exit__(self,exc_type,exc_value,traceback): # __exit__会在管理器退出时调用 self.conn.close() return False 使⽤ create_conn_obj 可以创建会⾃动关闭的连接对象：\n1 2 3 4 5 with create_conn_obj(host,port,timeout=None) as conn: try: conn.send_text(\u0026#39;Hello,world\u0026#39;) except Exception as e: print(f\u0026#39;Unable to use connection: {e}\u0026#39;) 除了回收资源外，你还可以⽤ exit ⽅法做许多其他事情，⽐如对异常进⾏⼆次处理后重新抛出，⼜⽐如忽略某种异常，等等\n用于忽略异常 如果使⽤上下⽂管理器，我们可以很⽅便地实现可复⽤的“忽略异常”功能——只要在 exit ⽅法⾥稍微写⼏⾏代码就⾏\n1 2 3 4 5 6 7 8 9 10 class ignore_closed: \u0026#34;\u0026#34;\u0026#34;忽略已经关闭的连接\u0026#34;\u0026#34;\u0026#34; def __enter__(self): pass def __exit__(self,exc_type,exc_value,traceback): if exc_type == AlreadyClosedError: return True return False 当你想忽略 AlreadyClosedError 异常时，只要把代码⽤ with 语句包裹起来即可：\n1 2 with ignore_closed(): close_conn(conn) 通过 with 实现的“忽略异常”功能，主要利⽤了上下⽂管理器的 exit ⽅法。exit 接收三个参数：exc_type、exc_value 和 traceback。\n如果你在真实项⽬中要忽略某类异常，可以直接使⽤标准库模块 contextlib ⾥的 suppress 函数，它提供了现成的“忽略异常”功能\n使用contextmanager装饰器 虽然上下⽂管理器很好⽤，但定义⼀个符合协议的管理器对象其实挺⿇烦的——得⾸先创建⼀个类，然后实现好⼏个魔法⽅法。为了简化这部分⼯作，Python 提供了⼀个⾮常好⽤的⼯具：@contextmanager 装饰器\n@contextmanager位于内置模块contextlib下，它可以把任何一个生成器函数直接转换为一个上下文管理器\n举个例⼦，我在前⾯实现的⾃动关闭连接的 create_conn_obj 上下⽂管理器，假如⽤函数 来改写，可以简化成下⾯这样：\n1 2 3 4 5 6 7 8 9 10 from contextlib import contextmanager @contextmanager def create_conn_obj(host,port,timeout=None): \u0026#34;\u0026#34;\u0026#34;创建连接对象，并在退出上下文时自动关闭\u0026#34;\u0026#34;\u0026#34; conn = create_conn(host,port,timeout=timeout) try: yield conn finally: conn.close() 以 yield 关键字为界，yield 前的逻辑会在进⼊管理器时执⾏（类似于 enter），yield 后的逻辑会在退出管理器时执⾏（类似于 exit）\n如果要在上下⽂管理器内处理异常，必须⽤ try 语句块包裹 yield 语句\n在⽇常⼯作中，我们⽤到的⼤多数上下⽂管理器，可以直接通过“⽣成器函数 + @contextmanager”的⽅式来定义，这⽐创建⼀个符合协议的类要简单得多。\n异常捕获不是在拿着捕⾍⽹玩捕⾍游戏，谁捕的⾍⼦多谁就获胜。弄⼀个庞⼤的 try 语句，把所有可能出错、不可能出错的代码，⼀股脑⼉地全部⽤ except Exception：包起来，显然是不妥当的。\n“Python 之禅”⾥也提到了这个建议：“除⾮有意静默，否则不要⽆故忽视异常。” （Errors should never pass silently. Unless explicitly silenced.）\n在数据校验这块，pydantic 模块是⼀个不错的选择。\n在编写代码时，我们应当尽量避免⼿动校验任何数据。因为数据校验任务独⽴性很强，所以应该引⼊合适的第三⽅校验模块（或者⾃⼰实现），让它们来处理这部分专业⼯作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pydantic import BaseModel,coint,ValidationError class NumberInput(BaseModel): # 使用类型注解conint定义number属性的取值范围 number:conint(ge=0,le=100) def input_a_number_with_pydantic(): while True: number = input(\u0026#39;Please input a number (0-100):\u0026#39;) # 实例化为pydantic模型，捕获校验错误异常 try: number_input = NumberInput(number=number) except ValidationError as e: print(e) continue number = number_input.number break print(f\u0026#39;Your number is {number}\u0026#39;) 请不要拿 assert 来做参数校验，⽤ raise 语句来替代它\n空对象模式 简单来说，“空对象模式”就是本该返回 None 值或抛出异常时，返回⼀个符合正常结果接⼝的特制“空类型对象”来代替，以此免去调⽤⽅的错误处理⼯作。\n“空对象模式”也是⼀种转换设计观念以避免错误处理的技巧。当函数进⼊边界情况时，“空对象模式”不再抛出错误，⽽是让其返回⼀个类似于正常结果的特殊 对象，因此使⽤⽅⾃然就不必处理任何错误，⼈们写起代码来也会更轻松。\n总结 基础知识 ⼀个 try 语句⽀持多个 except ⼦句，但请记得把更精确的异常类放在前⾯ try 语句的 else 分⽀会在没有异常时执⾏，因此它可⽤来替代标记变量 不带任何参数的 raise 语句会重复抛出当前异常 上下⽂管理器经常⽤来处理异常，它最常⻅的⽤途是替代 finally ⼦句 上下⽂管理器可以⽤来忽略某段代码⾥的异常 使⽤ @contextmanager 装饰器可以轻松定义上下⽂管理器 错误处理与参数校验 当你可以选择编写条件判断或异常捕获时，优先选异常捕获（EAFP） 不要让函数返回错误信息，直接抛出⾃定义异常吧 ⼿动校验数据合法性⾮常烦琐，尽量使⽤专业模块来做这件事 不要使⽤ assert 来做参数校验，⽤ raise 替代它 处理错误需要付出额外成本，假如能通过设计避免它就再好不过了 在设计 API 时，需要慎重考虑是否真的有必要抛出错误 使⽤“空对象模式”能免去⼀些针对边界情况的错误处理⼯作 当你捕获异常时 过于模糊和宽泛的异常捕获可能会让程序免于崩溃，但也可能会带来更⼤的⿇烦 异常捕获贵在精确，只捕获可能抛出异常的语句，只捕获可能的异常类型 有时候，让程序提早崩溃未必是什么坏事 完全忽略异常是⻛险⾮常⾼的⾏为，⼤多数情况下，⾄少记录⼀条错误⽇志 当你抛出异常时 保证模块内抛出的异常与模块⾃⾝的抽象级别⼀致 如果异常的抽象级别过⾼，把它替换为更低级的新异常 如果异常的抽象级别过低，把它包装成更⾼级的异常，然后重新抛出 不要让调⽤⽅⽤字符串匹配来判断异常种类，尽量提供可区分的异常 循环与可迭代对象 iter()和next()内置函数 调⽤ iter() 会尝试返回⼀个迭代器对象\n对不可迭代的类型执⾏ iter() 会抛出 TypeError 异常\n什么是迭代器（iterator）？顾名思义，这是⼀种帮助你迭代其他对象的对象。迭代器最鲜明的特征是：不断对它执⾏ next() 函数会返回下⼀次迭代结果。\n当你使⽤ for 循环遍历某个可迭代对象时，其实是先调⽤了 iter() 拿到它的迭代器，然后不断地⽤ next() 从迭代器中获取值。\n自定义迭代器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Range7: \u0026#34;\u0026#34;\u0026#34;生成某个范围内可以被7整除或包含7的整数 :param start:开始数字 :param end:结束数字 \u0026#34;\u0026#34;\u0026#34; def __init__(self,start,end): self.start = start self.end = end # 使用current保存当前所处的位置 self.current = start def __iter__(self): return self def __next__(self): while True: # 当已经到达边界时，抛出异常终止迭代 if self.current \u0026gt;= self.end: raise StopIteration if self.num_is_valid(self.current): ret = self.current self.current += 1 return ret self.current += 1 def num_is_valid(self,num): \u0026#34;\u0026#34;\u0026#34;判断数字是否满足要求\u0026#34;\u0026#34;\u0026#34; if num == 0: return False return num % 7 == 0 or \u0026#39;7\u0026#39; in str(num) ⼀个合法的迭代器，必须同时实现 iter 和 next 两个魔法⽅法。\n可迭代对象只需要实现 iter ⽅法，不⼀定得实现 next ⽅法。\n所以，如果想让 Range7 对象在每次迭代时都返回完整结果，我们必须把现在的代码拆成两部分：可迭代类型 Range7 和迭代器类型 Range7Iterator。代码如下所⽰：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class Range7: \u0026#34;\u0026#34;\u0026#34;⽣成某个范围内可被 7 整除或包含 7 的数字\u0026#34;\u0026#34;\u0026#34; def __init__(self, start, end): self.start = start self.end = end def __iter__(self): # 返回一个新的迭代器对象 return Range7Iterator(self) class Range7Iterator: def __init__(self, range_obj): self.range_obj = range_obj self.current = range_obj.start def __iter__(self): return self def __next__(self): while True: if self.current \u0026gt;= self.range_obj.end: raise StopIteration if self.num_is_valid(self.current): ret = self.current self.current += 1 return ret self.current += 1 def num_is_valid(self, num): if num == 0: return False return num % 7 == 0 or \u0026#39;7\u0026#39; in str(num) 如果⼀个类型没有定义 iter，但是定义了 getitem ⽅法，那么 Python 也会认为它是可迭代的。在遍历它时，解释器会不断使⽤数字索引值(0, 1, 2,…)来调⽤getitem ⽅法获得返回值，直到抛出 IndexError 为⽌。\n生成器是迭代器 ⽣成器是⼀种“懒惰的”可迭代对象，使⽤它来替代传统列表可以节约内存，提升执⾏效率\n但除此之外，⽣成器还是⼀种简化的迭代器实现，使⽤它可以⼤⼤降低实现传统迭代器的编码成本。因此在平时，我们基本不需要通过 iter 和 next 来实现迭代器，只要写上⼏个 yield 就⾏。\n如果利⽤⽣成器，上⾯的 Range7Iterator 可以改写成⼀个只有 5 ⾏代码的函数：\n1 2 3 4 5 6 7 def range_7_gen(start,end): \u0026#34;\u0026#34;\u0026#34;生成器版本的Rang7Iterator\u0026#34;\u0026#34;\u0026#34; num = start while num \u0026lt; end: if num != 0 and (num % 7 == 0 or \u0026#39;7\u0026#39; in str(num)): yield num num += 1 ⽣成器（generator）利⽤其简单的语法，⼤⼤降低了迭代器的使⽤⻔槛，是优化循环代码时最得⼒的帮⼿。\nenumerate() 是 Python 的⼀个内置函数，它接收⼀个可迭代对象作为参数，返回⼀个不断⽣成 ( 当前下标 , 当前元素 ) 的新可迭代对象。对于这个场景，使⽤它再适合不过了\n⽣成器函数 even_only()，它专⻔负责偶数过滤⼯作：\n1 2 3 4 def even_only(numbers): for num in numbers: if num % 2 == 0: yield num 之后在 sum_even_only_v2() ⾥，只要先⽤ even_only() 函数修饰 numbers 变量，循环内的“偶数过滤”逻辑就可以完全去掉，只需简单求和即可：\n1 2 3 4 5 6 def sum_even_only_v2(numbers): \u0026#34;\u0026#34;\u0026#34;对numbers里面所有的偶数求和\u0026#34;\u0026#34;\u0026#34; result = 0 for num in even_only(numbers): result += num return result 总结⼀下，“修饰可迭代对象”是指⽤⽣成器（或普通的迭代器）在循环外部包装原本的循环主体，完成⼀些原本必须在循环内部执⾏的⼯作——⽐如过滤特定成员、提供额外结果等，以此简化循环代码\n除了⾃定义修饰函数外，你还可以直接使⽤标准库模块 itertools ⾥的许多现成⼯具。\n使用itertools模块优化循环 itertools 是⼀个和迭代器有关的标准库模块，其中包含许多⽤来处理可迭代对象的⼯具函数。\n使用product()扁平化多层嵌套循环 1 2 3 4 5 6 7 def find_twelve(num_list1, num_list2, num_list3): \u0026#34;\u0026#34;\u0026#34;从 3 个数字列表中，寻找是否存在和为 12 的 3 个数\u0026#34;\u0026#34;\u0026#34; for num1 in num_list1: for num2 in num_list2: for num3 in num_list3: if num1 + num2 + num3 == 12: return num1, num2, num3 对于这种嵌套遍历多个对象的多层循环代码，我们可以使⽤ product() 函数来优化它。product() 接收多个可迭代对象作为参数，然后根据它们的笛卡⼉积不断⽣成结果：\n⽤ product() 优化函数⾥的嵌套循环：\n1 2 3 4 5 6 from itertools import product def find_twelve_v2(num_list1,num_list2,num_list3): for num1,num2,num3 in product(num_list1,num_list2,num_list3): if num1+num2+num3 == 12: return num1,num2,num3 相⽐之前，新函数只⽤了⼀层 for 循环就完成了任务，代码变得更精练了。\n使用islice()实现循环内隔行处理 islice(seq, start, end, step) 函数和数组切⽚操作（list[start:stop:step]）接收的参数⼏乎完全⼀致。如果需要在循环内部实现隔⾏处理，只要设置第三个参数 step（递进步⻓）的值为 2 即可：\n1 2 3 4 5 6 7 from itertools import islice def parse_titles_v2(filename)： with open(filename,\u0026#39;r\u0026#39;) as fp: # 设置 step = 2,跳过无意义的 --- 分隔符 for line in islice(fp,0,None,2): yield line.strip() 使用takewhile()替代break语句 takewhile(predicate, iterable) 会在迭代第⼆个参数 iterable 的过程中，不断使⽤当前值作为参数调⽤ predicate() 函数，并对返回结果进⾏真值测试，如果为 True，则返回当前值并继续迭代，否则⽴即中断本次迭代。\n使⽤ takewhile() 后代码会变成这样：\n1 2 3 4 from itertools import takewhile for user in takewhile(is_qualified,users): .... 除了上⾯这三个函数以外，itertools 还有其他⼀些有意思的⼯具函数，它们都可以搭配循环使⽤，⽐如⽤ chain() 函数可以扁平化双层嵌套循环、⽤ zip_longest() 函数可以同时遍历多个对象，等等\n使用while循环加read()方法分块读取 除了直接遍历⽂件对象来逐⾏读取⽂件内容外，我们还可以调⽤更底层的 file.read() ⽅法。与直接⽤循环迭代⽂件对象不同，每次调⽤ file.read(chunk_size),会⻢上读取从游标位置往后 chunk_size ⼤⼩的⽂件内容，不必等待任何换⾏符出现。\n使⽤ file.read() 读取⽂件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def count_digits_v2(fname): \u0026#34;\u0026#34;\u0026#34;计算文件里包含多少个数字字符，每次读取8kb\u0026#34;\u0026#34;\u0026#34; count = 0 block_size = 1024 * 8 with open(fname) as file: while True: chunk = file.read(block_size) # 当文件没有更多内容时，read调用将会返回空字符串 if not chunk: break for s in chunk: if s.isdigit(): count += 1 return count 在新函数中，我们使⽤了⼀个 while 循环来读取⽂件内容，每次最多读 8 KB，程序不再需要在内存中拼接⻓达数吉字节的字符串，内存占⽤会⼤幅降低。不过，新代码虽然解决了⼤⽂件读取时的性能问题，循环内的逻辑却变得更零碎了。如果使⽤iter() 函数，我们可以进⼀步简化代码\niter()的另一个用法 当我们以 iter(callable, sentinel) 的⽅式调⽤ iter() 函数时，会拿到⼀个特殊的迭代器对象。⽤循环遍历这个迭代器，会不断返回调⽤ callable() 的结果，假如结果等于sentinel，迭代过程中⽌。利⽤这个特点，我们可以把上⾯的 while 重新改为 for，让循环内部变得更简单，如代码清单 6-3 所⽰。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from functools import partial def count_digit_v3(fname): count = 0 block_size = 1024 * 8 with open(fname) as fp: # 使用functools.partial 构造一个新的无须参数的函数 _read = partial(fp.read,block_size) # 利用iter()构造一个不断调用_read的迭代器 for chunk in iter(_read,\u0026#39;\u0026#39;): if s.isdigit(): count += 1 return count 要解耦循环体，⽣成器（或迭代器）是⾸选。在这个案例中，我们可以定义⼀个新的⽣成器函数：read_file_digits()，由它来负责所有与“数据⽣成”相关的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 读取数字内容的⽣成器函数 def read_file_digits(fp,block_size=1024 * 8): \u0026#34;\u0026#34;\u0026#34;生成器函数：分块读取文件内容，返回其中的数字字符\u0026#34;\u0026#34;\u0026#34; _read = partial(fp.read,block) for chunk in iter(_read,\u0026#39;\u0026#39;): for s in chunk: if s.isdigit(): yield s # 复⽤读取函数后的统计函数 def count_digits_v4(fname): count = 0 with open(fname) as file: for _ in read_file_digits(file): count += 1 return count # 复⽤读取函数后的统计偶数函数 from collections import defaultdict def count_even_groups(fname): \u0026#34;\u0026#34;\u0026#34;分别统计文件里每个偶数字符出现的次数\u0026#34;\u0026#34;\u0026#34; counter = defaultdict(int) with open(fname) as file: for num in read_file_digits(file): if int(num)%2 == 0: counter[int(num)] += 1 return counter ⼩ R 的故事告诉了我们⼀个道理。在编写循环时，我们需要时常问⾃⼰：循环体内的代码是不是过⻓、过于复杂了？如果答案是肯定的，那就试着把代码按职责分类，抽象成独⽴的⽣成器（或迭代器）吧。这样不光能让代码变得更整洁，可复⽤性也会极⼤提升。\n中断嵌套循环的正确方式 如果想快速从嵌套循环⾥跳出，其实有个更好的做法，那就是把循环代码拆分为⼀个新函数，然后直接使⽤ return。\n巧用next()函数 举个例⼦，假如有⼀个字典 d，你要怎么拿到它的第⼀个 key 呢？ 只要先⽤ iter() 获取⼀个 d.keys() 的迭代器，再对它调⽤ next() 就能⻢上拿到第⼀个元素。这样做不需要遍历字典的所有 key，⾃然⽐先转换列表的⽅法效率更⾼。\n假设有⼀个装了⾮常多整数的列表对象 numbers，我需要找到⾥⾯第⼀个可以被 7 整除的数字。除了编写传统的“for 循环配合 break”式代码，你也可以直接⽤ next() 配合⽣成器表达式来完成任务：\n1 2 numbers = [3,6,8,2,21,30,42] print(next(i for i in numbers if i%7==0)) 当心已被耗尽的迭代器 因此在平时，你需要将⽣成器（迭代器）的“可被⼀次性耗尽”特点铭记于⼼，避免写出由它所导致的 bug。假如要重复使⽤⼀个⽣成器，可以调⽤ list() 函数将它转成列表后再使⽤\n总结 迭代与迭代器原理 使用iter()函数会尝试获取一个迭代器对象 使⽤ next() 函数会获取迭代器的下⼀个内容 可以将 for 循环简单地理解为 while 循环 + 不断调⽤ next() ⾃定义迭代器需要实现 iter 和 next 两个魔法⽅法 ⽣成器对象是迭代器的⼀种 iter(callable, sentinel) 可以基于可调⽤对象构造⼀个迭代器 迭代器与可迭代对象 迭代器和可迭代对象是不同的概念 可迭代对象不⼀定是迭代器，但迭代器⼀定是可迭代对象 对可迭代性对象使用iter()会返回迭代器，迭代器则会返回它自身 每个迭代器的被迭代过程时一次性的，可迭代对象则不一定 可迭代对象只需要实现 iter ⽅法，⽽迭代器要额外实现 next ⽅法 代码可维护性技巧 通过定义生成器函数来侠士可迭代对象，可以优化循环内部代码 itertools模块里有许多函数可以⽤来修饰可迭代对象 生成器函数可以用来解耦循环代码，提升可复用性 不要使用多个break，拆分为函数然后直接return更好 使⽤ next() 函数有时可以完成⼀些意想不到的功能 文件操作知识 使⽤标准做法读取⽂件内容，在处理没有换⾏符的⼤⽂件时会很慢 调⽤ file.read() ⽅法可以解决读取⼤⽂件的性能问题 函数 别将可变类型作为参数默认值 Python 函数的参数默认值只会在函数定义阶段被创建⼀次，之后不论再调⽤多少次，函数内拿到的默认值都是同⼀个对象。\n假如再多花点⼉功夫，你甚⾄可以通过函数对象的保留属性 defaults 直接读取这个默认值：\n因此，熟悉 Python 的程序员通常不会将可变类型作为参数默认值。这是因为⼀旦函数在执⾏时修改了这个默认值，就会对之后的所有函数调⽤产⽣影响。\n为了规避这个问题，使⽤ None 来替代可变类型默认值是⽐较常⻅的做法：\n定义特殊对象来区分是否提供了默认参数 最常⻅的做法是定义⼀个特殊对象（标记变量）作为参数默认值\n定义仅限关键字参数 在经典编程图书《代码整洁之道》 中，作者 Robert C. Martin 提到：“函数接收的参数不要太多，最好不要超过 3 个。”这个建议很有道理，因为参数越多，函数的调⽤⽅式就会变得越复杂，代码也会变得更难懂。\n所以，当你要调⽤参数较多（超过 3 个）的函数时，使⽤关键字参数模式可以⼤⼤提⾼代码的可读性\n尽量只返回一种类型 好的函数设计⼀定是简单的，这种简单体现在各个⽅⾯。返回多种类型明显违反了简单原则。这种做法不光会给函数本⾝增加不必要的复杂度，还会提⾼⽤⼾理解和使⽤函数的成本。\n像上⾯的例⼦，更好的做法是将它拆分为两个独⽴的函数\nget_user_by_id(user_id)：返回单个⽤⼾ get_active_users()：返回多个⽤⼾列表。 这样就能让每个函数只返回⼀种类型，变得更简单易⽤\n谨慎返回None值 在编程语⾔的世界⾥，“空值”随处可⻅，它通常⽤来表⽰某个应该存在但是缺失的东西。“空值”在不同编程语⾔⾥有不同的名字，⽐如 Go 把它叫作 nil，Java 把它叫作 null， Python 则称它为 None。\n在 Python 中，None 是独⼀⽆⼆的存在。因为它有着⼀种独特的“虚⽆”含义，所以经常会⽤作函数返回值。\n当我们需要让函数返回 None 时，主要是下⾯ 3 种情况\n操作类函数的默认返回值 当某个操作类函数不需要任何返回值时，通常会返回 None。与此同时，None 也是不带任何 return 语句的函数的默认返回值：\n意料之中的缺失值 还有⼀类函数，它们所做的事情天⽣就是在尝试，⽐如从数据库⾥查找⼀个⽤⼾、在⽬录中查找⼀个⽂件。视条件不同，函数执⾏后可能有结果，也可能没有结果。⽽重点在于，对于函数的调⽤⽅来说，“没有结果”是意料之中的事情。\n针对这类函数，使⽤ None 作为“没有结果”时的返回值通常也是合理的。\n在执行失败时代表错误 有时候，None 也会⽤作执⾏失败时的默认返回值。 对这些函数来说，⽤抛出异常来代替返回 None 会更为合理。这也很好理解：当函数被调⽤时，如果⽆法返回正常结果，就代表出现了意料以外的状况，⽽“意料之外”正是异常所掌管的领域。\n早返回，多返回 ⾃打我开始写代码以来，常常会听⼈说起⼀条叫“单⼀出⼝”的原则。这条原则是说：“函数应该保证只有⼀个出⼝。 因此，在编写函数时，请不要纠结函数是不是应该只有⼀个 return，只要尽早返回结果可以提升代码可读性，那就多多返回吧\n但在 Python 中，“单⼀出⼝原则建议函数只写⼀个 return”只能算是⼀种误读，在“单⼀出⼝”和“多多返回”之间，我们完全可以选择可读性更强的那个。\n常用函数模块functools 这是⼀个很常⻅的函数使⽤场景：⾸先有⼀个接收许多参数的函数 a，然后额外定义⼀个接收更少参数的函数 b，通过在 b 内部补充⼀些预设参数，最后返回调⽤ a 函数的结果。\n针对这类场景，我们其实不需要像前⾯⼀样，⽤ def 去完全定义⼀个新函数——直接使⽤functools 模块提供的⾼阶函数 partial() 就⾏。\n1 2 3 4 5 6 7 8 9 10 result = multiply(2, value) val = multiply(2, number) def double(value): \u0026#34;\u0026#34;\u0026#34;返回multiply函数调用结果\u0026#34;\u0026#34;\u0026#34; return multiply(2,value) # 调用代码变得更简单 result = double(value) val = double(number) partial 的调⽤⽅式为 partial(func, *arg, **kwargs)，其中：\nfunc 是完成具体功能的原函数； *args/**kwargs 是可选位置与关键字参数，必须是原函数 func 所接收的合法参数。 举个例⼦，当你调⽤ partial(func, True, foo=1) 后，函数会返回⼀个新的可调⽤对象（callable object）——偏函数 partial_obj。\n拿到这个偏函数后，如果你不带任何参数调⽤它，效果等同于使⽤构建 partial_obj 对象时的参数调⽤原函数：partial_obj() 等同于 func(True, foo=1)。\n但假如你在调⽤ partial_obj 对象时提供了额外参数，前者就会⾸先将本次调⽤参数和构造 partial_obj 时的参数进⾏合并，然后将合并后的参数透传给原始函数 func 处理，也就是说，partial_obj(bar=2) 与 func(True, foo=1, bar=2) 效果相同。使⽤ functools.partial，上⾯的 double() 函数定义可以变得更简洁：\n1 2 3 import functools double = functools.partial(multiply,2) functools.partial 是 Python 标准库中的一个函数，用于固定函数的部分参数，创建一个新的函数（偏函数）。这样可以简化函数调用，特别是在需要多次调用同一个函数但参数略有不同的情况下。\nfunctools.lru_cache() 在编码时，我们的函数常常需要做⼀些耗时较⻓的操作，⽐如调⽤第三⽅ API、进⾏复杂运算等。这些操作会导致函数执⾏速度慢，⽆法满⾜要求。为了提⾼效率，给这类慢函数加上缓存是⽐较常⻅的做法\n在缓存⽅⾯，functools 模块为我们提供⼀个开箱即⽤的⼯具：lru_cache()。使⽤它，你可以⽅便地给函数加上缓存功能，同时不⽤修改任何函数内部代码\n在使⽤ lru_cache() 装饰器时，可以传⼊⼀个可选的 maxsize 参数，该参数代表当前函数最多可以保存多少个缓存结果。当缓存的结果数量超过 maxsize 以后，程序就会基于“最近最少使⽤”（least recently used，LRU）算法丢掉旧缓存，释放内存。默认情况下，maxsize 的值为 128。\n如果你把 maxsize 设置为 None，函数就会保存每⼀个执⾏结果，不再剔除任何旧缓存。这时如果被缓存的内容太多，就会有占⽤过多内存的⻛险。\n在函数式编程（functional programming）领域，有⼀个术语纯函数（pure function）。它最⼤的特点是，假如输⼊参数相同，输出结果也⼀定相同，不受任何其他因素影响。换句话说，纯函数是⼀种⽆状态的函数。\n虽然全局变量能满⾜需求，⽽且看上去似乎挺简单，但千万不要被它的外表蒙蔽了双眼。⽤全局变量保存状态，其实是写代码时最应该避开的事情之⼀。\n如果多个模块在不同线程⾥，同时导⼊并使⽤mosaic_global_var() 函数，整个字符轮换的逻辑就会乱掉，因为多个调⽤⽅共享同⼀个全局标记变量 _mosaic_char_index。\n总⽽⾔之，⽤全局变量管理状态，在各种场景下⼏乎都是下策，仅可在迫不得已时作为终极⼿段使⽤。\n给函数加上状态：闭包 闭包是一种允许函数访问已执行完成的其他函数里的私有变量的技术，是为函数增加状态的另一种方式\n正常情况下，当python完成依次函数执行后，本次使用的局部变量都会在调用结束后被回收，无法继续返回。但是如果你使用下面这种函数套函数的方式，在外层函数执行结束后，返回内嵌函数，后者就可以继续访问前者的局部变量，形成一个闭包结构\n1 2 3 4 5 6 7 8 9 def counter(): value = 0 def _counter(): # nonlocal ⽤来标注变量来⾃上层作⽤域，如不标明，内层函数将⽆法直接修改外层函数变量 nonlocal value value += 1 return value return _counter 使用闭包的有状态替换函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def make_mosaic(): \u0026#34;\u0026#34;\u0026#34; 将匹配到的模式替换为其他字符，使用闭包实现轮换字符效果 \u0026#34;\u0026#34;\u0026#34; char_index = 0 mmosaic_chars = [\u0026#39;*\u0026#39;,\u0026#39;x\u0026#39;] def _mosaic(matchobj): nonlocal char_index char = mosaic_chars[char_index] char_index = (char_index + 1) % len(mosaic_chars) length = len(matchobj.group()) return char * length return _mosaic 相⽐全局变量，使⽤闭包最⼤的特点就是封装性要好得多。在闭包代码⾥，索引变量 called_cnt 完全处于闭包内部，不会污染全局命名空间，⽽且不同闭包对象之间也不会相互 影响。总⽽⾔之，闭包是⼀种⾮常有⽤的⼯具，⾮常适合⽤来实现简单的有状态函数。不过，除了闭包之外，还有⼀个天⽣就适合⽤来实现“状态”的⼯具：类\n给函数加上状态：类 类（class）是⾯向对象编程⾥最基本的概念之⼀。在⼀个类中，状态和⾏为可以被很好地封装在⼀起，因此它天⽣适合⽤来实现有状态对象。\n基于类实现有状态替换方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 class CyclicMosaic: \u0026#34;\u0026#34;\u0026#34;使用会轮换的屏蔽字符，基于类实现\u0026#34;\u0026#34;\u0026#34; _chars = [\u0026#39;*\u0026#39;,\u0026#39;x\u0026#39;] def __init__(self): self._char_index = 0 def generate(self,matchobj): char = self._chars[self._char_index] self._char_index = (self._char_index + 1) % len(self._chars) length = len(matchobj.group()) return char * length 类实例的状态⼀般都在 init 函数⾥初始化\n不过严格说来，这个⽅案最终依赖的 CycleMosaic().generate，并⾮⼀个有状态的函数，⽽是⼀个有状态的实例⽅法。但⽆论是函数还是实例⽅法，它们都是“可调⽤对象”的⼀种，都可以作为 re.sub() 函数的 repl 参数使⽤。\n别写太复杂的函数 Steve McConnell 提到函数的理想⻓度范围是 65 到 200 ⾏\n圈复杂度 在 Python 中，你可以通过 radon ⼯具计算⼀个函数的圈复杂度。radon 基于 Python编写，使⽤ pip install radon 即可完成安装。 一个函数只包含一层抽象 什么是抽象 通⽤领域⾥的“抽象”，是指在⾯对复杂事物（或概念）时，主动过滤掉不需要的细节，只关注与当前⽬的有关的信息的过程\n举个例⼦，我吃完饭在⼤街上散步，⾛得有点⼉累了，于是对⾃⼰说：“腿真疼啊，找把椅⼦坐吧。”此时此刻，“椅⼦”在我脑中就是⼀个抽象的概念。 我脑中的椅⼦： 有⼀个平坦的表⾯可以把屁股放上去；离地 20 到 50 厘⽶，能⽀撑 60 千克以上的重量。对这个抽象概念来说，路边的⾦属⿊⾊⻓椅是我需要的椅⼦，饭店⻔⼝的塑料扶⼿椅同样也是 我需要的椅⼦，甚⾄某个⼀尘不染的台阶也可以成为我要的“椅⼦”。在这个抽象下，椅⼦的其他特征，⽐如使⽤什么材料（⽊材还是⾦属）、涂的什么颜⾊（⽩⾊还是⿊⾊），对于我来说都不重要。于是在⼀次逛街中，我不知不觉完成了⼀次对椅⼦的抽象，解决了屁股坐哪⼉的问题。 所以简单来说，抽象就是⼀种选择特征、简化认知的⼿段。接下来，我们看看抽象与软件开发的关系\n抽象与软件开发 什么是分层？分层就在设计⼀个复杂系统时，按照问题抽象程度的⾼低，将系统划分为不同的抽象层（abstraction layer）。低级的抽象层⾥包含较多的实现细节。随着层级变⾼，细节越来越少，越接近我们想要解决的实际问题。\n在这种分层结构下，每⼀层抽象都只依赖⽐它抽象级别更低的层，同时对⽐它抽象级别更⾼的层⼀⽆所知。因此，每层都可以脱离更⾼级别的层独⽴⼯作。⽐如活跃在传输层的 TCP 协议，可以对应⽤层的 HTTP、HTTPS 等应⽤协议毫⽆感知，独⽴⼯作。\n正因为抽象与分层理论特别有⽤，所以不管你有没有意识到，其实在各个维度上都活跃着“分层”的⾝影，如下所⽰。 项⽬间的分层：电商后端 API（⾼层抽象）→数据库（低层抽象）。 项⽬内的分层：账单模块（⾼层抽象）→ Django 框架（低层抽象）。 模块内的分层：函数名–获取账⼾信息（⾼层抽象）→函数内–处理字符串（低层抽象）。\n因此，即便是在⾮常微观的层⾯上，⽐如编写⼀个函数时，我们同样需要考虑函数内代码与抽象级别的关系。假如⼀个函数内同时包含了多个抽象级别的内容，就会引发⼀系列的问题。\n脚本案例：调用api查找歌手的第一张专辑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 \u0026#34;\u0026#34;\u0026#34;通过 iTunes API搜索歌手发布的第一张专辑\u0026#34;\u0026#34;\u0026#34; import sys from json.decoder import JSONDecodeError import requests from requests.exceptions import HTTPError ITUNES_API_ENDPIOINT = \u0026#39;https://itunes.apple.com/search\u0026#39; def command_first_album(): \u0026#34;\u0026#34;\u0026#34;通过脚本输入查找并打印歌手的第一张专辑信息\u0026#34;\u0026#34;\u0026#34; if not len(sys.argv) == 2: print(f\u0026#39;usage: python {sys.argv[0]} {{SEARCH_TERM}}\u0026#39;) sys.exit(1) term = sys.argv[1] resp = requests.get( ITUNES_API_ENDPOINT, { \u0026#39;term\u0026#39;:term, \u0026#39;media\u0026#39;:\u0026#39;music\u0026#39;, \u0026#39;entity\u0026#39;:\u0026#39;album\u0026#39;, \u0026#39;attribute\u0026#39;:\u0026#39;artisitTerm\u0026#39;, \u0026#39;limit\u0026#39;:200, }, ) try: resp.raise_for_status() except HTTPError as e: print(f\u0026#39;Error: failed to call iTunes API, {e}\u0026#39;) sys.exit(2) try: albums = resp.json()[\u0026#39;results\u0026#39;] except JSONDecodeError: print(f\u0026#39;Error: response is not valid JSON format\u0026#39;) sys.exit(2) if not albums: print(f\u0026#39;Error: no albums found for artist \u0026#34;{term}\u0026#34;\u0026#39;) sys.exit(1) sorted_albums = sorted(albums,key=lambda item: item[\u0026#39;releaseDate\u0026#39;]) first_album = sorted_album[0] # 去除发布日期的小时和分钟信息 release_date = first_album[\u0026#39;releaseDate\u0026#39;].split(\u0026#39;T\u0026#39;)[0] # 打印结果 print(f\u0026#34;{term}\u0026#39;s first album: \u0026#34;) print(f\u0026#34; * Name: {first_album[\u0026#39;collectionName\u0026#39;]}\u0026#34;) print(f\u0026#34; * Genre: {first_album[\u0026#39;primaryGenreName\u0026#39;]}\u0026#34;) print(f\u0026#34; * Released at: {release_date}\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: command_first_album() 上⾯脚本的主函数 command_first_album() 显然不符合这个标准。在函数内部，不同抽象级别的代码随意混合在了⼀起。⽐如，当请求 API 失败时（数据层），函数直接调⽤sys.exit() 中断了程序执⾏（⽤⼾界⾯层）\n这种抽象级别上的混乱，最终导致了下⾯两个问题。 函数代码的说明性不够：如果只是简单读⼀遍 command_first_album()，很难搞清楚\n它的主流程是什么，因为⾥⾯的代码五花⼋⻔，什么层次的信息都有。 函数的可复⽤性差：假如现在要开发新需求——查询歌⼿的所有专辑，你⽆法复⽤已有函数的任何代码 所以，如果缺乏设计，哪怕是⼀个只有 40 ⾏代码的简单函数，内部也很容易产⽣抽象混乱问题。要优化这个函数，我们需要重新梳理程序的抽象级别。 在我看来，这个程序⾄少可以分为以下三层。\n用户界面层：处理⽤⼾输⼊、输出结果。 第一张专辑层：找到第⼀张专辑。 专辑数据层：调⽤ API 获取专辑信息。 在每⼀个抽象层内，程序所关注的事情都各不相同 基于抽象层重构代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 \u0026#34;\u0026#34;\u0026#34; 通过 iTunes API 搜索歌⼿发布的第⼀张专辑\u0026#34;\u0026#34;\u0026#34; import sys from json.decoder import JSONDecodeError import requests from requests.exceptions import HTTPError ITUNES_API_ENDPOINT = \u0026#39;https://itunes.apple.com/search\u0026#39; class GetFirstAlbumError(Exception): \u0026#34;\u0026#34;\u0026#34; 获取第⼀张专辑失败\u0026#34;\u0026#34;\u0026#34; class QueryAlbumsError(Exception): \u0026#34;\u0026#34;\u0026#34;获取专辑列表失败\u0026#34;\u0026#34;\u0026#34; def command_first_album(): \u0026#34;\u0026#34;\u0026#34;通过输⼊参数查找并打印歌⼿的第⼀张专辑信息\u0026#34;\u0026#34;\u0026#34; if not len(sys.argv) == 2: print(f\u0026#39;usage: python {sys.argv[0]} {{SEARCH_TERM}}\u0026#39;) sys.exit(1) artist = sys.argv[1] try: album = get_first_album(artist) except GetFirstAlbumError as e: print(f\u0026#34;error: {e}\u0026#34;, file=sys.stderr) sys.exit(2) print(f\u0026#34;{artist}\u0026#39;s first album: \u0026#34;) print(f\u0026#34; * Name: {album[\u0026#39;name\u0026#39;]}\u0026#34;) print(f\u0026#34; * Genre: {album[\u0026#39;genre_name\u0026#39;]}\u0026#34;) print(f\u0026#34; * Released at: {album[\u0026#39;release_date\u0026#39;]}\u0026#34;) def get_first_album(artist): \u0026#34;\u0026#34;\u0026#34;根据专辑列表获取第⼀张专辑 :param artist: 歌⼿名字 :return: 第⼀张专辑 :raises: 获取失败时抛出 GetFirstAlbumError \u0026#34;\u0026#34;\u0026#34; try: albums = query_all_albums(artist) except QueryAlbumsError as e: raise GetFirstAlbumError(str(e)) sorted_albums = sorted(albums,key=lambda item:item[\u0026#39;releaseDate\u0026#39;]) first_album = sorted_albums[0] # 去除发布日期里的小时与分钟信息 release_date = first_album[\u0026#39;realeaseDate\u0026#39;].split(\u0026#39;T\u0026#39;)[0] return { \u0026#39;name\u0026#39;:first_album[\u0026#39;collectionName\u0026#39;], \u0026#39;genre_name\u0026#39;:first_album[\u0026#39;primaryGenreName\u0026#39;], \u0026#39;release_date\u0026#39;:release_date, } def query_all_albums(artist): \u0026#34;\u0026#34;\u0026#34; 根据歌⼿名字搜索所有专辑列表 :param artist: 歌⼿名字 :return: 专辑列表，List[Dict] :raises: 获取专辑失败时抛出 GetAlbumsError \u0026#34;\u0026#34;\u0026#34; resp = requests.get( ITUNES_API_ENDPOINT, { \u0026#39;term\u0026#39;: artist, \u0026#39;media\u0026#39;: \u0026#39;music\u0026#39;, \u0026#39;entity\u0026#39;: \u0026#39;album\u0026#39;, \u0026#39;attribute\u0026#39;: \u0026#39;artistTerm\u0026#39;, \u0026#39;limit\u0026#39;: 200, }, ) try: resp.raise_for_status() except HTTPError as e: raise QueryAlbumsError(f\u0026#39;failed to call iTunes API, {e}\u0026#39;) try: albums = resp.json()[\u0026#39;results\u0026#39;] except JSONDecodeError: raise QueryAlbumsError(\u0026#39;response is not valid JSON format\u0026#39;) if not albums: raise QueryAlbumsError(f\u0026#39;no albums found for artist \u0026#34;{artist}\u0026#34;\u0026#39;) return albums 在新代码中，旧的主函数被拆分成了三个不同的函数。\ncommand_first_album()：程序主⼊⼝，对应⽤⼾界⾯层。 get_first_album()：获取第⼀张专辑，对应“第⼀张专辑”层。 query_all_albums()：调⽤ API 获取数据，对应专辑数据层。 优先使用列表推导式 函数式编程是⼀种编程⻛格，它最⼤的特征，就是通过组合⼤量没有副作⽤的“纯函数”来实现复杂的功能。如果你想在 Python 中实践函数式编程，最常⽤的⼏个⼯具如下所⽰。\nmap(func, iterable)：遍历并执⾏ func 获取结果，迭代返回新结果。 filter(func, iterable)：遍历并使⽤ func 测试成员，仅当结果为真时返回。 lambda：定义⼀个⼀次性使⽤的匿名函数。 举个例⼦，假如你想获取所有处于活跃状态的⽤⼾积分，代码可以这么写：\npoints = list(map(query_points,filter(lambda user:user.is_active(),users)))\n但⽐起上⾯这种 map 套 filter 的写法，我们其实完全可以使⽤列表推导式来搞定这个问题：\npoints = [query_points(user) for user in users if user.is_active()]\n在⼤多数情况下，相⽐函数式编程，使⽤列表推导式的代码通常更短，⽽且描述性更强。所以，当列表推导式可以满⾜需求时，请优先使⽤它吧。\n你没那么需要lambda Python 中有⼀类特殊的函数：匿名函数。你可以⽤ lambda 关键字来快速定义⼀个匿名函数，⽐如 lambda x, y: x + y。匿名函数最常⻅的⽤途就是作为 sorted() 函数的排序参数使⽤\n对于任何递归代码来说，⼀劳永逸的办法是将其改写成循环。\n总结 函数参数与返回相关基础知识 不要使⽤可变类型作为参数默认值，⽤ None 来代替 使⽤标记对象，可以严格区分函数调⽤时是否提供了某个参数 定义仅限关键字参数，可以强制要求调⽤⽅提供参数名，提升可读性 函数应该拥有稳定的返回类型，不要返回多种类型 适合返回 None 的情况——操作类函数、查询类函数表⽰意料之中的缺失值 在执⾏失败时，相⽐返回 None，抛出异常更为合适 如果提前返回结果可以提升可读性，就提前返回，不必追求“单⼀出⼝” 代码可维护性技巧 不要编写太⻓的函数，但⻓度并没有标准，65 ⾏算是⼀个危险信号 圈复杂度是评估函数复杂程度的常⽤指标，圈复杂度超过 10 的函数需要重构 抽象与分层思想可以帮我们更好地构建与管理复杂的系统 同⼀个函数内的代码应该处在同⼀抽象级别 函数与状态 没有副作⽤的⽆状态纯函数易于理解，容易维护，但⼤多数时候“状态”不可避免 避免使⽤全局变量给函数增加状态 当函数状态较简单时，可以使⽤闭包技巧 当函数需要较为复杂的状态管理时，建议定义类来管理状态 语言机制对函数的影响 functools.partial() 可以⽤来快速构建偏函数 functools.lru_cache() 可以⽤来给函数添加缓存 ⽐起 map 和 filter，列表推导式的可读性更强，更应该使⽤ lambda 函数只是⼀种语法糖，你可以使⽤ operator 模块等⽅式来替代它 Python 语⾔⾥的递归限制较多，可能的话，请尽量使⽤循环来替代 装饰器 作为最流⾏的 Web 开发框架，Django 提供了⾮常强⼤的功能。它有⼀个清晰的 MTV（model-template-view，模型—模板—视图）分层架构和开箱即⽤的 ORM 引擎，以及丰富到令⼈眼花缭乱的可配置项\n装饰器是⼀种通过包装⽬标函数来修改其⾏为的特殊⾼阶函数，绝⼤多数装饰器是利⽤函数的闭包原理实现的。\n先进⾏⼀次调⽤，传⼊装饰器参数，获得第⼀层内嵌函数 decorator 进⾏第⼆次调⽤，获取第⼆层内嵌函数 wrapper\n在应⽤有参数装饰器时，⼀共要做两次函数调⽤，所以装饰器总共得包含三层嵌套函数。正因为如此，有参数装饰器的代码⼀直都难写、难读。\n使用functools.wraps()修饰包装函数 在装饰器包装⽬标函数的过程中，常会出现⼀些副作⽤，其中⼀种是丢失函数元数据。\n⾸先，由 calls_counter 对函数进⾏包装，此时的 random_sleep 变成了新的包装函数，包含 print_counter 属性\n使⽤ timer 包装后，random_sleep 变成了 timer 提供的包装函数，原包装函数额外的 print_counter 属性被⾃然地丢掉了\n要解决这个问题，我们需要在装饰器内包装函数时，保留原始函数的额外属性。⽽ functools模块下的 wraps() 函数正好可以完成这件事情。\n添加 @wraps(wrapped) 来装饰 decorated 函数后，wraps() ⾸先会基于原函数func 来更新包装函数 decorated 的名称、⽂档等内置属性，之后会将 func 的所有额外属性赋值到 decorated 上\n正因为如此，在编写装饰器时，切记使⽤ @functools.wraps() 来修饰包装函数。\n实现可选参数装饰器 有参数装饰器的这个特点提⾼了它的使⽤成本——如果使⽤者忘记添加那对括号，程序就会出错\n那么有没有什么办法，能让我们省去那对括号，直接使⽤ @delayed_start 这种写法呢？答案是肯定的，利⽤仅限关键字参数，你可以很⽅便地做到这⼀点。\n代码清单 8-5 定义了可选参数的装饰器 delayed_start\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def delayed_start(func=None,*,duration = 1): \u0026#34;\u0026#34;\u0026#34;装饰器：在执⾏被装饰函数前，等待⼀段时间 :param duration: 需要等待的秒数 \u0026#34;\u0026#34;\u0026#34; def decorator(_func): def wrapper(*args,**kwargs): print(f\u0026#39;Wait for {duration} second before starting...\u0026#39;) time.sleep(duration) return _func(*args, **kwargs) return wrapper if func is None: return decorator else: return decorator(func) ❶ 把所有参数都变成提供了默认值的可选参数 ❷ 当 func 为 None 时，代表使⽤⽅提供了关键字参数，⽐如 @delayed_start(duration=2)，此时返回接收单个函数参数的内层⼦装饰器 decorator ❸ 当位置参数 func 不为 None 时，代表使⽤⽅没提供关键字参数，直接⽤了⽆括号的 @ delayed_start 调⽤⽅式，此时返回内层包装函数 wrapper\n把参数变为可选能有效降低使⽤者的⼼智负担，让装饰器变得更易⽤。标准库dataclasses 模块⾥的 @dataclass 装饰器就使⽤了这个⼩技巧\n用类来实现装饰器(函数替换) 绝⼤多数情况下，我们会选择⽤嵌套函数来实现装饰器，但这并⾮构造装饰器的唯⼀⽅式。事实上，某个对象是否能通过装饰器（@decorator）的形式使⽤只有⼀条判断标准，那就是decorator 是不是⼀个可调⽤的对象\n函数⾃然是可调⽤对象，除此之外，类同样也是可调⽤对象\n使⽤ callable() 内置函数可以判断某个对象是否可调⽤\n如果⼀个类实现了 call 魔法⽅法，那么它的实例也会变成可调⽤对象：\n调⽤类实例时，可以像调⽤普通函数⼀样提供额外参数\n基于类的这些特点，我们完全可以⽤它来实现装饰器。\n如果按装饰器⽤于替换原函数的对象类型来分类，类实现的装饰器可分为两种，⼀种是“函数替换”，另⼀种是“实例替换”。下⾯我们先来看⼀下前者。\n函数替换装饰器虽然是基于类实现的，但⽤来替换原函数的对象仍然是个普通的包装函数。这种技术最适合⽤来实现接收参数的装饰器。\n⽤类实现的 timer 装饰器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class timer: \u0026#34;\u0026#34;\u0026#34; 装饰器：打印函数耗时 :param print_args: 是否打印⽅法名和参数，默认为 False \u0026#34;\u0026#34;\u0026#34; def __init__(self,print_args): self.print_args = print_args def __call__(self,func): @wrap(func) def decorated(*args,**kwargs): st = time.perf_counter() ret = func(*args,**kwargs) if self.print_args: print(f\u0026#39;\u0026#34;{func.__name__}\u0026#34;, args: {args}, kwargs: {kwargs}\u0026#39;) print(\u0026#39;time cost: {} seconds\u0026#39;.format(time.perf_counter() - st)) return ret return decorated 还记得我之前说过，有参数装饰器⼀共得提供两次函数调⽤吗？通过类实现的装饰器，其实就是把原本的两次函数调⽤替换成了类和类实例的调⽤。\n(1) 第⼀次调⽤：_deco = timer(print_args=True) 实际上是在初始化⼀个 timer 实例。 (2) 第⼆次调⽤：func = _deco(func) 是在调⽤ timer 实例，触发 call ⽅法。\n相⽐三层嵌套的闭包函数装饰器，上⾯这种写法在实现有参数装饰器时，代码更清晰⼀些，⾥⾯的嵌套也少了⼀层。不过，虽然装饰器是⽤类实现的，但最终⽤来替换原函数的对象，仍然是⼀个处在 call ⽅法⾥的闭包函数 decorated。\n用类来实现装饰器(实例替换) 和“函数替换”装饰器不⼀样，“实例替换”装饰器最终会⽤⼀个类实例来替换原函数。通过组合不同的⼯具，它既能实现⽆参数装饰器，也能实现有参数装饰器。\n实现无参数装饰器 ⽤类来实现装饰器时，被装饰的函数 func 会作为唯⼀的初始化参数传递到类的实例化⽅法 init 中。同时，类的实例化结果——类实例（class instance），会作为包装对象替换原始函数。\n实例替换的⽆参数装饰器 DelayedStart\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class DelayedStart: \u0026#34;\u0026#34;\u0026#34;在执⾏被装饰函数前，等待 1 秒钟\u0026#34;\u0026#34;\u0026#34; def __init__(self,func): update_wrapper(self,func) self.func = func def __call__(self,*args,**kwargs): print(f\u0026#39;Wait for 1 second before starting...\u0026#39;) time.sleep(1) return self.func(*args,**kwargs) def eager_call(self,*args,**kwargs): \u0026#34;\u0026#34;\u0026#34;跳过等待，立刻执行被装饰函数\u0026#34;\u0026#34;\u0026#34; print(\u0026#39;Call without delay\u0026#39;) return self.func(*args,**kwargs) ❶ update_wrapper 与前⾯的 wraps ⼀样，都是把被包装函数的元数据更新到包装者（在这⾥是 DelayedStart 实例）上 ❷ 通过实现 call ⽅法，让 DelayedStart 的实例变得可调⽤，以此模拟函数的调⽤⾏为 ❸ 为装饰器类定义额外⽅法，提供更多样化的接⼝\n❶ 被装饰的 hello 函数已经变成了装饰器类 DelayedStart 的实例，但是因为 update_wrapper 的作⽤，这个实例仍然保留了被装饰函数的元数据 ❷ 此时触发的其实是装饰器类实例的 call ⽅法 ❸ 使⽤额外的 eager_call 接⼝调⽤函数\n实现有参数装饰器 同普通装饰器⼀样，“实例替换”装饰器也可以⽀持参数。为此我们需要先修改类的实例化⽅法，增加额外的参数，再定义⼀个新函数，由它来负责基于类创建新的可调⽤对象，这个新函数同时也是会被实际使⽤的装饰器。\n实例替换的有参数装饰器 delayed_start\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class DelayedStart: \u0026#34;\u0026#34;\u0026#34;在执⾏被装饰函数前，等待⼀段时间 :param func: 被装饰的函数 :param duration: 需要等待的秒数 \u0026#34;\u0026#34;\u0026#34; def __init__(self,func,*,duration=1): update_wrapper(self,func) self.func = func self.duration = duration def __call__(self,*args,**kwargs): print(f\u0026#39;Wait for {self.duration} second before starting...\u0026#39;) time.sleep(self.duration) return self.func(*args, **kwargs) def eager_call(self,*args,*kwargs):... def delayed_start(**kwargs): \u0026#34;\u0026#34;\u0026#34;装饰器：推迟某个函数的执⾏\u0026#34;\u0026#34;\u0026#34; return functools.partial(DelayedStart,**kwargs) ❶ 把 func 参数以外的其他参数都定义为“仅限关键字参数”，从⽽更好地区分原始函数与装饰器的其他参数 ❷ 通过 partial 构建⼀个新的可调⽤对象，这个对象接收的唯⼀参数是待装饰函数func，因此可以⽤作装饰器\n相⽐传统做法，⽤类来实现装饰器（实例替换）的主要优势在于，你可以更⽅便地管理装饰器的内部状态，同时也可以更⾃然地为被装饰对象追加额外的⽅法和属性。\n使用wrapt模块助力装饰器编写 我实现了⼀个⾃动注⼊函数参数的装饰器 provide_number，它在装饰函数后，会在后者被调⽤时⾃动⽣成⼀个随机数，并将其注⼊为函数的第⼀个位置参数。\n注入数字的装饰器provide_number 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import random def provide_number(min_num,max_num): \u0026#34;\u0026#34;\u0026#34; 装饰器：随机⽣成⼀个在 [min_num, max_num] 范围内的整数， 并将其追加为函数的第⼀个位置参数 \u0026#34;\u0026#34;\u0026#34; def wrapper(func): def decorated(*args,**kwargs) num = random.randint(min_num,max_num) # 将num追加为第一个参数，然后调用函数 return func(num,*args,**kwargs) return decorated return wrapper 使⽤效果如下：\n@provide_number(1, 100) \u0026hellip; def print_random_number(num): \u0026hellip; print(num) \u0026hellip; print_random_number() 57\n类⽅法（method）和函数（function）在⼯作机制上有细微的区别。当类实例⽅法被调⽤时，第⼀个位置参数总是当前绑定的类实例 self 对象。因此，当装饰器向 *args 前追加随机数时，其实已经把 *args ⾥的 self 挤到了 num 参数所在的位置\n为了修复这个问题，provide_number 装饰器在追加位置参数时，必须聪明地判断当前被修饰的对象是普通函数还是类⽅法。假如被修饰的对象是类⽅法，那就得跳过藏在 *args ⾥的类实例变量，才能正确将 num 作为第⼀个参数注⼊。 假如要⼿动实现这个判断，装饰器内部必须增加⼀些烦琐的兼容代码，费⼯费时。幸运的是，wrapt 模块可以帮我们轻松处理好这类问题。 wrapt 是⼀个第三⽅装饰器⼯具库，利⽤它，我们可以⾮常⽅便地改造 provide_number 装饰器，完美地解决这个问题。\n基于 wrapt 模块实现的 provide_number 装饰器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import wrapt def provide_number(min_num,max_num): @wrapt.decorator def wrapper(wrapped,instance,args,kwargs): # 参数含义 # # - wrapped：被装饰的函数或类方法 # - instance: # - 如果被装饰者为普通类方法，则该值为类实例 # - 如果被装饰者为classmethod类方法，则该值为类 # - 如果被装饰器为类/函数/静态方法，则该值为None # # - args:调用时的位置参数(注意没有*符号) # - kwargs:调用时的关键字参数(注意没有**符号) # num = random.randint(min_num,max_num) # 无须关注wrapped是类方法还是普通函数，直接在头部追加参数 args = (num,)+args return wrapped(*args,**kwargs) return wrapped 使⽤ wrapt 模块编写的装饰器，除了解决了类⽅法兼容问题以外，代码嵌套层级也⽐普通装饰器少，变得更扁平、更易读。如果你有兴趣，可以参阅 wrapt 模块的官⽅⽂档了解更多信息。\n了解装饰器的本质优势 装饰器带来的改变，主要在于把修改函数的调⽤提前到了函数定义处，⽽这⼀点⼉位置上的⼩变化，重塑了读者理解代码的整个过程。\n所以，装饰器的优势并不在于它提供了动态修改函数的能⼒，⽽在于它把影响函数的装饰⾏为移到了函数头部，降低了代码的阅读与理解成本。\n“动态修改函数”的能⼒，其实并不是由装饰器提供的。假如没有装饰器，我们也能在定义完函数后，⼿动调⽤装饰函数来修改它。\n为了充分发挥这个优势，装饰器特别适合⽤来实现以下功能。\n运行时校验：在执行阶段进行特定校验，当校验不通过时终止执行 适合原因：装饰器可以⽅便地在函数执⾏前介⼊，并且可以读取所有参数辅助校验。 代表样例：Django 框架中的⽤⼾登录态校验装饰器 @login_required。\n注入额外参数：在函数被调⽤时⾃动注⼊额外的调⽤参数。 适合原因：装饰器的位置在函数头部，⾮常靠近参数被定义的位置，关联性强。 代表样例：unittest.mock 模块的装饰器 @patch。\n缓存执⾏结果：通过调⽤参数等输⼊信息，直接缓存函数执⾏结果。 适合原因：添加缓存不需要侵⼊函数内部逻辑，并且功能⾮常独⽴和通⽤。 代表样例：functools 模块的缓存装饰器 @lru_cache。\n注册函数：将被装饰函数注册为某个外部流程的一部分 适合原因：在定义函数时可以直接完成注册，关联性强。 代表样例：Flask 框架的路由注册装饰器 @app.route。\n替换为复杂对象：将原函数（⽅法）替换为更复杂的对象，⽐如类实例或特殊的描述符对象 适合原因：在执⾏替换操作时，装饰器语法天然⽐ foo = staticmethod(foo) 的写法要直观得多。 代表样例：静态类⽅法装饰器 @staticmethod。\n在设计新的装饰器时，你可以先参考上⾯的常⻅装饰器功能列表，琢磨琢磨⾃⼰的设计是否能很好地发挥装饰器的优势。切勿滥⽤装饰器技术，设计出⼀些天⻢⾏空但难以理解的 API。吸取前⼈经验，同时在设计上保持克制，才能写出更好⽤的装饰器。\n使用类装饰器替代元类 Python 中的元类（metaclass）是⼀种特殊的类。就像类可以控制实例的创建过程⼀样，元类可以控制类的创建过程。通过元类，我们能实现各种强⼤的功能。⽐如下⾯的代码就利⽤元类统⼀注册所有 Validator 类： 元类是类的类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 _validators = {} class ValidatorMeta(type): \u0026#34;\u0026#34;\u0026#34;元类:统一注册所有校验器类，方便后续使用\u0026#34;\u0026#34;\u0026#34; def __new__(cls,name,bases,attrs): ret = super().__new__(cls,name,bases,attrs) _validators[attrs[\u0026#39;name\u0026#39;]] = ret return ret class StringValidator(metaclass=ValidatiorMeta) name = \u0026#39;string\u0026#39; class IntegerValidator(metaclass=ValidatorMeta) name = \u0026#39;int\u0026#39; 类装饰器的⼯作原理与普通装饰器类似。下⾯的代码就⽤类装饰器实现了 ValidatorMeta 元 类的功能\n1 2 3 4 5 6 7 8 9 10 11 12 def register(cls): \u0026#34;\u0026#34;\u0026#34;装饰器:统一注册所有校验器类：方便后续使用\u0026#34;\u0026#34;\u0026#34; _validators[cls.name] = cls return cls @register class StringValidator: name = \u0026#39;string\u0026#39; @register class IntegerValidator: name = \u0026#39;int\u0026#39; 别弄混装饰器模式和装饰器 装饰器模式属于⾯向对象领域。实现装饰器模式，需要具备以下关键要素： 设计⼀个统⼀的接⼝； 编写多个符合该接⼝的装饰器类，每个类只实现⼀个简单的功能； 通过组合的⽅式嵌套使⽤这些装饰器类； 通过类和类之间的层层包装来实现复杂的功能。\n代码清单 8-11 是我⽤ Python 实现的⼀个简单的装饰器模式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Numbers: \u0026#34;\u0026#34;\u0026#34;一个包含多个数字的简单类\u0026#34;\u0026#34;\u0026#34; def __init__(self,numbers): self.numbers = numbers def get(self): return self.numbers class EvenOnlyDecorator: \u0026#34;\u0026#34;\u0026#34;装饰器类:过滤所有偶数\u0026#34;\u0026#34;\u0026#34; def __init__(self,decorated): self.decorated = decorated def get(self): return [num for num in self.decorated.get() if num % 2 == 0] class GreaterThanDecorator: \u0026#34;\u0026#34;\u0026#34;装饰器类：过滤大于某个数的数\u0026#34;\u0026#34;\u0026#34; def __init__(self,decorated,min_value): self.decorated = decorated self.min_value = min_value def get(self): return [num for num in self.decorated.get() if num \u0026gt; self.min_value] obj = Numbers([42, 12, 13, 17, 18, 41, 32]) even_obj = EvenOnlyDecorator(obj) gt_obj = GreaterThanDecorator(even_obj,min_value=30) print(gt_obj.get()) \u0026gt;\u0026gt; [42, 32] 从上⾯的代码中你能发现，装饰器模式和 Python ⾥的装饰器毫不相⼲。如果硬要找⼀点⼉联 系，它俩可能都和“包装”有关——⼀个包装函数，另⼀个包装类。\n所以，请不要混淆装饰器和装饰器模式，它们只是名字⾥刚好都有“装饰器”⽽已。\n浅装饰器，深实现 归根结底，装饰器其实只是⼀类特殊的 API，⼀种提供服务的⽅式。⽐起把所有核⼼逻辑都放在装饰器内，不如让装饰器⾥只有⼀层浅浅的包装层，⽽把更多的实现细节放在其他函数或类中。 这样做之后，假如你未来需要为模块增加装饰器以外的其他 API，⽐如上下⽂管理器，就会发现⾃⼰之前写的⼤部分核⼼代码仍然可以复⽤，因为它们并没有和装饰器耦合。\n总结 基础与技巧 装饰器最常见的实现方式，是利用闭包原理通过多层嵌套函数实现 在实现装饰器时，请记得使用wrap()更新包装函数的元数据 wraps()不光可以保留元数据，还能保留包装函数的额外属性\n使用类来实现装饰器 只要是可调用的对象，都可以用作装饰器 实现了 call 方法的类实例可调用 基于类的装饰器分为两种：函数替换与实例替换 “函数替换”装饰器与普通装饰器没什么区别，只是嵌套层级更少、 通过类来实现“实例替换”装饰器，在管理状态和追加⾏为上有天然的优势 混合使⽤类和函数来实现装饰器，可以灵活满⾜各种场景\n使用wrapt模块 使用wrapt模块可以方便地让装饰器同时兼容函数和类方法 使⽤ wrapt 模块可以帮你写出结构更扁平的装饰器代码\n装饰器设计技巧 装饰器将包装调用提前到函数被定义的位置，它的大部分优点也源于此 在编写装饰器时，请考虑你的设计是否能很好发挥装饰器的优势 在某些场景下，类装饰器可以替代元类，并且代码更简单 装饰器和装饰器模式截然不同，不要弄混它们 装饰器⾥应该只有⼀层浅浅的包装代码，要把核⼼逻辑放在其他函数与类中\n面向对象编程 实例内容都在字典⾥ 我提到 Python 语⾔内部⼤量使⽤了字典类型，⽐如⼀个类实例的所有成员，其实都保存在了⼀个名为 dict 的字典属性中\n❶ 实例的 dict ⾥，保存着当前实例的所有数据 ❷ 类的 dict ⾥，保存着类的⽂档、⽅法等所有数据\n虽然普通的属性赋值会被 setattr 限制，但如果你直接操作实例的 dict 字典，就可以⽆视这个限制：\n在某些特殊场景下，合理利⽤ dict 属性的这个特性，可以帮你完成常规做法难以做到、的⼀些事情。\n内置类方法装饰器 类方法 不过，虽然普通⽅法⽆法通过类来调⽤，但你可以⽤ @classmethod 装饰器定义⼀种特殊的⽅法：类⽅法（class method），它属于类但是⽆须实例化也可调⽤。\n普通⽅法接收类实例（self）作为参数，但类⽅法的第⼀个参数是类本⾝，通常使⽤名字 cls\n作为⼀种特殊⽅法，类⽅法最常⻅的使⽤场景，就是像上⾯⼀样定义⼯⼚⽅法来⽣成新实例。类⽅法的主⻆是类型本⾝，当你发现某个⾏为不属于实例，⽽是属于整个类型时，可以考虑使⽤类⽅法。\n静态方法 如果你发现某个⽅法不需要使⽤当前实例⾥的任何内容，那可以使⽤ @staticmethod 来定义⼀个静态⽅法。\n静态⽅法不接收当前实例作为第⼀个位置参数\n和普通⽅法相⽐，静态⽅法不需要访问实例的任何状态，是⼀种与状态⽆关的⽅法，因此静态⽅法其实可以改写成脱离于类的外部普通函数。\n选择静态⽅法还是普通函数，可以从以下⼏点来考虑： 如果静态⽅法特别通⽤，与类关系不⼤，那么把它改成普通函数可能会更好； 如果静态⽅法与类关系密切，那么⽤静态⽅法更好； 相⽐函数，静态⽅法有⼀些先天优势，⽐如能被⼦类继承和重写等。\n属性装饰器 在⼀个类⾥，属性和⽅法有着不同的职责：属性代表状态，⽅法代表⾏为。⼆者对外的访问接⼝也不⼀样，属性可以通过 inst.attr 的⽅式直接访问，⽽⽅法需要通过inst.method() 来调⽤。\n@property 除了可以定义属性的读取逻辑外，还⽀持⾃定义写⼊和删除逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class FilePath: @property def basename(self): \u0026#34;\u0026#34;\u0026#34;获取文件名\u0026#34;\u0026#34;\u0026#34; return self.path.rsplit(os.sep,1)[-1] @basename.setter def basename(self,name): \u0026#34;\u0026#34;\u0026#34;修改当前路径里的文件名部分\u0026#34;\u0026#34;\u0026#34; new_path = self.path.rsplit(os.sep,1)[:-1] + [name] self.path = os.sep.join(new_path) @basename.deleter def basename(self): raise RuntimeError(\u0026#39;Can not delete basename!\u0026#39;) ❶ 经过 @property 的装饰以后，basename 已经从⼀个普通⽅法变成了 property对象，因此这⾥可以使⽤ basename.setter ❷ 定义 setter ⽅法，该⽅法会在对属性赋值时被调⽤ ❸ 定义 deleter ⽅法，该⽅法会在删除属性时被调⽤\n@property 是个⾮常有⽤的装饰器，它让我们可以基于⽅法定义类属性，精确地控制属性的读取、赋值和删除⾏为，灵活地实现动态属性等功能。\n⼈们在读取属性时，总是期望能迅速拿到结果，调⽤⽅法则不⼀样——快点⼉慢点⼉都⽆所谓。让⾃⼰设计的接⼝符合他⼈的使⽤预期，也是写代码时很重要的⼀环。\n鸭子类型及其局限性 ⾸先，鸭⼦类型不推荐做类型检查，因此编码者可以省去⼤量与之相关的烦琐⼯作。其次，鸭⼦类型只关注对象是否能完成某件事，⽽不对类型做强制要求，这⼤⼤提⾼了代码的灵活性\n你甚⾄可以从零开始⾃⼰实现⼀个新类型：\n1 2 3 4 5 6 7 8 9 10 11 12 class StringList: \u0026#34;\u0026#34;\u0026#34;用于保存多个字符串的数据类，实现了read()和可迭代接口\u0026#34;\u0026#34;\u0026#34; def __init__(self,string): self.strings = strings def read(self): return \u0026#39;\u0026#39;.join(self.strings) def __iter__(self): for s in self.strings: yield s 虽然上⾯的 StringList 类和⽂件类型⼋竿⼦打不着，但是因为 count_vowels() 函数遵循了鸭⼦类型编程⻛格，⽽ StringList 恰好实现了它所需要的接⼝，因此 StringList 对象也可以完美适⽤于 count_vowels 函数：\n鸭⼦类型的第⼀个缺点是：缺乏标准。在编写鸭⼦类型代码时，虽然我们不需要做严格的类型校验，但是仍然需要频繁判断对象是否⽀持某个⾏为，⽽这⽅⾯并没有统⼀的标准\n鸭⼦类型的另⼀个问题是：过于隐式。在鸭⼦类型编程⻛格下，对象的真实类型变得不再重要，取⽽代之的是对象所提供的接⼝（或者叫协议）变得⾮常重要。但问题是，鸭⼦类型⾥的所有接⼝和协议都是隐式的，它们全藏在代码和函数的注释中\n综合考虑了鸭⼦类型的种种特点后，你会发现，虽然这⾮常有效和实⽤，但有时也会让⼈觉得过于灵活、缺少规范。尤其是在规模较⼤的 Python 项⽬中，如果代码⼤量使⽤了鸭⼦类型，编码者就需要理解很多隐式的接⼝与规则，很容易不堪重负。\n幸运的是，除了鸭⼦类型以外，Python 还为类型系统提供了许多有效的补充，⽐如类型注解与静态检查（mypy）、抽象类（abstract class）等。\n抽象类 抽象类的子类化机制 为了演⽰这个机制，我把前⾯的 Validator 改造成了⼀个抽象类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from abc import ABC class Validator(ABC): \u0026#34;\u0026#34;\u0026#34;校验器抽象类\u0026#34;\u0026#34;\u0026#34; @classmethod def __subclasshook__(cls,C): \u0026#34;\u0026#34;\u0026#34;任何提供了validate方法的类，都被当作Validator的子类\u0026#34;\u0026#34;\u0026#34; if any(\u0026#34;validate\u0026#34; in B.__dict__ for B in C.__mro__): return True return NotImplemented def validate(self,value): raise NotImplementedError ❶ 要定义⼀个抽象类，你需要继承 ABC 类或使⽤ abc.ABCMeta 元类 ❷ C.mro 代表 C 的类派⽣路线上的所有类（⻅ 9.1.5 节）\n上⾯代码的重点是 subclasshook 类⽅法。subclasshook 是抽象类的⼀个特殊⽅法，当你使⽤ isinstance 检查对象是否属于某个抽象类时，如果后者定义了这个⽅法，那么该⽅法就会被触发，然后： 实例所属类型会作为参数传⼊该⽅法（上⾯代码中的 C 参数）； 如果⽅法返回了布尔值，该值表⽰实例类型是否属于抽象类的⼦类； 如果⽅法返回 NotImplemented，本次调⽤会被忽略，继续进⾏正常的⼦类判断逻辑。\n在我编写的 Validator 类中，subclasshook ⽅法的逻辑是：所有实现了validate ⽅法的类都是我的⼦类。\n通过 subclasshook 类⽅法，我们可以定制抽象类的⼦类判断逻辑。这种⼦类化形式只关⼼结构，不关⼼真实继承关系，所以常被称为“结构化⼦类”。\n这也是之前的 ThreeFactory 类能通过 Iterable 类型校验的原因，因为 Iterable 抽象类对⼦类只有⼀个要求：实现了 iter ⽅法即可。\n除了通过 subclasshook 类⽅法来定义动态的⼦类检查逻辑外，你还可以为抽象类⼿动注册新的⼦类。\n❶ 默认情况下，Foo 类和 Validator 类没有任何关系 ❷ 调⽤ .register() 把 Foo 注册为 Validator 的⼦类 ❸ 完成注册后，Foo 类的实例就能通过 Validator 的类型校验了\n总结⼀下，抽象类通过 subclasshook 钩⼦和 .register() ⽅法，实现了⼀种⽐继承更灵活、更松散的⼦类化机制，并以此改变了 isinstance() 的⾏为。\n有了抽象类以后，我们便可以使⽤ isinstance(obj, type) 来进⾏鸭⼦类型编程⻛格的类型校验了。只要待匹配类型 type 是抽象类，类型检查就符合鸭⼦类型编程⻛格——只校验⾏为，不校验类型。\n抽象类的其他功能 除了更灵活的⼦类化机制外，抽象类还提供了⼀些其他功能。⽐如，利⽤ abc 模块的 @abstractmethod 装饰器，你可以把某个⽅法标记为抽象⽅法。假如抽象类的⼦类在继承时，没有重写所有抽象⽅法，那么它就⽆法被正常实例化。\n这个机制可以帮我们更好地控制⼦类的继承⾏为，强制要求其重写某些⽅法。\n此外，虽然抽象类名为抽象，但它也可以像任何普通类⼀样提供已实现好的⾮抽象⽅法。⽐如collections.abc 模块⾥的许多抽象类（如 Set、Mapping 等）像普通基类⼀样实现了⼀些公⽤⽅法，降低了⼦类的实现成本\n最后，我们总结⼀下鸭⼦类型和抽象类：\n鸭⼦类型是⼀种编程⻛格，在这种⻛格下，代码只关⼼对象的⾏为，不关⼼对象的类型； 鸭⼦类型降低了类型校验的成本，让代码变得更灵活； 传统的鸭⼦类型⾥，各种对象接⼝和协议都是隐式的，没有统⼀的显式标准； 普通的 isinstance() 类型检查和鸭⼦类型的理念是相违背的； 抽象类是⼀种特殊的类，它可以通过钩⼦⽅法来定制动态的⼦类检查⾏为； 因为抽象类的定制⼦类化特性，isinstance() 也变得更灵活、更契合鸭⼦类型了； 使⽤ @abstractmethod 装饰器，抽象类可以强制要求⼦类在继承时重写特定⽅法； 除了抽象⽅法以外，抽象类也可以实现普通的基础⽅法，供⼦类继承使⽤； 在 collections.abc 模块中，有许多与容器相关的抽象类。 多重继承与MRO 在解决多重继承的⽅法优先级问题时，Python 使⽤了⼀种名为 MRO（method resolutionorder）的算法。该算法会遍历类的所有基类，并将它们按优先级从⾼到低排好序。\n基于 MRO 算法的基类优先级列表，不光定义了类⽅法的找寻顺序，还影响了另⼀个常⻅的内置函数：super()。\n在许多⼈的印象中，super() 是⼀个⽤来调⽤⽗类⽅法的⼯具函数。但这么说并不准确，super() 使⽤的其实不是当前类的⽗类，⽽是它在 MRO 链条⾥的上⼀个类。\n⼤多数情况下，你需要的并不是多重继承，⽽也许只是⼀个更准确的抽象模型，在该模型下，最普通的继承关系就能完美解决问题。\nMixin模式 顾名思义，Mixin 是⼀种把额外功能“混⼊”某个类的技术。有些编程语⾔（⽐如 Ruby）为Mixin 模式提供了原⽣⽀持，⽽在 Python 中，我们可以⽤多重继承来实现 Mixin 模式。 要实现 Mixin 模式，你需要先定义⼀个 Mixin 类：\n1 2 3 4 5 6 7 8 9 class InfoDumperMixin: \u0026#34;\u0026#34;\u0026#34;Mixin:输出当前实例信息\u0026#34;\u0026#34;\u0026#34; def dump_info(self): d = self.__dict__ print(\u0026#34;Number of members: {}\u0026#34;.format(len(d))) print(\u0026#34;Details:\u0026#34;) for key,value in d.items(): print(f\u0026#39; - {key}: {value}\u0026#39;) Mixin 类名常以“Mixin”结尾，这算是⼀种不成⽂的约定\n相⽐普通类，Mixin 类有⼀些鲜明的特征。\nMixin 类通常很简单，只实现⼀两个功能，所以很多时候为了实现某个复杂功能，⼀个类常常会同时混⼊多个 Mixin 类。另外，⼤多数 Mixin 类不能单独使⽤，它们只有在被混⼊其他类时才能发挥最⼤作⽤\n下⾯是⼀个使⽤ InfoDumperMixin 的例⼦：\n1 2 3 4 class Person(InfoDumperMixin): def __init__(self,name,age): self.name = name self.age = age 1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; p = Person(\u0026#39;jack\u0026#39;,20) \u0026gt;\u0026gt;\u0026gt; p.dump_info() Number of members: 2 Details: - name: jack - age: 20 虽然 Python 中的 Mixin 模式基于多重继承实现，但令 Mixin 区别于普通多重继承的最⼤原因在于：Mixin 是⼀种有约束的多重继承。在 Mixin 模式下，虽然某个类会同时继承多个基类，但⾥⾯最多只会有⼀个基类表⽰真实的继承关系，剩下的都是⽤于混⼊功能的Mixin 类。这条约束⼤⼤降低了多重继承的潜在危害性\n许多流⾏的 Web 开发框架使⽤了 Mixin 模式，⽐如 Django、DRF 等\n假如你想使⽤ Mixin 模式，需要精⼼设计 Mixin 类的职责，让它们和普通类有所区分，这样才能让 Mixin 模式发挥最⼤的潜⼒\n元类 元类是 Python 中的⼀种特殊对象。元类控制着类的创建⾏为，就像普通类控制着实例的创建⾏为⼀样。\ntype 是 Python 中最基本的元类，利⽤ type，你根本不需要⼿动编写 class \u0026hellip; : 代码来创建⼀个类——直接调⽤ type() 就⾏：\n1 2 3 4 5 Foo = type(\u0026#39;Foo\u0026#39;,(),{\u0026#39;bar\u0026#39;:3}) Foo \u0026lt;class \u0026#39;__main__.Foo\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; Foo().bar 3 在调⽤ type() 创建类时，需要提供三个参数，它们的含义如下。\nname:str,需要创建的类名 bases:tuple[Type],包含其他类的元组，代表类的所有基类 attrs:Dict[str,Any],包含所有类成员(属性，方法)的字典 虽然 type 是最基本的元类，但在实际编程中使⽤它的场景其实⽐较少。更多情况下，我们会创建⼀个继承 type 的新元类，然后在⾥⾯定制⼀些与创建类有关的⾏为。\n为了演⽰元类能做什么，代码清单 9-3 实现了⼀个简单的元类，它的主要功能是将类⽅法⾃动转换成属性对象。另外，该元类还会在创建实例时，为其增加⼀个代表创建时间的created_at 属性。\n⽰例元类 AutoPropertyMeta\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import time import types class AutoPropertyMeta(type): \u0026#34;\u0026#34;\u0026#34;元类： - 把所有类方法变成动态属性 - 为所有实例增加创建时间属性 \u0026#34;\u0026#34;\u0026#34; def __new__(cls,name,bases,attrs): for key,value in attrs.items(): if isinstance(value,types.FunctionType) and not key.startswith(\u0026#39;_\u0026#39;): attrs[key] = property(value) return super().__new__(cls,name,bases,attrs) def __call__(cls,*args,**kwargs) inst = super().__call__(*args,**kwargs) inst.created_at = time.time() return inst 元类通常会继承基础元类 type 对象 元类的 new ⽅法会在创建类时被调⽤ 将⾮私有⽅法转换为属性对象 调⽤ type() 完成真正的类创建 元类的 call ⽅法，负责创建与初始化类实例\n下⾯的 Cat 类使⽤了 AutoPropertyMeta 元类：\n1 2 3 4 5 6 7 8 9 import random class Cat(metaclass=AutoPropertyMeta): def __init__(self,name): self.name = name def sound(self): repeats = random.randrange(1,10) return \u0026#39; \u0026#39;.join([\u0026#39;Meow\u0026#39;] * repeats) 效果如下：\nmilo = Cat(\u0026lsquo;milo\u0026rsquo;) milo.sound ❶ \u0026lsquo;Meow Meow Meow Meow Meow Meow Meow\u0026rsquo; milo.created_at ❷ 1615000104.0704262\nsound 原本是⽅法，但是被元类⾃动转换成了属性对象 读取由元类定义的创建时间\n通过上⾯这个例⼦，你会发现元类的功能相当强⼤，它不光可以修改类，还能修改类的实例。\n同时它也相当复杂，⽐如在例⼦中，我只简单演⽰了元类的 new 和 call ⽅法，除此之外，元类其实还有⽤来准备类命名空间的 prepare ⽅法\n和 Python ⾥的其他功能相⽐，元类是个相当⾼级的语⾔特性。通常来说，除⾮要开发⼀些框架类⼯具，否则你在⽇常⼯作中根本不需要⽤到元类。\n元类是⼀种深奥的“魔法”，99% 的⽤⼾不必为之操⼼。如果你在琢磨是否需要元类，那你肯定不需要（那些真正要使⽤元类的⼈确信⾃⼰的需求，⽽⽆须解释缘由）\n元类很少被使⽤的原因，除了应⽤场景少以外，还在于它其实有许多“替代品”，它们是：\n类装饰器 init_subclass 钩⼦⽅法 描述符 继承是把双刃剑 Unique Visitor 的⾸字⺟缩写，表⽰访问⽹站的独⽴访客。对于如何统计独⽴访客，常⻅的算法是把每个注册⽤⼾算作⼀个独⽴访客，或者把每个 IP 地址算作⼀个独⽴访客\n⼩ Y ⾛后，⼩ R 开始写起了代码。要基于⽇志来统计每天的 UV 数，程序⾄少需要做到这⼏件事：获取⽇志内容、解析⽇志、完成统计\n代码清单 9-4 统计某⽇ UV 数的类 UniqueVisitorAnalyzer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class UniqueVisitorAnalyzer: \u0026#34;\u0026#34;\u0026#34;统计某日UV数 :param date:需要统计的日期 \u0026#34;\u0026#34;\u0026#34; def __init__(self,date)： self.date = date def analyze(self): \u0026#34;\u0026#34;\u0026#34;通过解析与分析API访问日志，返回UV数 :return:uv数 \u0026#34;\u0026#34;\u0026#34; for entry in self.get_log_entries(): ... # 省略：根据 entry.user_id 统计 UV 数并返回结果 def get_log_entries(self): \u0026#34;\u0026#34;\u0026#34;获取当天所有日志记录\u0026#34;\u0026#34;\u0026#34; for line in self.read_log_lines(): yield self.parse_log(line) def read_log_lines(self): \u0026#34;\u0026#34;\u0026#34;逐⾏获取访问⽇志\u0026#34;\u0026#34;\u0026#34; ... # 省略：根据⽇志 self.date 读取⽇志⽂件并返回结果 def parse_log(self,line): \u0026#34;\u0026#34;\u0026#34;将纯文本格式的日志解析为结构化对象 :param line:纯文本格式日志 :return: 结构化的日志条目LogEntry对象 \u0026#34;\u0026#34;\u0026#34; ... # 省略：复杂的⽇志解析过程 return LogEntry( time=..., ip=..., path=..., user_agent=..., user_id=..., ) 所以，⼩ R 决定通过继承来复⽤ UniqueVisitorAnalyzer 类⾥的⽇志读取和解析逻辑，这样他只要写很少的代码就能完成需求。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Top10CommentsAnalyzer(UniqueVisitorAnalyzer): \u0026#34;\u0026#34;\u0026#34;获取某日点赞量最高的10条评论 :param date:需要统计的日期 \u0026#34;\u0026#34;\u0026#34; limit = 10 def analyze(self): \u0026#34;\u0026#34;\u0026#34;通过解析与统计 API 访问⽇志，返回点赞量最⾼的评论 :return:评论ID列表 \u0026#34;\u0026#34;\u0026#34; for entry in self.get_log_entries(): comment_id = self.extract_comment_id(entry.path) ... # 省略：统计过程与返回结果 def extract_comment_id(self,path): \u0026#34;\u0026#34;\u0026#34; 根据日志访问路径，获取评论ID 有效的评论点赞API路径格式：/comments/\u0026lt;ID\u0026gt;/up_votes/ :return:仅当路径是评论点赞 API 时，返回 ID，否则返回 None \u0026#34;\u0026#34;\u0026#34; matched_obj = re.match(\u0026#39;/comments/(.*)/up_votes/\u0026#39;.path) return matched_obj and matched_obj.group(1) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import re class UniqueVisitorAnalyzer: ... def get_log_entries(self): \u0026#34;\u0026#34;\u0026#34;获取当天所有日志记录\u0026#34;\u0026#34;\u0026#34; for line in self.read_log_lines(): entry = self.parse_log(line) if not self.match_news_pattern(entry.path): continue yield entry def match_news_pattern(self,path): \u0026#34;\u0026#34;\u0026#34;判断API路径是不是在访问新闻 :param path: API 访问路径 :return: bool \u0026#34;\u0026#34;\u0026#34; return re.match(r\u0026#39;^/news/[^/]*?/$\u0026#39;,path) 于是⼩ R 打开统计热⻔评论的代码，很快就找到了问题的原因： ①修改⽗类函数，②⼦类受到影响\n但继承是⼀种类与类之间紧密的耦合关系。让⼦类继承⽗类，虽然看上去毫⽆成本地获取了⽗类的全部能⼒，但同时也意味着，从此以后⽗类的所有改动都可能影响⼦类。继承关系越复杂，这种影响就越容易超出⼈们的控制范围。\n⼩ R 使⽤继承的初衷，是为了复⽤⽗类中的⽅法。但如果只是为了复⽤代码，其实没有必要使⽤继承。当⼩ R 发现新需求要⽤到 UniqueVisitorAnalyzer 类的“读取⽇志”“解析⽇志”⾏为时，他完全可以⽤组合（composition）的⽅式来解决复⽤问题。\n要⽤组合来复⽤ UniqueVisitorAnalyzer 类，我们需要先分析这个类的职责与⾏为。在我看来，UniqueVisitorAnalyzer 类主要负责以下⼏件事\n读取日志:根据日期找到并读取日志文件 解析日志:把文本日志信息解析并转化成LogEntry 统计日志:统计日志，计算UV数 基于这些事情，我们可以对 UniqueVisitorAnalyzer 类进⾏拆分，把其中需要复⽤的两个⾏为创建为新的类：\nLogReader 和 LogParser 两个新类，分别对应 UniqueVisitorAnalyzer 类⾥的“读取⽇志”和“解析⽇志”⾏为。\n相⽐之前把所有⾏为都放在 UniqueVisitorAnalyzer 类⾥的做法，新的代码其实体现了另⼀种⾯向对象建模⽅式——针对事物的⾏为建模，⽽不是对事物本⾝建模\n在多数情况下，基于事物的⾏为来建模，可以孵化出更好、更灵活的模型设计。\n继承是⼀种极为紧密的耦合关系。为了避免继承惹来⿇烦，每当你想创建新的继承关系时，应该试着问⾃⼰⼏个问题。\n我要让 B 类继承 A 类，但 B 和 A 真的代表同⼀种东西吗？如果它俩不是同类，为什么要继承？ 即使 B 和 A 是同类，那它们真的需要⽤继承来表明类型关系吗？要知道，Python 是鸭⼦类型的，你不⽤继承也能实现多态 如果继承只是为了让 B 类复⽤ A 类的⼏个⽅法，那么⽤组合来替代继承会不会更好？ 同样是复⽤代码，组合产⽣的耦合关系⽐继承松散得多。如果组合可以达到复⽤⽬的，并且能够很好表达事物间的联系，那么常常是更好的选择。这也是⼈们常说“多⽤组合，少⽤继承”的原因\n但这并不代表我们应该完全弃⽤继承。继承所提供的强⼤复⽤能⼒，仍然是组合所⽆法替代的。许多设计模式（⽐如模板⽅法模式——template method pattern）都是依托继承来实现的\n使用 __init__subclass__替代元类 init_subclass 是类的⼀个特殊钩⼦⽅法，它的主要功能是在类派⽣出⼦类时，触发额外的操作。假如某个类实现了这个钩⼦⽅法，那么当其他类继承该类时，钩⼦⽅法就会被触发。\n通过上⾯的例⼦，你会发现 init_subclass ⾮常适合在这种需要触达所有⼦类的场景中使⽤。⽽且同元类相⽐，钩⼦⽅法只要求使⽤者了解继承，不⽤掌握更⾼深的元类相关知识，⻔槛低了不少。它和类装饰器⼀样，都可以有效替代元类。\n在分支中寻找多态的应用时机 当你发现⾃⼰的代码出现以下特征时：\n有许多 if/else 判断，并且这些判断语句的条件都⾮常类似； 有许多针对类型的 isinstance() 判断逻辑。 你应该问⾃⼰⼀个问题：代码是不是缺少了某种抽象？如果增加这个抽象，这些分布在各处的条件分⽀，是不是可以⽤多态来表现？如果答案是肯定的，那就去找到那个抽象吧！\n有序组织你的类方法 作为惯例，init 实例化⽅法应该总是放在类的最前⾯，new ⽅法同理。\n公有⽅法应该放在类的前⾯，因为它们是其他模块调⽤类的⼊⼝，是类的⻔⾯，也是所有⼈最关⼼的内容。以 _ 开头的私有⽅法，⼤部分是类⾃⾝的实现细节，应该放在靠后的\n⾄于类⽅法、静态⽅法和属性对象，你不必将它们区分对待，直接参考公有 / 私有的思路即可。⽐如，⼤部分类⽅法是公有的，所有它们通常会⽐较靠前。⽽静态⽅法常常是内部使⽤的私有⽅法，所以常放在靠后的位置。\n以 __ 开头的魔法⽅法⽐较特殊，我通常会按照⽅法的重要程度来决定它们的位置。⽐如⼀个迭代器类的 iter ⽅法应该放在⾮常靠前的位置，因为它是构成类接⼝的重要⽅法。\n最后⼀点，当你从上往下阅读类时，所有⽅法的抽象级别应该是不断降低的，就好像阅读⼀篇新闻⼀样，第⼀段是新闻的概要，之后才会描述细节。\n⽤函数降低 API 使⽤成本 在 Python 中，像上⾯这种⽤函数搭配⾯向对象的代码⾮常多⻅，它有点⼉像设计模式中的外观模式（facade pattern）。在该模式中，函数作为⼀种简化 API 的⼯具，封装了复杂的⾯向对象功能，⼤⼤降低了使⽤成本。\n实现预绑定方法模式 假设你在开发⼀个程序，它的所有配置项都保存在⼀个特定⽂件中。在项⽬启动时，程序需要从配置⽂件中读取所有配置项，然后将其加载进内存供其他模块使⽤。\n由于程序执⾏时只需要⼀个全局的配置对象，因此你觉得这个场景⾮常适合使⽤经典设计模式：单例模式（singleton pattern）。\n下⾯的代码就应⽤了单例模式的配置类 AppConfig：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class AppConfig: \u0026#34;\u0026#34;\u0026#34;程序配置类，使用单例模式\u0026#34;\u0026#34;\u0026#34; _instance = None def __new__(cls): if cls._instance is None: inst = super().__new__(cls) # 省略：从外部配置⽂件读取配置 ... cls._instance = inst return cls._instance def get_database(self): \u0026#34;\u0026#34;\u0026#34;读取数据库配置\u0026#34;\u0026#34;\u0026#34; ... def reload(self): \u0026#34;\u0026#34;\u0026#34;重新读取配置文件，刷新配置\u0026#34;\u0026#34;\u0026#34; ... 在 Python 中，实现单例模式的⽅式有很多，⽽上⾯这种最为常⻅，它通过重写类的__new__ ⽅法来接管实例创建⾏为。当 new ⽅法被重写后，类的每次实例化返回的不再是新实例，⽽是同⼀个已经初始化的旧实例 cls._instance\nc1 = AppConfig() c2 = AppConfig() c1 is c2 ❶ True\n❶ 测试单例模式，调⽤ AppConfig() 总是会产⽣同⼀个对像\n基于上⾯的设计，如果其他⼈想读取数据库配置，代码需要这样写：\n1 2 3 4 5 6 from project.config import AppConfig db_conf = AppConfig().get_database() # 重新加载配置 AppConfig().reload() 虽然在处理这种全局配置对象时，单例模式是⼀种⾏之有效的解决⽅案，但在 Python 中，其实有⼀种更简单的做法——预绑定⽅法模式\n预绑定方法模式是一种将对象方法绑定为函数的模式，要实现该模式，第一步就是完全删掉 AppConfig ⾥的单例设计模式。因为在 Python ⾥，实现单例压根⼉不⽤这么⿇烦，我们有⼀个随⼿可得的单例对象——模块（module）\n当你在 Python 中执⾏ import 语句导⼊模块时，⽆论 import 执⾏了多少次，每个被导⼊的模块在内存中只会存在⼀份（保存在 sys.modules 中）。因此，要实现单例模式，只需在模块⾥创建⼀个全局对象即可\n1 2 3 4 5 6 7 8 class AppConfig: \u0026#34;\u0026#34;\u0026#34;程序配置类，使用单例模式\u0026#34;\u0026#34;\u0026#34; def __init__(self): # 省略：从外部配置⽂件读取配置 ... _config = AppConfig() ❶ 完全删掉单例模式的相关代码，只实现 init ⽅法 ❷ _config 就是我们的“单例 AppConfig 对象”，它以下划线开头命名，表明⾃⼰是⼀个私有全局变量，以免其他⼈直接操作\n下⼀步，为了给其他模块提供好⽤的 API，我们需要将单例对象 _config 的公有⽅法绑定到 config模块上：\n1 2 3 4 5 # file: project/config.py _config = Config() get_database_conf = _config.get_database reload_config = _config.reload 之后，其他模块就可以像调⽤普通函数⼀样操作应⽤配置对象了：\n1 2 3 4 from project.config import get_ddatabase_conf db_conf = get_database_conf() reload_config() 通过“预绑定⽅法模式”，我们既避免了复杂的单例设计模式，⼜有了更易使⽤的函数 API，可谓⼀举两得。\n总结 语⾔基础知识 类与实例的数据，都保存在⼀个名为 dict 的字典属性中 灵活利⽤ dict 属性，能帮你做到常规做法难以完成的⼀些事情 使⽤ @classmethod 可以定义类⽅法，类⽅法常⽤作⼯⼚⽅法 使⽤ @staticmethod 可以定义静态⽅法，静态⽅法不依赖实例状态，是⼀种⽆状态⽅法 使⽤ @property 可以定义动态属性对象，该属性对象的获取、设置和删除⾏为都⽀持⾃定义 面向对象高级特性 Python 使⽤ MRO 算法来确定多重继承时的⽅法优先级 super() 函数获取的并不是当前类的⽗类，⽽是当前 MRO 链条⾥的下⼀个类 Mixin 是⼀种基于多重继承的有效编程模式，⽤好 Mixin 需要精⼼的设计 元类的功能相当强⼤，但同时也相当复杂，除⾮开发⼀些框架类⼯具，否则你极少需要使⽤元类 元类有许多更简单的替代品，⽐如类装饰器、⼦类化钩⼦⽅法等 通过定义 init_subclass 钩⼦⽅法，你可以在某个类被继承时执⾏⾃定义逻辑 鸭⼦类型与抽象类 鸭鸭“鸭⼦类型”是 Python 语⾔最为鲜明的特点之⼀，在该⻛格下，⼀般不做任何严格的类型检查 虽然“鸭⼦类型”⾮常实⽤，但是它有两个明显的缺点——缺乏标准和过于隐式 抽象类提供了⼀种更灵活的⼦类化机制，我们可以通过定义抽象类来改变 isinstance() 的⾏为 通过 @abstractmethod 装饰器，你可以要求抽象类的⼦类必须实现某个⽅法 面向对象设计 继承提供了相当强⼤的代码复⽤机制，但同时也带来了⾮常紧密的耦合关系’ 错误使⽤继承容易导致代码失控 对事物的⾏为⽽不是事物本⾝建模，更容易孵化出好的⾯向对象设计 在创建继承关系时应当谨慎。⽤组合来替代继承有时是更好的做法 函数与面向对象的配合 Python ⾥的⾯向对象不必特别纯粹，假如⽤函数打⼀点⼉配合，你可以设计出更好的代码 可以像 requests 模块⼀样，⽤函数为⾃⼰的⾯向对象模块实现⼀些更易⽤的 API 在 Python 中，我们极少会应⽤真正的“单例模式”，⼤多数情况下，⼀个简单的模块级全局对象就够了 使⽤“预绑定⽅法模式”，你可以快速为普通实例包装出类似普通函数的 API 代码编写细节 Python 的成员私有协议并不严格，如果你想标⽰某个属性为私有，使⽤单下划线前缀就够了 编写类时，类⽅法排序应该遵循某种特殊规则，把读者最关⼼的内容摆在最前⾯ 多态是⾯向对象编程⾥的基本概念，同时也是最强⼤的思维⼯具之⼀ 多态可能的介⼊时机：许多类似的条件分⽀判断、许多针对类型的 isinstance() 判断 面向对象设计原则(上) 《设计模式》中的⼤部分设计模式是作者⽤静态编程语⾔，在⼀个有着诸多限制的⾯向对象环境⾥创造出来的。⽽ Python 是⼀⻔动态到⻣⼦⾥的编程语⾔，它有着⼀等函数对象、“鸭⼦类型”、可⾃定义的数据模型等各种灵活特性。因此，我们极少会⽤ Python 来⼀⽐⼀还原经典设计模式，⽽⼏乎总是会为每种设计模式找到更适合 Python 的表现形式\nSOLID 单词⾥的 5 个字⺟，分别代表 5 条设计原则。\nS：single responsibility principle（单⼀职责原则，SRP）。 O：open-closed principle（开放–关闭原则，OCP）。 L：Liskov substitution principle（⾥式替换原则，LSP）。 I：interface segregation principle（接⼝隔离原则，ISP）。 D：dependency inversion principle（依赖倒置原则，DIP）。 类型注解基础 typing 是类型注解⽤到的主要模块，除了 List 以外，该模块内还有许多与类型有关的特殊对象，举例如下。\nDict：字典类型，例如 Dict[str, int] 代表键为字符串，值为整型的字典。 Callable：可调⽤对象，例如 Callable[[str, str], List[str]] 表⽰接收两个字符串作为参数，返回字符串列表的可调⽤对象。 TextIO：使⽤⽂本协议的类⽂件类型，相应地，还有⼆进制类型 BinaryIO。 Any：代表任何类型。 SRP：单⼀职责原则 SRP 认为：⼀个类应该仅有⼀个被修改的理由。换句话说，每个类都应该只承担⼀种职责。\n违反 SRP 的坏处 单个类承担的职责越多，就意味着这个类越复杂，越难维护。在⾯向对象领域，有⼀种“臭名昭著”的类：God Class，专指那些包含了太多职责、代码特别多、什么事情都能做的类。GodClass 是所有程序员的噩梦，每个理智尚存的程序员在碰到 God Class 后，第⼀个想法总是逃跑，逃得越远越好\n违反 SRP 的坏处说了⼀箩筐，那么，究竟怎么修改脚本才能让它符合 SRP 呢？办法有很多，其中最传统的就是把⼤类拆分为⼩类。\n大类拆小类 单⼀职责是⾯向对象领域的设计原则，通常⽤来形容类。⽽在 Python 中，单⼀职责的适⽤范围不限于类——通过定义函数，我们同样能让上⾯的代码符合单⼀职责原则\n将某个职责拆分为新函数是⼀个具有 Python 特⾊的解决⽅案。它虽然没有那么“⾯向对象”，却⾮常实⽤，甚⾄在许多场景下⽐编写类更简单、更⾼效\nOCP：开放 - 关闭原则 该原则认为：类应该对扩展开放，对内修改封闭。换句话说，你可以在不修改某个类的前提下，扩展它的⾏为。\n现在，假如我想改变 sorted() 的排序逻辑，⽐如，让它使⽤所有元素对 3 取模后的结果排序。我是不是得去修改 sorted() 函数的源码呢？当然不⽤，我只要在调⽤函数时，传⼊⾃定义的key 参数就⾏了：\n通过上⾯的例⼦可以发现，sorted() 函数是⼀个符合 OCP 的绝佳例⼦，原因如下。\n对外扩展开放：可以通过传⼊⾃定义 key 参数来扩展它的⾏为。 对内修改关闭：⽆须修改 sort() 函数本⾝ 。 正如古希腊哲学家赫拉克利特所⾔：这世间唯⼀不变的，只有变化本⾝。\n通过继承改造代码 继承与 OCP 之间有着重要的联系。继承允许我们⽤⼀种新增⼦类⽽不是修改原有类的⽅式来扩展程序的⾏为，这恰好符合 OCP。⽽要做到有效地扩展，关键点在于先找到⽗类中不稳定、会变动的内容。只有将这部分变化封装成⽅法（或属性），⼦类才能通过继承重写这部分⾏为。\n在这个框架下，只要需求变化和“⽤⼾对条⽬是否感兴趣”有关，我都不需要修改原本的HNTopPostsSpider ⽗类，⽽只要不断地在其基础上创建新的⼦类即可。通过继承，我最终实现了OCP 所说的“对扩展开放，对改变关闭”\n通过组合与依赖注入 虽然继承功能强⼤，但它并⾮通往 OCP 的唯⼀途径。除了继承外，我们还可以采⽤另⼀种思路：组合（composition）。更具体地说，使⽤基于组合思想的依赖注⼊（dependency injection）技术。\n与继承不同，依赖注⼊允许我们在创建对象时，将业务逻辑中易变的部分（常被称为“算法”）通过初始化参数注⼊对象⾥，最终利⽤多态特性达到“不改代码来扩展类”的效果。\n如之前所分析的，在这个脚本⾥，“条⽬过滤算法”是业务逻辑⾥的易变部分。要实现依赖注⼊，我们需要先对过滤算法建模。\n⾸先定义⼀个名为 PostFilter 的抽象类：\n1 2 3 4 5 6 7 from abc import ABC,abstractmethod class PostFilter(ABC): \u0026#34;\u0026#34;\u0026#34;抽象类:定义如何过滤帖子结果\u0026#34;\u0026#34;\u0026#34; @abstractmethod def validate(self,post:Post) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;判断帖子是否应该保留\u0026#34;\u0026#34;\u0026#34; 随后，为了实现脚本的原始逻辑：不过滤任何条⽬，我们创建⼀个继承该抽象类的默认算法类DefaultPostFilter，它的过滤逻辑是保留所有结果。\n要实现依赖注⼊，HNTopPostsSpider 类也需要做⼀些调整，它必须在初始化时接收⼀个名为post_filter 的结果过滤器对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class DefaultPostFilter(PostFilter): \u0026#34;\u0026#34;\u0026#34;保留所有帖⼦\u0026#34;\u0026#34;\u0026#34; def validate(self, post: Post) -\u0026gt; bool: return True class HNTopPostsSpider: \u0026#34;\u0026#34;\u0026#34;抓取 Hacker News Top 内容条⽬ :param limit: 限制条⽬数，默认为 5 :param post_filter: 过滤结果条⽬的算法，默认保留所有 \u0026#34;\u0026#34;\u0026#34; items_url = \u0026#39;https://news.ycombinator.com/\u0026#39; def __init__(self, limit: int = 5, post_filter: Optional[PostFilter] = None): self.limit = limit self.post_filter = post_filter or DefaultPostFilter() ❶ def fetch(self) -\u0026gt; Iterable[Post]: # ... counter = 0 for item in items: # ... post = Post(...) # 使⽤测试⽅法来判断是否返回该帖⼦ if self.post_filter.validate(post): counter += 1 yield post 因为HNTopPostsSpider 类所依赖的过滤器是通过初始化参数注入的，所以这个技术被称为依赖注入\n通过依赖注入实现OCP 抽象类不是必需的\n类型注解会让 Python 更接近静态语⾔。启⽤类型注解，你就必须时刻寻找那些能作为注解的实体类型。类型注解会强制我们把⼤脑⾥的隐式“接⼝”和“协议”显式地表达出来。\n使用数据驱动 在实现 OCP 的众多⼿法中，除了继承与依赖注⼊外，还有另⼀种常⽤⽅式：数据驱动。它的核⼼思想是：将经常变动的部分以数据的⽅式抽离出来，当需求变化时，只改动数据，代码逻辑可以保持不动。\n听上去数据驱动和依赖注⼊有点⼉像，它们都是把变化的东西抽离到类外部。⼆者的不同点在于：依赖注⼊抽离的通常是类，⽽数据驱动抽离的是纯粹的数据。\n影响每种⽅案可定制性的根本原因在于，各⽅案所处的抽象级别不⼀样。⽐如，在依赖注⼊⽅案下，我选择抽象的内容是“条⽬过滤⾏为”；⽽在数据驱动⽅案下，抽象内容则是“条⽬过滤⾏为的有效站点地址”。很明显，后者的抽象级别更低，关注的内容更具体，所以灵活性不如前者。\n总结 SRP 一个类只应该有一种被修改的原因 编写更小的类通常更不容易违反SRP SRP同样适用于函数，你可以让函数和类协同工作 OCP 类应该对内修改关闭，对外扩展开放 通过分析需求，找到代码中易变的部分，是让类符合 OCP 的关键 使用子类继承的方式可以让类符合OCP 通过算法类与依赖注入，也可以让类符合OCP 将数据与逻辑分离，使用数据驱动的方式也是实践OCP的好办法 面向对象设计原则(下) LSP：⾥式替换原则 给定⼀个属于类型 T 的对象 x，假如 q(x) 成⽴，那么对于 T 的⼦类型 S 来说，S类型的任意对象 y 也都能让 q(y) 成⽴。\n这⾥⽤⼀种更通俗的⽅式来描述 LSP：LSP 认为，所有⼦类（派⽣类）对象应该可以任意替代⽗类（基类）对象使⽤，且不会破坏程序原本的功能。\n要让⼦类符合 LSP，我们必须让⽤⼾类 User 的“不⽀持停⽤”特性变得更显式，最好将其设计到⽗类协议⾥去，⽽不是让⼦类随⼼所欲地抛出异常。\n虽然在 Python ⾥，根本没有“⽗类的异常协议”这种东西，但我们⾄少可以做两件事。\n第⼀件事是创建⾃定义异常类。我们可以为“⽤⼾不⽀持停⽤”这件事创建⼀个专⽤的异常类：\n第⼆件事是在⽗类 User 和⼦类 Admin 的⽅法⽂档⾥，增加与抛出异常相关的说明：\n⽐如，我可以调整 deactivate_users() ⽅法，让它在每次调⽤ deactivate() 时都显式地捕获异常：\n1 2 3 4 5 6 7 8 9 def deactivate_users(users: Iterable[User]): \u0026#34;\u0026#34;\u0026#34;批量停⽤多个⽤⼾\u0026#34;\u0026#34;\u0026#34; for user in users: try: user.deactivate() except DeactivationNotSupported: logger.info( f\u0026#39;user {user.username} does not allow deactivating, skip.\u0026#39; ) 只要遵循⽗类的异常规范，当前的⼦类 Admin 对象以及未来可能出现的其他⼦类对象，都可以替代 User 对象。通过对异常做了⼀些微调，我们最终让代码满⾜了 LSP 的要求。\n子类随意调整方法参数与返回值 通过上⼀节内容我们了解到，当⼦类⽅法随意抛出⽗类不认识的异常时，代码就会违反 LSP。除此之外，还有两种常⻅的违反 LSP 的情况，分别和⼦类⽅法的返回值与参数有关。\n调整返回值以符合LSP 但要符合 LSP，⼦类⽅法与⽗类⽅法所返回的结果不能只是碰巧有⼀些共性。LSP 要求⼦类⽅法的返回值类型与⽗类完全⼀致，或者返回⽗类结果类型的⼦类对象\n假如我把之前两个类的⽅法返回值调换⼀下，让⽗类 User 的 list_related_posts()⽅法返回 Iterable[int] 对象，让⼦类 Admin 的⽅法返回 List[int] 对象，这样的设计就完全符合 LSP，因为 List 是 Iterable 类型的⼦类：\n列表（以及所有容器类型）都是 Iterable（可迭代类型抽象类）的⼦类\n在这种情况下，当我⽤ Admin 对象替换 User 对象时，虽然⽅法返回值类型变了，但新的返回值⽀持旧返回值的所有操作（List ⽀持 Iterable 类型的所有操作——可迭代）。因此，所有依赖旧返回值（Iterable）的代码，都能拿着新的⼦类返回值（List）继续正常执⾏。\n方法参数违反LSP 简单来说，要让⼦类符合 LSP，⼦类⽅法的参数必须与⽗类完全保持⼀致，或者，⼦类⽅法所接收的参数应该⽐⽗类更为抽象，要求更为宽松\n第⼀条很好理解。⼤多数情况下，我们的⼦类⽅法不应该随意改动⽗类⽅法签名，否则就会违背 LSP\n不过，当⼦类⽅法参数与⽗类不⼀致时，有些特殊情况其实仍然可以满⾜ LSP。\n第⼀类情况是，⼦类⽅法可以接收⽐⽗类更多的参数，只要保证这些新增参数是可选的即可\n⼦类新增了可选参数 include_hidden，保证了与⽗类兼容。当其他⼈把 Admin对象当作 User 使⽤时，不会破坏程序原本的功能\n第⼆类情况是，⼦类与⽗类参数⼀致，但⼦类的参数类型⽐⽗类的更抽象：\n简单总结⼀下，前⾯我展⽰了违反 LSP 的⼏种常⻅⽅式：\n子类抛出父类所不认识的异常类型 子类的方法返回值类型与父类不同，并且该类型不是父类返回值类型的子类 子类的方法参数与父类不同，并且参数要求没有变得更宽松（可选参数）、同名参数没有更抽象 基于隐式合约违反LSP 在 Rectangle 类的设计中，有⼀个隐式的合约：⻓⽅形的宽和⾼应该总是可以单独修改，不会互相影响。上⾯的测试代码正是这个合约的⼀种表现形式\n在这个场景下，⼦类 Square 对象并不能替换 Rectangle 使⽤，因此代码违反了 LSP。在真实项⽬中，这种因⼦类打破隐式合约违反 LSP 的情况，相⽐其他原因来说更难察觉，尤其需要当⼼\n假如这些⼦类不符合 LSP，那么⾯向对象所提供给我们的最⼤好处之⼀——多态，就不再可靠，变成了⼀句空谈。LSP 能促使我们设计出更合理的继承关系，将多态的潜能更好地激发出来。\n在编写代码时，假如你发现⾃⼰的设计违反了 LSP，就需要竭尽所能解决这个问题。有时你得在⽗类中引⼊新的异常类型，有时你得尝试⽤组合替代继承，有时你需要调整⼦类的⽅法参数。总之，只要深⼊思考类与类之间的关系，总会找到正确的解法。\nDIP:依赖倒置原则 不论多复杂的程序，都是由⼀个个模块组合⽽成的。当你告诉别⼈：“我正在写⼀个很复杂的程序”时，你其实并不是直接在写那个程序，⽽是在逐个完成它的模块，最后⽤这些模块组成程序\n在⽤模块组成程序的过程中，模块间⾃然产⽣了依赖关系。举个例⼦，你的个⼈博客站点可能依赖 Flask 模块，⽽ Flask 依赖 Werkzeug，Werkzeug ⼜由多个低层模块组成。\n在正常的软件架构中，模块间的依赖关系应该是单向的，⼀个⾼层模块往往会依赖多个低层模块。整个依赖图就像⼀条蜿蜒⽽下、不断分叉的河流。\nDIP 是⼀条与依赖关系相关的原则。它认为：高层模块不应该依赖底层模块，二者都应该依赖抽象\n乍⼀看，这个原则有些违反我们的常识——⾼层模块不就是应该依赖低层模块吗？还记得第⼀堂编程课上，在我学会编写 Hello World 程序时，⾼层模块（main() 函数）分明依赖了低层模块（printf()）。\n你可以发现，上⾯的单元测试暴露了 SiteSourceGrouper 类的⼀个问题：它的执⾏链路依赖 requests 模块和⽹络条件，这严格限制了单元测试的执⾏环境。\n使用mock模块 mock 是测试领域的⼀个专有名词，代表⼀类特殊的测试假对象。\n假如你的代码依赖了其他模块，但你在执⾏单元测试时不想真正调⽤这些依赖的模块，那么你可以选择⽤⼀些特殊对象替换真实模块，这些⽤于替换的特殊对象常被统称为 mock。\n在 Python ⾥，单元测试模块 unittest 为我们提供了⼀个强⼤的 mock ⼦模块，⾥⾯有许多和 mock 技术有关的⼯具，如下所⽰\nMock：mock 主类型，Mock() 对象被调⽤后不执⾏任何逻辑，但是会记录被调⽤的情况——包括次数、参数等。 MagicMock：在 Mock 类的基础上追加了对魔法⽅法的⽀持，是 patch() 函数所使⽤的默认类型。 patch()：补丁函数，使⽤时需要指定待替换的对象，默认使⽤⼀个 MagicMock() 替换原始对象，可当作上下⽂管理器或装饰器使⽤。 对于我的脚本来说，假如⽤ unittest.mock 模块来编写单元测试，我需要做以下⼏件事：\n(1) 把⼀份正确的 Hacker News ⻚⾯内容保存为本地⽂件 static_hn.html； (2) ⽤ mock 对象替换真实的⽹络请求⾏为； (3) 让 mock 对象返回⽂件 static_hn.html 的内容。 使⽤ mock 的测试代码如下所⽰：\n1 2 3 4 5 6 7 8 9 10 11 from unittest import mock @mock.patch(\u0026#39;hn_site_grouper.requests.get\u0026#39;) def test_grouper_returning_valid_type(mocked_get): \u0026#34;\u0026#34;\u0026#34;测试 get_groups 是否返回了正确类型\u0026#34;\u0026#34;\u0026#34; with open(\u0026#39;static_hn.html\u0026#39;,\u0026#39;r\u0026#39;) as fp: mocked_get.return_value.text = fp.read() grouper = SiteSourceGrouperO(\u0026#39;https://news.ycombinator.com/\u0026#39;) result = grouper.get_groups() assert isinstance(result,Counter),\u0026#34;groups should be Counter instance\u0026#34; ❶ 通过 patch 装饰器将 requests.get 函数替换为⼀个 MagicMock 对象 ❷ 该 MagicMock 对象将会作为函数参数被注⼊ ❸ 将 get() 函数的返回结果（⾃动⽣成的另⼀个 MagicMock 对象）的 text 属性替换为来⾃本地⽂件的内容\n通过 mock 技术，我们最终让单元测试不再依赖⽹络环境，可以成功地在 CI 环境中执⾏。\n当我们编写单元测试时，有⼀条⾮常重要的指导原则：测试程序的⾏为，⽽不是测试具体实现。\n正因为如此，mock 应该总是被当作⼀种应急的技术，⽽不是⼀种低成本、让单元测试能快速开展的⼿段。\n实现DIP DIP ⾥的“抽象”特指编程语⾔⾥的⼀类特殊对象，这类对象只声明⼀些公开的 API，并不提供任何具体实现。⽐如在 Java 中，接⼝就是⼀种抽象 下⾯是⼀个提供“画”动作的接⼝：\n1 2 3 interface Drawable { public void draw(); } ⽽ Python ⾥并没有上⾯这种接⼝对象，但有⼀个和接⼝⾮常类似的东西——抽象类\n1 2 3 4 5 6 from abc import ABC,abstractmethod class Drawable(ABC): @abstracctmethod def draw(self): pass 搞清楚“抽象”是什么后，接着就是 DIP ⾥最重要的⼀步：设计抽象，其主要任务是确定这个抽象的职责与边界。\n在上⾯的脚本⾥，⾼层模块主要依赖 requests 模块做了两件事 (1) 通过 requests.get() 获取响应 response 对象； (2) 利⽤ response.text 获取响应⽂本。\n可以看出，这个依赖关系的主要⽬的是获取 Hacker News 的⻚⾯⽂本。因此，我可以创建⼀个名为 HNWebPage 的抽象，让它承担“提供⻚⾯⽂本”的职责。\n下⾯的 HNWebPage 抽象类就是实现 DIP 的关键：\n1 2 3 4 5 6 7 8 from abc import ABC,abstractmethod class HNWebPage(ABC): \u0026#34;\u0026#34;\u0026#34;抽象类:Hacker News站点页面\u0026#34;\u0026#34;\u0026#34; @abstractmethod def get_text(self) -\u0026gt; str: raise NotImplementedError() 定义好抽象后，接下来分别让⾼层模块和低层模块与抽象产⽣依赖关系。我们从低层模块开始。\n低层模块与抽象间的依赖关系表现为它会提供抽象的具体实现。在下⾯的代码⾥，我实现了RemoteHNWebPage 类，它的作⽤是通过 requests 模块请求 Hacker News ⻚⾯，返回⻚⾯内容\n1 2 3 4 5 6 7 8 9 class RemoteHNWebPage(HNWebPage): \u0026#34;\u0026#34;\u0026#34;远程⻚⾯，通过请求 Hacker News 站点返回内容\u0026#34;\u0026#34;\u0026#34; def __init__(self,url:str): self.url = url def get_text(self) -\u0026gt; str: resp = requests.get(self.url) return resp.text 此时的依赖关系表现为类与类的继承。除继承外，与抽象类的依赖关系还有许多其他表现形式，⽐如使⽤抽象类的 .register() ⽅法，或者定义⼦类化钩⼦⽅法\n处理完低层模块的依赖关系后，接下来我们需要调整⾼层模块 SiteSourceGrouper 类的代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class SiteSourceGrouper: \u0026#34;\u0026#34;\u0026#34;对 Hacker News ⻚⾯的新闻来源站点进⾏分组统计\u0026#34;\u0026#34;\u0026#34; def __init__(self, page: HNWebPage): ❶ self.page = page def get_groups(self) -\u0026gt; Dict[str, int]: \u0026#34;\u0026#34;\u0026#34;获取 (域名, 个数) 分组\u0026#34;\u0026#34;\u0026#34; html = etree.HTML(self.page.get_text()) ❷ ... def main(): page = RemoteHNWebPage(url=\u0026#34;https://news.ycombinator.com/\u0026#34;) ❸ grouper = SiteSourceGrouper(page).get_groups() ❶ 在初始化⽅法⾥，我⽤类型注解表明了所依赖的是抽象的 HNWebPage 类型 ❷ 调⽤ HNWebPage 类型的 get_text() ⽅法，获取⻚⾯⽂本内容 ❸ 实例化⼀个符合抽象 HNWebPage 的具体实现：RemoteHNWebPage 对象\nSiteSourceGrouper 和 RemoteHNWebPage 都依赖抽象 HNWebPage\n可以看到，图 11-3 ⾥的⾼层模块不再直接依赖低层模块，⽽是依赖处于中间的抽象：HNWebPage。低层模块也不再是被依赖的⼀⽅，⽽是反过来依赖处于上⽅的抽象层，这便是 DIP ⾥inversion（倒置）⼀词的由来。\n倒置后的单元测试 通过创建抽象实现 DIP 后，我们回到之前的单元测试问题。为了满⾜单元测试的⽆⽹络需求，基于 HNWebPage 抽象类，我可以实现⼀个不依赖⽹络的新类型 LocalHNWebPage：\n1 2 3 4 5 6 7 8 9 10 11 12 class LocalHNWebPage(HNWebPage): \u0026#34;\u0026#34;\u0026#34;本地⻚⾯，根据本地⽂件返回⻚⾯内容 :param path: 本地⽂件路径 \u0026#34;\u0026#34;\u0026#34; def __init__(self,path:str): self.path = path def get_text(self) -\u0026gt; str: with open(self.path,\u0026#39;r\u0026#39;) as fp: return fp.read() 单元测试代码也可以进⾏相应的调整：\n1 2 3 4 5 def test_grouper_from_local(): page = LocalHNWebPage(path=\u0026#34;./static_hn.html\u0026#34;) grouper = SiteSourceGrouper(page) result = grouper.get_groups() assert isinstance(result, Counter), \u0026#34;groups should be Counter instance\u0026#34; 有了额外的抽象后，我们解耦了 SiteSourceGrouper ⾥的外⽹访问⾏为。现在的测试代码不需要任何 mock 技术，在⽆法访问外⽹的 CI 服务器上也能正常执⾏。\nDIP 要求代码在互相依赖的模块间创建新的抽象概念。当⾼层模块依赖抽象⽽不是具体实现后，我们就能更⽅便地⽤其他实现替换底层模块，提⾼代码灵活性。\n退后一步是鸭子，向前一步是协议 如果在抽象类⽅案下，往后退⼀步，从代码⾥删掉抽象类，同时删掉所有的类型注解，你会发现代码仍然可以正常执⾏。在这种情况下，依赖关系仍然是倒过来的，但是处在中间的“抽象”变成了⼀个隐式概念。\n没有抽象类后，代码变成了“鸭⼦类型”，依赖倒置也变成了⼀种符合“鸭⼦类型”的倒置。\n在 Python 3.8 版本⾥，类型注解 typing 模块增加了⼀个名为“协议”（Protocol）的类型。从各种意义上来说，Protocol 都⽐抽象类更接近传统的“接⼝”。\n下⾯是⽤ Protocol 实现的 HNWebPage：\n1 2 3 4 5 class HNWebPage(Protocol): \u0026#34;\u0026#34;\u0026#34;协议：Hacker News站点页面\u0026#34;\u0026#34;\u0026#34; def get_text(self)-\u0026gt;str: ... 虽然 Protocol 提供了定义协议的能⼒，但像类型注解⼀样，它并不提供运⾏时的协议检查，它的真正实⼒仍然需要搭配 mypy 才能发挥出来。\n通过 Protocol 与 mypy 类型检查⼯具，你能实现真正的基于协议的抽象与结构化⼦类技术。也就是说，只要某个类实现了 get_text() ⽅法，并且返回了 str 类型，那么它便可以当作 HNWebPage 使⽤。\n不过，Protocol 与 mypy 的上⼿⻔槛较⾼，如果不是⼤型项⽬，实在没必要使⽤。在多数情况下，普通的抽象类或鸭⼦类型已经够⽤了。\n事实是，抽象的好处显⽽易⻅：它解耦了模块间的依赖关系，让代码变得更灵活。但抽象同时也带来了额外的编码与理解成本。\n所以，了解何时不抽象与何时抽象同样重要。只有对代码中那些容易变化的东西进⾏抽象，才能获得最⼤的收益。\nISP 接口是编程语言里的一类特殊对象，它包含一些公开的抽象协议，可以用来构建模块间的依赖关系，在不同的编程语言里，接口有不同的表现形式，在python中，接口可以是抽象类，Protocol，也可以是鸭子类型里的某个隐式概念\n接口是⼀种⾮常有⽤的设计⼯具，为了更好地发挥它的能⼒，ISP 对如何使⽤接⼝提出了要求：客户不应该依赖任何它不使用的方法\n拿上⼀节统计 Hacker News ⻚⾯条⽬的例⼦来说：\n使⽤⽅（客⼾模块）——SiteSourceGrouper； 接⼝（其实是抽象类）——HNWebPage 依赖关系——调⽤接⼝⽅法 get_text() 获取⻚⾯⽂本 我设计的接⼝ HNWebPage 就是符合 ISP 的，因为它没有提供任何使⽤⽅不需要的⽅法\n对 HNWebPage 接⼝的盲⽬扩展暴露出⼀个问题：更丰富的接⼝协议，意味着更⾼的实现成本，也更容易给实现⽅带来⿇烦。\n分拆接口 在设计接⼝时有⼀个简单的技巧：让客⼾（调⽤⽅）来驱动协议设计。在现在的程序⾥，HNWebPage 接⼝共有两个客⼾\nSiteSourceGrouper：按域名来源统计，依赖 get_text()。 SiteAchiever：⻚⾯归档程序，依赖 get_text()、get_size() 和 get_generated_at()。 根据这两个客⼾的需求，我可以把 HNWebPage 分离成两个不同的抽象类： 当你认识到 ISP 带来的种种好处后，很⾃然地会养成写⼩类、⼩接⼝的习惯。在现实世界⾥，其实已经有很多⼩⽽精的接⼝设计可供参考，⽐如：\nPython 的 collections.abc 模块⾥⾯有⾮常多的⼩接⼝； Go 语⾔标准库⾥的 Reader 和 Writer 接⼝。 总结 LSP LSP认为子类应该可以任意替代父类使用 子类不应该抛出父类不认识的异常 子类方法应该返回与父类一致的类型，或者返回父类返回值的子类型对象 子类的方法参数应该合父类方法完全一致，或者要求更为宽松 某些类可能会存在隐式合约，违反这些合约也会导致违反LSP DIP DIP认为高层模块合底层模块都应该依赖于抽象 编写单元测试有一个原则：测试行为，而不是测试实现 单元测试不宜使⽤太多 mock，否则需要调整设计 依赖抽象的好处是，修改低层模块实现不会影响高层代码 在python中，你可以用abc模块来定义抽象类 除abc以外，你也可以用Protocol等技术来完成依赖倒置 3.ISP\nISP认为客户依赖的接口不应该包含任何它不需要的方法 设计接口就是设计抽象 写更小的类，更小的接口在大多数情况是个好主意 数据模型与描述符 我们常说的数据模型（或者叫对象模型）就是这套规则。假如把 Python 语⾔看作⼀个框架，数据模型就是这个框架的说明书。数据模型描述了框架如何⼯作，创建怎样的对象才能更好地融⼊Python 这个框架\n除了 print() 以外，str() 与 .format() 函数同样也会触发 str ⽅法\n上⾯展⽰的 str 就是 Python 数据模型⾥最基础的⼀部分。当对象需要当作字符串使⽤时，我们可以⽤ str ⽅法来定义对象的字符串化结果。\n使用@total_ordering @total_ordering 是 functools 内置模块下的⼀个装饰器。它的功能是让重载⽐较运算符变得更简单。\n如果使⽤ @total_ordering 装饰⼀个类，那么在重载类的⽐较运算符时，你只要先实现__eq__ ⽅法，然后在 lt、le、gt、ge 四个⽅法⾥随意挑⼀个实现即可，@total_ordering 会帮你⾃动补全剩下的所有⽅法。\n使⽤ @total_ordering，前⾯的 Square 类可以简化成下⾯这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from functools import total_ordering @total_ordering class Square: \u0026#34;\u0026#34;\u0026#34;正方形 :param length:边长 \u0026#34;\u0026#34;\u0026#34; def __init__(slef,length): self.length = length def area(self): return self.length ** 2 def __eq__(self,other): if isinstance(other,self.__class__): return self.length == other.length return False def __lt__(self,other): if isinstance(other,self.__class__): return self.length \u0026lt; other.length return NotImplemented 描述符 这是因为所有的⽅法、类⽅法、静态⽅法以及属性等诸多 Python 内置对象，都是基于描述符协议实现的。\n在⽇常⼯作中，描述符的使⽤并不算频繁。但假如你要开发⼀些框架类⼯具，就会发现描述符⾮常有⽤。接下来我们通过开发⼀个⼩功能，来看看描述符究竟能如何帮助我们。\n使⽤ @property 把 age 定义为 property 对象后，我可以很⽅便地增加校验逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Person: ... @property def age(self): return self._age @age.setter def age(self, value): \u0026#34;\u0026#34;\u0026#34;设置年龄，只允许 0〜150 之间的数值\u0026#34;\u0026#34;\u0026#34; try: value = int(value) except (TypeError, ValueError): raise ValueError(\u0026#39;value is not a valid integer!\u0026#39;) if not (0 \u0026lt; value \u0026lt; 150): raise ValueError(\u0026#39;value must between 0 and 150!\u0026#39;) self._age = value 描述符简介 描述符是python对象模型里的一种特殊协议，它主要和 4 个魔法⽅法有关： get、set、delete 和 set_name。\n从定义上来说，除了最后一个方法 set_name 以外，任何一个实现了 get,set 或 delete 的类，都可以称为描述符类，它的实例则叫作描述符对象\n描述符之所以叫这个名字，是因为它描述了python获取与设置一个类(实例)成员的整个过程。我们通过简单的代码⽰例，来看看描述符的⼏个魔法⽅法究竟有什么⽤。\n从最常⽤的 get ⽅法开始：\n1 2 3 4 5 6 7 8 9 class InfoDescriptor: \u0026#34;\u0026#34;\u0026#34;打印帮助信息的描述符\u0026#34;\u0026#34;\u0026#34; def __get__(self,instance,owner=None): print(f\u0026#39;Calling __get__, instance: {instance}, owner: {owner}\u0026#39;) if not instance: print(\u0026#39;Calling without instance...\u0026#39;) return self return \u0026#39;informative descriptor\u0026#39; 上⾯的 InfoDescriptor 是⼀个实现了 get ⽅法的描述符类。\n要使⽤⼀个描述符，最常⻅的⽅式是把它的实例对象设置为其他类（常被称为 owner 类）的属性：\n1 2 class Foo: bar = InfoDescriptor() 描述符的 get ⽅法，会在访问 owner 类或 owner 类实例的对应属性时被触发。get ⽅法⾥的两个参数的含义如下。\nowner:描述符对象所绑定的类 instance:假如用实例来访问描述符属性，该参数值为实例对象，如果通过类来访问该值为None 与 get ⽅法相对应的是 set ⽅法，它可以⽤来⾃定义设置某个实例属性时的⾏为\ninstance:属性当前绑定的实例对象 value:待设置的属性值 值得⼀提的是，描述符的 set 仅对实例起作⽤，对类不起作⽤。这和 get ⽅法不⼀样，get 会同时影响描述符所绑定的类和类实例。当你通过类设置描述符属性值时，不会触发任何特殊逻辑，整个描述符对象会被覆盖：\n用描述符实现属性校验功能 为了提供更⾼的可复⽤性，这次我在年龄字段的基础上抽象出了⼀个⽀持校验功能的整型描述符类型：IntegerField。它的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class IntegerField: \u0026#34;\u0026#34;\u0026#34;整型字段，只允许一定范围内的整型值 :param min_value:允许的最小值 :param max_value:允许的最大值 \u0026#34;\u0026#34;\u0026#34; def __init__(self,min_value,max_value): self.min_value = min_value self.max_value = max_value def __get__(self,instance,owner=None): # 当不是通过实例访问时，直接返回描述符对象 if not instance: return self # 返回保存在实例字典里的值 return instance.__dict__[\u0026#39;_integer_field\u0026#39;] def __set__(self,instance,value): # 校验后将值保存在实例字典里 value = self._validate_value(value) instance.__dict__[\u0026#39;_integer_field\u0026#39;] = value def _validate_value(self,value): \u0026#34;\u0026#34;\u0026#34;校验值是否为符合要求的整数\u0026#34;\u0026#34;\u0026#34; try: value = int(value) except (TypeError, ValueError): raise ValueError(\u0026#39;value is not a valid integer!\u0026#39;) if not (self.min_value \u0026lt;= value \u0026lt;= self.max_value): raise ValueError( f\u0026#39;value must between {self.min_value} and {self.max_value}!\u0026#39; ) return value IntegerField 最核⼼的逻辑，就是在设置属性值时先做有效性校验，然后再保存数据。\n在 set ⽅法⾥，我使⽤了 instance.dict[\u0026rsquo;_integer_field\u0026rsquo;] = value 这样的语句来保存整型数字的值。也许你想问：为什么不直接写 self._integer_field = value，把值存放在描述符对象 self ⾥呢？\n这是因为每个描述符对象都是 owner 类的属性，⽽不是类实例的属性。也就是说，所有从owner 类派⽣出的实例，其实都共享了同⼀个描述符对象。假如把值存⼊描述符对象⾥，不同实例间的值就会发⽣冲突，互相覆盖。\n所以，为了避免覆盖问题，我把值放在了每个实例各⾃的 dict 字典⾥\n使用 set_name set_name(self, owner, name)是 Python 在 3.6 版本以后，为描述符协议增加的新⽅法，它所接收的两个参数的含义如下。\nowner：描述符对象当前绑定的类。 name：描述符所绑定的属性名称。 set_name ⽅法的触发时机是在 owner 类被创建时\n通过给 IntegerField 类增加 set_name ⽅法，我们可以⽅便地解决前⾯的数据冲突问题：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class IntegerField: def __init__(self,min_value,max_value): self.min_value = min_value self.max_value = max_value def __set_name__(self,owner,name): # 将绑定属性名保存在描述符对象中 # 对于 age = IntegerField(...) 来说，此处的 name 就是“age” self._name = name def __get__(self,instance,owner=None): if not instance: return self # 在数据存取时，使用动态的self._name return instance.__dict__[self.name] def __set__(self,instance,value): value = self._validate_value(value) instance.__dict__[self._name] = value def _validate_value(self, value): \u0026#34;\u0026#34;\u0026#34;校验值是否为符合要求的整数\u0026#34;\u0026#34;\u0026#34; # ... 使⽤描述符，我们最终实现了⼀个可复⽤的 IntegerField 类，它使⽤起来⾮常⽅便——⽆须继承任何⽗类、声明任何元类，直接将类属性定义为描述符对象即可。\n数据描述符与⾮数据描述符 按实现⽅法的不同，描述符可分为两⼤类。\n⾮数据描述符：只实现了 get ⽅法的描述符 数据描述符：实现了 set 或 delete 其中任何⼀个⽅法的描述符。 这两类描述符的区别主要体现在所绑定实例的属性存取优先级上。 对于⾮数据描述符来说，你可以直接⽤ instance.attr = \u0026hellip; 来在实例级别重写描述符属性 attr，让其读取逻辑不再受描述符的 get ⽅法管控。 ⽽对于数据描述符来说，你⽆法做到同样的事情。数据描述符所定义的属性存储逻辑拥有极⾼的优先级，⽆法轻易在实例层⾯被重写\n所有的 Python 实例⽅法、类⽅法、静态⽅法，都是⾮数据描述符，你可以轻易覆盖它们。⽽ property() 是数据描述符，你⽆法直接通过重写修改它的⾏为\n利用集合的游戏规则 要用集合来解决我们的问题，第一步是建模一个用来表示旅客记录的新类型\n1 2 3 4 5 6 7 8 9 10 11 12 class VisitRecord: \u0026#34;\u0026#34;\u0026#34;旅客记录 :param first_name: 名 :param last_name: 姓 :param phone_number: 电话号码 :param date_visited: 旅游时间 \u0026#34;\u0026#34;\u0026#34; def __init__(self, first_name, last_name, phone_number, date_visited): self.first_name = first_name self.last_name = last_name self.phone_number = phone_number self.date_visited = date_visited 默认情况下，Python 的⽤⼾⾃定义类型都是可哈希的。因此，VisitRecord 对象可以直接放进集合⾥，但⾏为可能会和你预想中的有些不同：\n出现上⾯这样的结果其实并不奇怪。因为对于任何⾃定义类型来说，当你对两个对象进⾏相等⽐较时，Python 只会判断它们是不是指向内存⾥的同⼀个地址。换句话说，任何对象都只和它⾃⾝相等\n因此，为了让集合能正确处理 VisitRecord 类型，我们⾸先要重写类型的 eq 魔法⽅法，让Python 在对⽐两个 VisitRecord 对象时，不再关注对象 ID，只关⼼记录的姓名与电话号码。\n1 2 3 4 5 6 7 8 9 def __eq__(self,other): if isinstance(other,self.__class__): return self.comparable_fields == other.comparable_fileds return False @property def comparable_fields(self): \u0026#34;\u0026#34;\u0026#34;获取用于对比对象的字段值\u0026#34;\u0026#34;\u0026#34; return (self.first_name,self.last_name,self.phone_number) 完成这⼀步后，VisitRecord 的相等运算就重写成了我们所需要的逻辑\n但要达到计算差集的⽬的，仅重写 eq 是不够的。如果我现在试着把⼀个新的 VisitRecord对象塞进集合，程序⻢上会报错：\n发⽣什么事了？VisitRecord 类型突然从可哈希变成了不可哈希！要弄清楚原因，得先从哈希表的⼯作原理讲起\n当 Python 把⼀个对象放⼊哈希表数据结构（如集合、字典）中时，它会先使⽤ hash() 函数计算出对象的哈希值，然后利⽤该值在表⾥找到对象应在的位置，之后完成保存。⽽当 Python 需要获知哈希表⾥是否包含某个对象时，同样也会先计算出对象的哈希值，之后直接定位到哈希表⾥的对应位置，再和表⾥的内容进⾏精确⽐较\n也就是说，⽆论是往集合⾥存⼊对象，还是判断某对象是否在集合⾥，对象的哈希值都会作为⼀个重要的前置索引被使⽤。\n在我重写 eq 前，对象的哈希值其实是对象的 ID（值经过⼀些转换，和 id() 调⽤结果并⾮完全⼀样）。但当 eq ⽅法被重写后，假如程序仍然使⽤对象 ID 作为哈希值，那么⼀个严重的悖论就会出现：即便两个不同的 VisitRecord对象在逻辑上相等，但它们的哈希值不⼀样，这在原理上和哈希表结构相冲突。\n因为对于哈希表来说，两个相等的对象，其哈希值也必须⼀样，否则⼀切算法逻辑都不再成⽴。所以，Python 才会在发现重写了 eq ⽅法的类型后，直接将其变为不可哈希，以此强制要求你为其设计新的哈希值算法。\n幸运的是，只要简单地重写 VisitRecord 的 hash ⽅法，我们就能解决这个问题：\n1 2 def __hash__(self): return hash(self.comparabble_fields) 因为 .comparable_fields 属性返回了由姓名、电话号码构成的元组，⽽元组本⾝就是可哈希类型，所以我可以直接把元组的哈希值当作 VisitRecord 的哈希值使⽤。\n完成 VisitRecord 建模，做完所有的准备⼯作后，剩下的事情便顺⽔推⾈了。基于集合差值运算的新版函数，只要⼀⾏核⼼代码就能完成操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class VisitRecord: \u0026#34;\u0026#34;\u0026#34;旅客记录 - 当两条旅客记录的姓名与电话号码相同时，判定⼆者相等。 \u0026#34;\u0026#34;\u0026#34; def __init__(self, first_name, last_name, phone_number, date_visited): self.first_name = first_name self.last_name = last_name self.phone_number = phone_number self.date_visited = date_visited def __hash__(self): return hash(self.comparable_fields) def __eq__(self, other): if isinstance(other, self.__class__): return self.comparable_fields == other.comparable_fields return False @property def comparable_fields(self): \u0026#34;\u0026#34;\u0026#34;获取⽤于⽐较对象的字段值\u0026#34;\u0026#34;\u0026#34; return (self.first_name, self.last_name, self.phone_number) def find_potential_customers_v3(): # 转换为 VisitRecord 对象后计算集合差值 return set(VisitRecord(**r) for r in users_visited_puket) - set( VisitRecord(**r) for r in users_visited_nz ) 因此，当 Python 通过哈希值在表⾥搜索时，并不会完全依赖哈希值，⽽⼀定会再做⼀次精准的相等⽐较运算 ==（使⽤ eq），这样才能最终保证程序的正确性。\n基本没有⼈会在实际⼯作中写出上⾯这种代码来解决这么⼀个简单问题。但是，有了下⾯这个模块的帮助，事情也许会有⼀些变化\n使⽤ dataclasses dataclasses 是 Python 在 3.7 版本后新增的⼀个内置模块。它最主要的⽤途是利⽤类型注解语法来快速定义像上⾯的 VisitRecord ⼀样的数据类。\n使⽤ dataclasses 可以极⼤地简化 VisitRecord 类，代码最终会变成下⾯这样：\n1 2 3 4 5 6 7 8 9 10 11 from dataclasses import dataclass,field @dataclass(frozen=True) class VisitRecordDC: first_name: str last_name: str phone_name: str date_visited: str = field(compare=True) def find_potential_customers_v4(): return set(VisitRecordDC(**r) for r in users_visited_puket) - set(VisitRecordDC(**r) for r in users_visited_nz) 要定义⼀个 dataclass 字段，只需提供字段名和类型注解即可 因为旅游时间 date_visited 不⽤于⽐较运算，所以需要指定 compare=False 跳过该字段\n通过 @dataclass 来定义⼀个数据类，我完全不⽤再⼿动实现 init ⽅法，也不⽤重写任何__eq__ 与 hash ⽅法，所有的逻辑都会由 @dataclass ⾃动完成。\n在上⾯的代码⾥，尤其需要说明的是 @dataclass(frozen=True) 语句⾥的 frozen 参数。在默认情况下，由 @dataclass 创建的数据类都是可修改的，不⽀持任何哈希操作。因此你必须指定frozen=True，显式地将当前类变为不可变类型，这样才能正常计算对象的哈希值。\n最后，在集合运算和数据类的帮助下，不⽤⼲任何脏活累活，总共不到⼗⾏代码就能完成所有的⼯作。\n认识 hash 的危险性 所以，设计哈希算法的原则是：在⼀个对象的⽣命周期⾥，它的哈希值必须保持不变，否则就会出现各种奇怪的事情。这也是 Python 把所有可变类型（列表、字典）设置为“不可哈希”的原因。\n每当你想要重写 hash ⽅法时，⼀定要保证⽅法产⽣的哈希值是稳定的，不会随着对象状态⽽改变。要做到这点，要么你的对象不可变，不允许任何修改——就像定义 dataclass 时指定的frozen=True；要么⾄少应该保证，被卷⼊哈希值计算的条件不会改变。\n数据模型不是“躺赢”之道 不要把数据模型当成写代码时的万能药，把所有脚都塞进数据模型这双靴⼦⾥\n恰当地使⽤数据模型，确实能让我们写出更符合 Python 习惯的代码，设计出更地道的 API。但也得注意不要过度，有时，“聪明”的代码反⽽不如“笨”代码，平铺直叙的“笨”代码或许更能表达出设计者的意图，更容易让⼈理解。\n不要依赖 del 方法 我经常⻅到⼈们把 del 当成⼀种⾃动化的资源回收⽅法来⽤。⽐如，⼀个请求其他服务的 Client 对象会在初始化时创建⼀个连接池。那么写代码的⼈极有可能会重写对象的 del⽅法，把关闭连接池的逻辑放在⽅法⾥。\n现在你应该明⽩了，⼀个对象的 del ⽅法，并⾮在使⽤ del 语句时被触发，⽽是在它被作为垃圾回收时触发。del 语句⽆法直接回收任何东西，它只是简单地删掉了指向当前对象的⼀个引⽤（变量名）⽽已。\n换句话说，del 让对象的引⽤计数减 1，但只有当引⽤计数降为 0 时，它才会⻢上被Python 解释器回收。因此，在 foo 仍然被列表 l 引⽤时，删除 foo 的其中⼀个引⽤是不会触发 del 的\n总⽽⾔之，垃圾回收机制是⼀⻔编程语⾔的实现细节\n正因为如此，依赖 del ⽅法来做⼀些清理资源、释放锁、关闭连接池之类的关键⼯作，其实⾮常危险。因为你创建的任何对象，完全有可能因为某些原因⼀直都不被作为垃圾回收。这时，⽹络连接会不断增⻓，锁也⼀直⽆法被释放，最后整个程序会在某⼀刻轰然崩塌。\n如果你要给对象定义资源清理逻辑，请避免使⽤ del。你可以要求使⽤⽅显式调⽤清理⽅法，或者实现⼀个上下⽂管理器协议——⽤ with 语句来⾃动清理（参考 Python 的⽂件对象），这些⽅式全都⽐ del 好得多。\n总结 字符串相关协议 使用 str 方法，可以定义对象的字符串值(被str()触发) 使用 repr 方法，可以定义对象对调试友好的详细字符串值（被 repr() 触发） 如果对象只定义了 repr ⽅法，它同时会⽤于替代 str 使⽤ format ⽅法，可以在对象被⽤于字符串模板渲染时，提供多种字符串值（被.format() 触发） 比较运算符重载 通过重载与⽐较运算符有关的 6 个魔法⽅法，你可以让对象⽀持 ==、\u0026gt;= 等⽐较运算 使⽤ functools.total_ordering 可以极⼤地减少重载⽐较运算符的⼯作量 描述符协议 使用描述符协议，你可以轻松实现可复用的属性对象 实现了 get、set 、delete 其中任何⼀个⽅法的类都是描述符类 要在描述符⾥保存实例级别的数据，你需要将其存放在 instance.dict ⾥，⽽不是直接放在描述符对象上 使⽤ set_name ⽅法能让描述符对象知道⾃⼰被绑定了什么名字 数据类与⾃定义哈希运算 要让⾃定义类⽀持集合运算，你需要实现 eq 与 hash 两个⽅法 如果两个对象相等，它们的哈希值也必须相等，否则会破坏哈希表的正确性 不同对象的哈希值可以⼀样，哈希冲突并不会破坏程序正确性，但会影响效率 使⽤ dataclasses 模块，你可以快速创建⼀个⽀持哈希操作的数据类 要让数据类⽀持哈希操作，你必须指定 frozen=True 参数将其声明为不可变类型 ⼀个对象的哈希值必须在它的⽣命周期⾥保持不变 其他建议 虽然数据模型能帮我们写出更 Pythonic 的代码，但切勿过度推崇 del ⽅法不是在执⾏ del 语句时被触发，⽽是在对象被作为垃圾回收时被触发 不要使⽤ del 来做任何“⾃动化”的资源回收⼯作 开发大型项目 常用工具介绍 flake8 isort black 作为⼀个代码格式化⼯具，black 最⼤的特点在于它的不可配置性。正如官⽅介绍所⾔，black 是⼀个“毫不妥协的代码格式化⼯具”\n不过，虽然我们没法统⼀每个⼈的 IDE，但⾄少⼤部分项⽬使⽤的版本控制软件是⼀样的—— Git。⽽ Git 有个特殊的钩⼦功能，它允许你给每个仓库配置⼀些钩⼦程序（hook），之后每当你进⾏特定的 Git 操作时——⽐如 git commit、git push，这些钩⼦程序就会执⾏。\npre-commit 就是⼀个基于钩⼦功能开发的⼯具。从名字就能看出来，pre-commit 是⼀个专⻔⽤于预提交阶段的⼯具。要使⽤它，你需要先创建⼀个配置⽂件 .pre-commitconfig.yaml。\n举个例⼦，下⾯是⼀个我常⽤的 pre-commit 配置⽂件内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fail_fast: true repos: - repo: https://github.com/timothycrosley/isort rev: 5.7.0 hooks: - id: isort additional_dependencies: [toml] - repo: https://github.com/psf/black rev: 20.8b1 hooks: - id: black args: [--config=./pyproject.toml] - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.4.0 hooks: - id: flake8 由于 pre-commit 的配置⽂件与项⽬源码存放在⼀起，都在代码仓库中，因此项⽬的所有开发者天然共享 pre-commit 的插件配置，每个⼈不⽤单独维护各⾃的配置，只要安装 pre-commit⼯具就⾏。\nmypy 单元测试简介 但如今，事情发⽣了很多变化。由于敏捷开发与快速迭代理论的流⾏，⼈们现在开始想尽办法压缩发布周期、提升发布频率，态度近乎狂热。不少百万⾏代码量级的互联⽹项⽬，每天要构建数⼗个版本，每周发布数次。由于构建和发布⼏乎⽆时⽆刻都在进⾏，⼤家给这类实践起了⼀个贴切的名字：持续集成（CI）与持续交付（CD）\n根据关注点的不同，⾃动化测试可分为不同的类型，⽐如 UI 测试、集成测试、单元测试等。不同类型的测试，各⾃关注着不同的领域，覆盖了不⼀样的场景。⽐如，UI 测试是模拟⼀位真实⽤⼾真正使⽤软件，以此验证软件的⾏为是否与预期⼀致。⽽单元测试通过单独执⾏项⽬代码⾥的每个功能单元，来验证它们的⾏为是否正常。\n在所有测试中，单元测试数量最多、测试成本最低，是整个⾃动化测试的基础和重中之重\nunittest 在 Python ⾥编写单元测试，最正统的⽅式是使⽤ unittest 模块。unittest 是标准库⾥的单元测试模块，使⽤⽅便，⽆须额外安装。\n我们先通过⼀个简单的测试⽂件来感受⼀下 unittest 的功能： 文件:test_upper.py\n1 2 3 4 5 6 7 8 import unittest class TestStringUpper(unittest.TestCase): def test_normal(self): self.assertEqual(\u0026#39;foo\u0026#39;.upper(),\u0026#39;FOO\u0026#39;) if __name__ = \u0026#39;__main__\u0026#39;: unittest.main() ⽤ unittest 编写测试⽤例的第⼀步，是创建⼀个继承 unittest.TestCase 的⼦类，然后编写许多以 test 开头的测试⽅法。在⽅法内部，通过调⽤⼀些以 assert 开头的⽅法来进⾏测试断⾔，如下所⽰\nself.assertEqual(x, y)：断⾔ x 和 y 必须相等。 self.assertTrue(x)：断⾔ x 必须为布尔真。 self.assertGreaterEqual(x, y)：断⾔ x 必须⼤于等于 y。 如果⼀个测试⽅法内的所有测试断⾔都能通过，那么这个测试⽅法就会被标记为成功；⽽如果有任何⼀个断⾔⽆法通过，就会被标记为失败。\n使⽤ python test_upper.py 来执⾏测试⽂件，会打印出测试⽤例的执⾏结果： 除了定义测试⽅法外，你还可以在 TestCase 类⾥定义⼀些特殊⽅法。⽐如，通过定义setUp() 和 tearDown() ⽅法，你可以让程序在执⾏每个测试⽅法的前后，运⾏额外的代码逻辑\n要搞清楚为什么 unittest 会采⽤这些奇怪设计，得从模块的历史出发。Python 的unittest 模块在最初实现时，⼤量参考了 Java 语⾔的单元测试框架 JUnit。因此，它的许多“奇怪”设计其实是“Java 化”的表现，⽐如只能⽤类来定义测试⽤例，⼜⽐如⽅法都采⽤驼峰命名法等。\n但在⽇常⼯作中，我其实更偏爱另⼀个在 API 设计上更接近 Python 语⾔习惯的单元测试框架：pytest。接下来我们看看如何⽤ pytest 做单元测试。\npytest pytest 功能更多，设计更复杂，上⼿难度也更⾼。但 pytest 的最⼤优势在于，它把Python 的⼀些惯⽤写法与单元测试很好地融合了起来。因此，当你掌握了 pytest 以后，⽤它写出的测试代码远⽐⽤ unittest 写的简洁\n为了测试函数的功能，我⽤ pytest 写了⼀份单元测试： ⽂件：test_string_utils.py\n1 2 3 4 from string_utils import string string_upper def test_string_upper(): assert string_upper(\u0026#39;foo\u0026#39;) = \u0026#39;FOO\u0026#39; ⾸先，TestCase 类消失了。使⽤ pytest 时，你不必⽤⼀个 TestCase 类来定义测试⽤例，⽤⼀个以 test 开头的普通函数也⾏。\n其次，当你要进⾏断⾔判断时，不需要调⽤任何特殊的 assert{X}() ⽅法，只要写⼀条原⽣的断⾔语句 assert {expression} 就好。\n正因为这些简化，⽤ pytest 来编写测试⽤例变得⾮常容易。\n为了让单元测试覆盖更多场景，最直接的办法是在 test_string_utils.py ⾥增加测试函数。\n1 2 3 4 5 6 7 8 9 10 from string_utils import string_upper def test_string_upper(): assert string_upper(\u0026#39;foo\u0026#39;) == \u0026#39;FOO\u0026#39; def test_string_empty(): ❶ assert string_upper(\u0026#39;\u0026#39;) == \u0026#39;\u0026#39; def test_string_mixed_cases(): assert string_upper(\u0026#39;foo BAR\u0026#39;) == \u0026#39;FOO BAR\u0026#39; ⽤ parametrize 编写参数化测试 在单元测试领域，有⼀种常⽤的编写测试代码的技术：表驱动测试（table-driven testing）。\n当你要测试某个函数在接收到不同输⼊参数的⾏为时，最直接的做法是像上⾯那样，直接编写许多不同的测试⽤例。但这种做法其实并不好，因为它很容易催⽣出重复的测试代码。\n表驱动测试是⼀种简化单元测试代码的技术。它⿎励你将不同测试⽤例间的差异点抽象出来，提炼成⼀张包含多份输⼊参数、期望结果的数据表，以此驱动测试执⾏。如果你要增加测试⽤例，直接往表⾥增加⼀份数据就⾏，不⽤写任何重复的测试代码。\n在 pytest 中实践表驱动测试⾮常容易。pytest 为我们提供了⼀个强⼤的参数测试⼯具：pytest.mark.parametrize。利⽤该装饰器，你可以⽅便地定义表驱动测试⽤例\n以测试⽂件 test_string_utils.py 为例，使⽤参数化⼯具，我可以把测试代码改造成代 代码清单 13-2 使⽤ parametrize 后的测试代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import pytest from string_utils import string_upper @pytest.mark.parametrize( \u0026#39;s,expected\u0026#39;, [ (\u0026#39;foo\u0026#39;,\u0026#39;FOO\u0026#39;), (\u0026#39;\u0026#39;,\u0026#39;\u0026#39;), (\u0026#39;foo BAR\u0026#39;,\u0026#39;FOO BAR\u0026#39;) ], ) def test_string_upper(s,expected): assert string_upper(s) == expected ⽤逗号分隔的参数名列表，也可以理解为数据表每⼀列字段的名称\n数据表的每⾏数据通过元组定义，元组成员与参数名⼀⼀对应\n在测试函数的参数部分，按 parametrize 定义的字段名，增加对应参数\n在测试函数内部，⽤参数替换静态测试数据\n在本节中，我演⽰了如何使⽤ @pytest.mark.parametrize 定义参数化测试，避免编写重复的测试代码。下⾯，我会介绍 pytest 的另⼀个重要功能：fixture（测试固定件）。\n使用@pytest.fixture 创建 fixture 对象 在编写单元测试时，我们常常需要重复⽤到⼀些东西。⽐如，当你测试⼀个图⽚操作模块时，可能需要在每个测试⽤例开始时，重复创建⼀张临时图⽚⽤于测试。\n这类被许多单元测试依赖、需要重复使⽤的对象，常被称为 fixture。在 pytest 框架下，你可以⾮常⽅便地⽤ @pytest.fixture 装饰器创建 fixture 对象。\n举个例⼦，在为某模块编写测试代码时，我需要不断⽤到⼀个⻓度为 32 的随机 token 字符串。为了简化测试代码，我可以创造⼀个名为 random_token 的 fixture，如代码清单13-3 所⽰。\n1 2 3 4 5 6 7 8 9 10 11 12 import pytest import string import random @pytest.fixture def random_token() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;生成随机token\u0026#34;\u0026#34;\u0026#34; token_l = [] char_pool = string.ascii_lowercase + string.digits for _ in range(32): token_l.append(random.choice(char_pool)) return \u0026#39;\u0026#39;.join(token_l) 定义完 fixture 后，假如任何⼀个测试⽤例需要⽤到随机 token，不⽤执⾏ import，也不⽤⼿动调⽤ random_token() 函数，只要简单调整测试函数的参数列表，增加random_token 参数即可：\n1 2 def test_foo(random_token): print(random_token) 之后每次执⾏ test_foo() 时，pytest 都会⾃动找到名为 random_token 的 fixutre对象，然后将 fixture 函数的执⾏结果注⼊测试⽅法中。\n假如你在 fixture 函数中使⽤ yield 关键字，把它变成⼀个⽣成器函数，那么就能为fixture 增加额外的清理逻辑。⽐如，下⾯的 db_connection 会在作为 fixture 使⽤时返回⼀个数据库连接，并在测试结束需要销毁 fixture 前，关闭这个连接：\n1 2 3 4 5 6 @pytest.fixture def db_connection(): \u0026#34;\u0026#34;\u0026#34;创建并返回一个数据库连接\u0026#34;\u0026#34;\u0026#34; conn = create_db_conn() yield conn conn.close() yield 前的代码在创建 fixture 前被调⽤\nyield 后的代码在销毁 fixture 前被调⽤\n除了作为函数参数，被主动注⼊测试⽅法中以外，pytest 的 fixture 还有另⼀种触发⽅式：⾃动执⾏。\n通过在调⽤ @pytest.fixture 时传⼊ autouse=True 参数，你可以创建⼀个会⾃动执⾏的 fixture。举个例⼦，下⾯的 prepare_data 就是⼀个会⾃动执⾏的 fixture：\n1 2 3 4 5 6 7 8 @pytest.fixture(autouse=True) def prepare_data(): # 在测试开始前，创建两个用户 User.objects.create(...) User.objects.create(...) yield # 在测试结束时，销毁所有用户 User.objects.all().delete() ⽆论测试函数的参数列表⾥是否添加了 prepare_data，prepare_data fixture ⾥的数据准备与销毁逻辑，都会在每个测试⽅法的开始与结束阶段⾃动执⾏。这类⾃动执⾏的fixture，⾮常适合⽤来做⼀些测试准备与事后清理⼯作\n除了 autouse 以外，fixture 还有⼀个⾮常重要的概念：作⽤域（scope）。\n在 pyetst 执⾏测试时，每当测试⽤例第⼀次引⽤某个 fixture，pytest 就会执⾏fixture 函数，将结果提供给测试⽤例使⽤，同时将其缓存起来。之后，根据 scope 的不\n同，这个被缓存的 fixture 结果会在不同的时机被销毁。⽽再次引⽤ fixture 会重新执⾏ fixture 函数获得新的结果，如此周⽽复始\npytest ⾥的 fixture 可以使⽤五种作⽤域，它们的区别如下。\nfunction（函数）：默认作⽤域，结果会在每个测试函数结束后销毁。 class（类）：结果会在执⾏完类⾥的所有测试⽅法后销毁。 module（模块）：结果会在执⾏完整个模块的所有测试后销毁。 package（包）：结果会在执⾏完整个包的所有测试后销毁。 session（测试会话）：结果会在测试会话（也就是⼀次完整的 pytest 执⾏过程）结束后销毁。 举个例⼦，假如你把上⾯ random_token fixture 的 scope 改为 session：\n1 2 3 @pytest.fixture(scope=\u0026#39;session\u0026#39;) def random_token() -\u0026gt; str: ... 那么，⽆论你在测试代码⾥引⽤了多少次 random_token，在⼀次完整的 pytest 会话⾥，所有地⽅拿到的随机 token 都是同⼀个值。\n因为 random_token 的作⽤域是 session，所以当 random_token 第⼀次被测试代码引⽤，创建出第⼀个随机值以后，这个值会被后续的所有测试⽤例复⽤。只有等到整个测试会话结束，random_token 的结果才会被销毁。\n总结⼀下，fixture 是 pytest 最为核⼼的功能之⼀。通过定义 fixture，你可以快速创建出⼀些可复⽤的测试固定件，并在每个测试的开始和结束阶段⾃动执⾏特定的代码逻辑。\n写单元测试不是浪费时间 不要总想着“补”测试 PR 是 Pull Request 的⾸字⺟缩写，它由开发者创建，⾥⾯包含对项⽬的代码修改。PR 在经过代码审查、讨论、调整的流程后，会并⼊主分⽀。PR 是⼈们通过 GitHub 进⾏代码协作的主要⼯具。\n但事实是，单元测试不光能验证程序的正确性，还能极⼤地帮助你改进代码设计。但这种帮助有⼀个前提，那就是你必须在编写代码的同时编写单元测试。当开发功能与编写测试同步进⾏时，你会来回切换⾃⼰的⻆⾊，分别作为代码的设计者和使⽤者，不断从代码⾥找出问题，调整设计。经过多次调整与打磨后，你的代码会变得更好、更具扩展性。\n测试代码并不⽐普通代码地位低，选择事后补测试，你其实⽩⽩丢掉了⽤测试驱动代码设计的机会。只有在编写代码时同步编写单元测试，才能更好地发挥单元测试的能⼒。\nTDD（test-driven development，测试驱动开发）是由 Kent Beck 提出的⼀种软件开发⽅式。在 TDD ⼯作流下，要对软件做⼀个改动，你不会直接修改代码，⽽会先写出这个改动所需要的测试⽤例。\nTDD 的⼯作流⼤致如下：\n写测试用例（哪怕测试⽤例引⽤的模块根本不存在）； 执⾏测试⽤例，让其失败； 编写最简单的代码（此时只关⼼实现功能，不关⼼代码整洁度）； 执⾏测试⽤例，让测试通过； 重构代码，删除重复内容，让代码变得更整洁； 执⾏测试⽤例，验证重构 重复整个过程。 但在实际⼯作中，我其实很少宣称⾃⼰在实践 TDD。因为在开发时，我基本不会严格遵循上⾯的 TDD 标准流程。⽐如，有时我会直接跳过 TDD 的前两个步骤，不写任何会失败的测试⽤例，直接就开始编写功能代码。\n难测试的代码就是烂代码 举个例⼦，当模块依赖了⼀个全局对象时，写单元测试就会变得很难。全局对象的基本特征决定了它在内存中永远只会存在⼀份。⽽在编写单元测试时，为了验证代码在不同场景下的⾏为，我们需要⽤到多份不同的全局对象。这时，全局对象的唯⼀性就会成为写测试最⼤的阻碍。\n因此，每当你发现很难为代码编写测试时，就应该意识到代码设计可能存在问题，需要努⼒调整设计，让代码变得更容易测试。也许你应该直接删掉全局对象，仅在它被⽤到的那⼏个地⽅每次⼿动创建⼀个新对象。也许你应该把 UserPostService 类按照不同的抽象级别，拆分为许多个不同的⼩类，把依赖 I/O 的功能和纯粹的数据处理完全隔离开来。\n单元测试是评估代码质量的标尺。每当你写好⼀段代码，都能清楚地知道到底写得好还是坏，因为单元测试不会撒谎。\n像应用代码一样对待测试代码 避免教条主义 好吧，我承认这个指责听上去有⼀些道理。但⾸先，单元测试⾥的单元（unit）其实并不严格地指某个⽅法、函数，其实指的是软件模块的⼀个⾏为单元，或者说功能单元\n其次，某个测试⽤例应该算作集成测试或单元测试，这真的重要吗？在我看来，所有的⾃动化测试只要能满⾜⼏条基本特征：快、⽤例间互相隔离、没有副作⽤，这样就够了。\n单元测试领域的理论确实很多，这刚好说明了⼀件事，那就是要做好单元测试真的很难。要更好地实践单元测试，你要做的第⼀件事就是抛弃教条主义，脚踏实地，不断寻求最合适当前项⽬的测试⽅案，这样才能最⼤地享受单元测试的好处。\n总结 除了本章提到的这些内容以外，我还建议你继续学习⼀些敏捷编程、领域驱动设计、整洁架构⽅⾯的内容。从我的个⼈经历来看，这些知识对于⼤型项⽬开发有很好的启发作⽤。\n⽆论如何，永远不要停⽌学习。\n结语 不要掉进完美主义的陷阱。因为写代码不是什么纯粹的艺术创作，完美的代码是不存在的。有时，代码只要能满⾜当前需求，⼜为未来扩展留了空间就⾜够了 在从事编程⼯作⼗余年后，我深知写代码这件事很难，⽽给⼤型项⽬写代码更是难上加难。写好代码没有捷径，⽆⾮是要多看书、多看别⼈的代码、多写代码⽽已，但这些事说起来简单，要做好并不容易\n","date":"2025-01-20T00:00:00Z","image":"https://a-b-ab.github.io/f0a862d920da24e3095d329565b8ad7942a031c4.jpg","permalink":"https://a-b-ab.github.io/p/python%E5%B7%A5%E5%8C%A0/","title":"python工匠"},{"content":"什么是TCP网络分层 应⽤层:应用层是网络协议栈中的最顶层，主要负责应用程序之间的通信。其中一种常见的应用层协议是HTTP协议，它定义了应用程序之间如何传递报文。 传输层:传输层是为两台主机之间的应用进程提供端到端的逻辑通信的层级。其中一种常见的传输层协议是TCP协议，它负责可靠的数据传输 ⽹络互连层:网络互连层负责主机之间的通信，它将传输层产生的数据包封装成分组数据包，并通过路由选择将其发送到目标主机。IP协议是网络互连层的主要协议，TCP和UDP协议都使用IP协议作为网络层协议。该层的主要作用是为数据包添加源地址和目标地址，并将数据包传送到目标地址。 网络访问层:网络访问层，也称为网络接口层，负责主机连接到物理网络所需的硬件和相关协议。以太网、Wi-Fi和蓝牙工作在这一层。网络访问层提供了主机与物理网络之间的接口。 三次握手 TCP的三次握手是为了建立可靠的连接，确保通信双方都能够正常发送和接收数据。下面是三次握手的过程：\n第一次握手：客户端向服务器发送一个带有SYM标志的数据包，表示请求建立连接，客户端将随机生成一个初始序列号(ISN)并将其放入SYN字段中 第二次握手：服务器段接收到客户端的请求后，会发送一个带有SYN和ACK标志的数据包作为响应，服务器将确定序列号(ACK)设置为客户端的ISN加一，并生成自己的初始序列号 第三次握手：客户端收到服务器的响应后，会发送一个带有ACK标志的数据包作为确认。客户端将确认序列号设置为服务器的ISN加一 如果只有两次握手，那么在某些情况下可能会导致不可靠的连接建立。例如，客户端发送了一个建立连接的请求，但由于网络延迟或其他原因，该请求在传输过程中被延迟到达服务器。服务器此时会误认为客户端要建立连接，于是发送确认响应。然而，客户端并未发送过请求，因此不会回复确认。这样就会导致服务器一直等待客户端的回复，浪费资源。\n四次挥手 TCP的四次挥手是为了结束已建立的连接，确保双方都能正确地关闭连接并释放资源。下面是四次挥手的过程\n第一次挥手：客户端发送一个带有FIN(结束)标志的数据包，表示自己已经没有数据要发送了，请求关闭连接 第二次挥手：服务器接收到客户端的结束请求后，会发送一个带有ACK(确认)标志的数据包作为响应，表示已收到客户端的结束请求 第三次挥手：服务器发送一个带有FIN标志的数据包，表示自己也没有数据要发送了，请求关闭连接 第四次挥手：客户端接收到服务器的结束请求后，会发送一个带有ACK标志的数据包作为确认，表示已收到服务器的结束请求 假设只有三次挥手，当客户端发送结束请求后，服务器收到后会发送确认，表示已收到客户端的结束请求。但是在此过程中，服务器可能还有未发送完的数据，如果直接关闭连接，那么这些数据就会丢失。因此，引入第三次挥手，服务器在发送结束请求前，先发送所有未发送完的数据，并等待客户端的确认。客户端接收到服务器的结束请求后，会确认并处理完未接收的数据，然后发送确认，表示自己已准备好关闭连接。\n","date":"2024-10-30T00:00:00Z","image":"https://a-b-ab.github.io/p/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/4489b3af8fc67423033c62d0af6023cdc7ccb3e7_hu801e5d6833fe0131a9e1c7bf315f011c_5558893_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/","title":"三次握手，四次挥手"},{"content":"为什么使用缓存 在程序内部使用缓存，比如使用map等数据结构作为内部缓存，可以快速获取对象。通过将经常使用的数据存储在缓存中，可以减少对数据库的频繁访问，从而提高系统的响应速度和性能。缓存可以将数据保存在内存中，读取速度更快，能够大大缩短数据访问的时间，提升用户体验。\n在业界中，通常在数据库之前添加一层Redis缓存，这样可以避免数据库的性能被大量的请求耗费。当有大量的并发请求时，数据库可能会成为瓶颈，而使用缓存可以有效地缓解数据库的压力。Redis作为一种高效的缓存解决方案，可以将热门数据存储在内存中，以快速响应用户的请求。这种缓存层的引入不仅可以提高系统的性能和吞吐量，还可以提高系统的可靠性和稳定性，因为即使数据库出现故障，缓存仍然可以提供部分服务。\n缓存还可以减少网络传输的负载，特别是在分布式系统中。通过将计算结果或频繁访问的数据缓存起来，可以避免重复计算和重复访问数据库，节省了网络带宽和服务器的资源消耗。这对于海量数据的查找和计算密集型任务尤为重要，可以大大提升系统的效率和可扩展性。\n总之，使用缓存可以优化系统的性能、提高响应速度、降低数据库负载、节省网络传输和服务器资源，从而提升用户体验和系统的可靠性。\n缓存穿透，击穿，雪崩 缓存穿透 缓存穿透指的是当一个请求查询的数据不在缓存中，也不在数据库中，导致每次请求都直接访问数据库，增加了数据库的负载。这可能是由于恶意攻击或者异常情况导致的。为了解决缓存穿透问题，可以采取以下措施\n在缓存中存储一个空值或者默认值，且设置成一定过期时间，以避免重复的无效查询，但是这种方案有缺陷就是redis会多出无用的key，浪费内存资源； 使用布隆过滤器等技术来过滤掉无效的请求，将可能不存在的数据快速过滤掉，布隆过滤器可以有效防止不存在的key进入业务调用数据库，但是需要提前将数据库数据预热到布隆过滤器中，并且他也有一种缺陷就是由于他的数据结构和算法导致无法删除热键，只能新增； 缓存击穿 缓存击穿指的是当某个热点数据过期或者被删除时，大量的请求同时涌入，导致数据库负载过高。这通常发生在高并发环境下。为了避免缓存击穿问题，可以采取以下措施：\n第一种就是将热点数据永久缓存进redis，并另起一个线程定时的去更新这个热点数据，那么就热点数据永远不会失效，但是缺陷是在定时任务启动前可能存在数据错误的情况； 第二种情况那么就是加锁，使用互斥锁或者分布式锁来保护对数据库的访问，确保只有一个请求能够重新加载数据到缓存中。但是这种虽然解决了数据库问题，但同时也带来了性能下降； 缓存雪崩 缓存雪崩指的是当缓存中大量的数据同时过期时，导致大量的请求直接访问数据库，造成数据库负载过高。这通常是由于缓存服务器故障、网络故障或者缓存数据过期时间设置不合理等原因导致的。为了避免缓存雪崩问题，可以采取以下措施：\n就是在给缓存数据设置过期时间的时候请加一个随机值使用不同的过期时间来分散缓存的失效时间，避免大量数据同时过期。 使用热点数据预加载技术，在缓存数据即将过期之前，提前加载数据到缓存中，确保数据的可用性。 如何保证缓存与数据库之间的数据一致性 保证缓存与数据库之间的强一致性是一个相对复杂的问题。尽管没有绝对的解决方案，但可以采取一些策略来尽可能地提高数据一致性。以下是几种常见的策略：\n第一种就是先删除缓存还是先写数据库，这两种都一样，我就说下先删除缓存带来的问题，先删除缓存确实可以在写完数据库后后续的操作都会更新缓存值，但是扛不住并发高，如果删除完缓存后还没来得及写入又被另一个线程读取了旧值更新缓存，那么这缓存白删除了，\n第二种就是先写数据库呢？如果数据库写完后，一是在删除缓存之前的读操作读取的仍然是旧值，二是，如果写操作完成后，缓存删除操作由于网络原因丢失了怎么办，以后读取操作都是旧值了；\n第三种也就是业界最常用的延时双删；但同时他也无法一定保证数据的一致性\n在操作数据库之前先删除缓存：首先，你需要先删除缓存中对应的数据，确保下一次读取请求不会命中旧的缓存数据 更新数据库：然后，你可以更新数据库中的数据，确保数据库中的数据是最新的。 再次删除缓存：最后，在延时之后，再次删除缓存中的数据。这样可以确保在延时结束后，读操作仍然可以从缓存中获取最新的数据。 如果写操作很频繁，那么缺陷就很明显：很容易产生脏数据并且也无法满足缓存与数据库之间的一致性； 第四种：引入MQ，当我们有两个消费者的时候，一个消费者只管消息的数据库操作，一个消费者只管消息的缓存操作，这样可以确保操作是原子操作。确保了不会删除缓存失败的问题。\n但是以上四种都无法保证缓存与数据库之间的强一致性，只能保证数据库与缓存之间的最终一致性；\n如何设置过期时间，实现原理是什么 redis有两种命令可以进行对key设置过期时间：expire和setex。这两种命令都可以用来给key设置过期时间。\n实现过期时间的原理可以分为两个部分。\n首先是主动删除。Redis会有一个定时任务，定期检查数据库中的key是否已经过期。如果发现某个key已经过期，那么Redis会直接将其删除。\n其次是被动删除。当应用程序尝试获取一个已经设置了过期时间的key时，Redis会检查该key是否已经过期。如果已经过期，Redis会在返回结果之前将该key删除。\n这样，通过主动删除和被动删除的组合，Redis实现了对key的过期时间的管理。这种混合实现的方式可以保证Redis中的数据始终是最新的，并且不会出现过期的数据。\n需要注意的是，Redis并不会为每个key都启动一个单独的定时任务去检查过期时间。相反，Redis会根据实际情况动态调整定时任务的执行频率，以提高性能和效率。这种设计可以有效地减少对系统资源的占用，提高Redis的性能和稳定性。\n海量数据下，如何快速查找一条记录 使用布隆过滤器：布隆过滤器是一种概率型数据结构，可以用于判断某个元素是否存在于集合中。在海量数据下，可以先使用布隆过滤器将不存在的key过滤掉，这样可以减少部分请求，提高查询效率。 合理选择存储结构：在缓存记录时，可以考虑使用适合的存储结构。如果存储的是大对象，使用key+value（json）形式，那么key可能会很大，不建议使用。而如果使用hash结构存储，可以充分利用Redis的哈希表特性，提高存储效率。此外，可以根据实际情况选择其他存储结构，如列表、有序集合等。 查询优化:如果Redis是集群部署的，数据根据槽位进行分配。如果我们自己对key进行了定位，可以直接访问对应的Redis节点，而不需要通过集群路由。这样可以减少Redis集群的机器计算，提高查询性能。 ","date":"2024-10-28T00:00:00Z","image":"https://a-b-ab.github.io/p/%E7%BC%93%E5%AD%98%E9%9D%A2%E8%AF%95/bde24615776e497ac96491a690a2dbe94932a7ce1b9eb5-A6LFaJ_huba2e2cae59d3d791bdf96efa52fa59e3_1810101_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/%E7%BC%93%E5%AD%98%E9%9D%A2%E8%AF%95/","title":"缓存面试"},{"content":"MySQL有哪几种数据存储引擎，有什么区别 MySQL支持多种数据存储引擎，其中最常见的是MyISAM和InnoDB引擎。可以通过使用\u0026quot;show engines\u0026ldquo;命令查看MySQL支持的存储引擎。\n存储方式：MyISAM引擎将数据和索引分别存储在两个不同的文件中，一个是.MYD用于存储数据，一个是.MYI文件用于存储索引，而InnoDB引擎将数据和索引存储在同一个文件中 锁机制：MyISAM引擎只支持表级锁，即在对某个表进行读写时，会锁住整个表，其他操作需要等待。而InnoDB引擎支持行级锁，可以在并发访问时只锁住需要操作的行，提高了并发性能。` 事务支持：MyISAM引擎不支持事务，而InnoDB引擎支持事务。事务是一种保证数据一致性和完整性的机制，可以将多个操作作为一个整体进行提交或回滚 外键支持：MyISAM引擎不支持外键约束，而InnoDB引擎支持外键约束。外键是用于维护表与表之间关系的一种机制，可以确保数据的一致性 什么是脏读、幻读、不可重复读？要怎么处理？ 脏读、不可重复读和幻读是数据库中的一些并发问题。\n脏读：是指一个事务在读取另一个未提交事务的数据时，如果未提交事务回滚了，则读取到的数据是不一致的 不可重复读：一个事务在相同的查询条件下，多次读取数据结果不一致，例如：事务A第一次读取数据时，事务B修改了相同的数据并提交，导致事务A第二次读取时数据不一致 幻读：一个事务在多次查询中，由于其他事务插入或删除数据，导致查询结构出现新增或减少的情况 处理这些问题的方法有几种：\n加锁：可以使用数据库提供的锁机制，如行级锁或表级锁，来控制并发访问，确保数据的一致性。但是加锁会降低并发性能。需要自己根据业务需要添加共享锁还是排它锁。 调整事务隔离级别：数据库提供了不同的事务隔离级别，如读未提交、读已提交、可重复读和串行化。可以根据具体情况选择合适的隔离级别来避免并发问题。 事务的基本特性和隔离级别有哪些 事务的基本特性ACID\n原子性：一个事务中的所有操作要么全部成功提交，要么全部失败回滚。事务是一个不可分割的工作单位，要么全部执行，要么全部不执行。 一致性：事务执行前后，数据库的完整性约束没有被破坏。事务的操作会将数据库从一个一致性状态转换为另一个一致性状态。 隔离性：并发执行的事务之间应该相互隔离，一个事务的执行不应该被其他事务干扰。隔离性确保每个事务在并发环境中都能独立执行，不会受其他事务的影响。 持久性：一旦事务提交成功，其所做的修改将永久保存在数据库中。即使发生系统崩溃或电源故障，数据库也能够恢复到事务提交后的状态。 隔离级别是控制事务隔离性的一个参数，常见的隔离级别包括：\n读未提交:最低级别的隔离级别，允许一个事务读取另一个事务未提交的数据。会出现脏读问题。 读已提交：保证一个事务只能读取到其他事务已经提交的数据，解决了脏读问题。但是可能会出现不可重复读问题。 可重复读：保证在一个事务中多次读取同一数据时，得到的结果是一致的。解决了不可重复读问题。但是可能会出现幻读问题。 串行化：最高级别的隔离级别，要求事务串行执行，避免了幻读问题。但是会降低并发性能。 MySQL的锁有哪些，什么时间隙锁 MySQL的锁可以根据锁的粒度进行划分，包括行锁、表锁和全局锁。下面是对每种锁的详细说明：\n行锁：InnoDB引擎支持行锁，它的粒度很小，可以提供较好的并发性能，但是会消耗更多的资源。行锁又可以细分为以下两种：\n共享锁：使用SELECT ** LOCK IN SHARE MODE语句来获取共享锁，读取操作期间可以共享，但会阻塞写操作。 排它锁：使用SELECT ** FOR UPDATE语句来获取排它锁，各种操作都会被阻塞，而且在执行INSERT、DELETE、UPDATE语句时会自动添加排它锁。 自增锁：当涉及到自增字段时，每次获取自增值时都会进行阻塞。需要注意的是，自增锁与业务逻辑无关。 表锁：直接锁住整张表而不是单独的行，表锁消耗的资源比较少，但是锁的粒度较大，导致并发性能较低。表锁可以细分为以下两种：\n表共享锁：多个会话可以同时获取表的共享锁，读操作可以并发进行，但是写（INSERT、DELETE、UPDATE）会被阻塞。\n表排它锁：获取表排它锁的会话会阻塞其他会话的所有操作，包括读和写。\n全局锁：使用FLUSH TABLES WITH READ LOCK语句来获取全局锁，通常用于表数据备份等场景。\n间隙锁：（Gap Lock）主要针对带有索引的字段。例如，对于一个拥有user_id索引的user表(user_id, name),数据有（1，a）（4，b）（9，c），当执行UPDATE user SET name = d WHERE user_id = 5时，会锁定 5-9 之间的记录（不包括自身）。而执行UPDATE user SET name = 5 WHERE user_id = 4时，则是记录锁，只锁住一条记录。\n另一种类型是Next-Key锁，它实际上是右侧界的记录锁。对于上述user表的索引，Next-Key锁会在(1, a)、(4, b)、(9, c)之间进行锁定。表示从左开区间到右开区间封闭，即（ -∞, 1], (1, 4], (4, 9], (9, +∞)。这样可以确保范围内的记录都受到锁的保护。\n通过使用间隙锁和Next-Key锁，可以更精确地控制并发操作，提高数据库的数据一致性和并发性能。\nMySQL的索引结构是什么样的？聚簇索引和非聚簇索引又是什么？ MySQL是一种广泛使用的关系型数据库管理系统，具有多种存储引擎和索引结构可供选择。存储引擎是MySQL用来处理数据的核心组件，而索引结构则用于提高数据检索的效率。\n在MySQL中，最常见和常用的存储引擎是InnoDB和MyISAM。InnoDB是一个支持事务处理和行级锁定的存储引擎，适用于处理大量并发操作和高可靠性要求的应用。而MyISAM则是一个更适合于读写不频繁的应用，它不支持事务处理，但速度较快。\n索引结构是用来加快数据检索速度的一种数据结构。\nB+树索引是MySQL中最常见的索引结构。它使用B+树的数据结构来存储索引值和对应的数据行位置。非叶子节点只存储索引值，叶子节点存储索引值和对应的数据行位置。B+树索引的叶子节点按照索引值的顺序排列，并且通过双向链表连接，使得范围查询和最左前缀匹配查询更高效。\n哈希索引将索引值通过哈希函数映射为一个唯一的哈希值，并将哈希值和对应的数据行位置存储在哈希表中。哈希索引适用于等值查询，但不支持范围查询和模糊查询。\n聚簇索引是索引值和表的数据存储在一起的索引结构。在InnoDB存储引擎中，聚簇索引使用B+树来实现，叶子节点存储数据行的实际数据。一个表只能有一个聚簇索引，如果没有显式指定主键，那么会找第一个unique字段当做主键索引，否则将会使用隐藏的rowid作为主键索引。\n非聚簇索引是索引值和表的数据分开存储的索引结构。在MyISAM存储引擎中，每个索引都是一个独立的文件，存储索引值和对应的数据行位置。一张表可以有多个非聚簇索引，比如表的普通索引。\nMySQL的覆盖索引和回表 覆盖索引是指索引包含了查询所需的所有字段，这样在查询时就可以直接使用索引中的数据，而无需回表去读取数据行。这种方式可以减少磁盘I/O操作，提高查询性能。\n当一个查询只需要从索引中获取数据而不需要回表时，就可以称之为覆盖索引查询。在这种情况下，数据库引擎只需要读取索引页，而不需要再去读取数据页，从而节省了磁盘I/O开销。\n使用覆盖索引可以提高查询性能的原因是，索引通常比数据行小很多，所以从索引中读取数据比从数据行中读取数据更快。此外，覆盖索引还可以减少内存的使用，因为不需要将数据行加载到内存中进行处理。\n为了使用覆盖索引，需要确保查询语句中只包含索引字段，并且索引能够满足查询条件和排序需求。如果查询语句中包含了非索引字段，那么数据库引擎仍然需要回表去读取数据行，无法实现覆盖索引查询的优化效果。\nMySQL的集群是如何搭建的，读写分离是怎么做的 MySQL的集群搭建通常使用主从复制的方式，并结合读写分离来提高数据库的性能和可用性。\n首先，在搭建MySQL集群之前，需要确定一个主节点和多个从节点。主节点负责处理写操作，从节点负责处理读操作。\n在主节点接收到写操作时，会将修改的数据记录到binlog日志中。binlog是二进制日志，用于记录数据库的所有修改操作。从节点会定期读取主节点的binlog日志，并将这些操作应用到自己的数据库中，实现主从数据的同步。\n通常情况下，当一个未提交的事务进行操作数据时，首先写入到undolog，其次写入redolog和binlog，但是提交事务之前并不会写入磁盘，只是在内存中，这种设计可以提高数据库的性能，因为将数据写入磁盘是比较耗时的操作，而将数据暂时保存在内存中可以减少磁盘访问的次数，从而提高数据库的处理速度。\n在读写分离的架构中，还需要考虑主从复制的延迟问题。由于主从复制是异步的，所以从节点上的数据可能不是实时同步的。可以通过设置合适的复制延迟时间来平衡数据的一致性和性能的需求。\n半同步复制是MySQL主从复制的一种机制，它在传输binlog日志时，主节点需要等待至少一个从节点确认收到并写入日志后才能继续进行下一步操作。\n具体的工作流程如下：\n当主节点完成一次事务的提交后，它会将binlog日志发送给一个或多个从节点。 主节点会等待至少一个从节点将binlog日志写入磁盘并发送一个确认消息给主节点。 一旦主节点收到至少一个从节点的确认消息，它才会认为该次提交已经完成，可以继续进行下一次操作。 谈谈如何对MySQL进行分库分表？多大数据量需要进行分库分表？分库分表的方式和分片策略由哪些？分库分表后，SQL语句的执行流程是怎样的？ 分库分表是一种常用的数据库架构优化技术，用于应对数据规模不断增长的情况。分库分表可以通过纵向拆分和横向拆分两种方式进行。\n纵向拆分是将不同的业务数据分开存储在不同的数据库中，每个数据库对应一个业务。这样可以有效避免单一数据库数据量过大导致的性能问题，但无法解决单个业务数据量过大的查询问题。 横向拆分是将单个表中的数据按照某些字段进行拆分，将数据分散存储在多个表中。这样可以减小单个表的数据量，提高查询效率。常见的分片策略有按照范围、按照哈希和按照取模等。 分库分表会引入新的问题，例如数据的拆分和合并、开发和维护的复杂度增加等。因此，在选择分库分表之前，需要根据业务的增长速度和数据量来判断是否需要进行分库分表。根据阿里的开发规范手册，一般在数据量达到500万或单个表文件大小增长到2G时，可以考虑进行分库分表的规划。\n通常，可以使用一些开源的分库分表中间件，如Mycat或ShardingSphere，来实现分库分表功能。这些中间件支持DDL、DML等语句的执行，能够进行排序、分组聚合等操作。但对于一些复杂的SQL语句，如子查询，可能存在一定的限制\nSQL语句的执行流程一般包括解析、优化、路由、分片和归并结果集等步骤。首先，数据库会对SQL语句进行解析，将其转换成内部数据结构。然后，通过优化器对SQL语句进行优化，生成最优的执行计划。接下来，根据分片策略，确定需要执行的数据库和表。然后，将SQL语句发送到相应的数据库节点执行。最后，将执行结果进行归并，返回给用户\n","date":"2024-10-27T00:00:00Z","image":"https://a-b-ab.github.io/p/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/2d5af84764dc1c4b23b15122a70a707d_hu19c350ce9a1e9c89d80ace30d58c3ce3_3734368_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"MySQL面试题"},{"content":"MVC模式 MVC 模式代表 Model-View-Controller（模型-视图-控制器） 模式。这种模式用于应用程序的分层开发。\nModel(模型) - 模型代表一个存取数据的对象或 JAVA POJO。它也可以带有逻辑，在数据变化时更新控制器。 View(视图) - 视图代表模型包含的数据的可视化。 Controller(控制器) - 控制器作用于模型和视图上。它控制数据流向模型对象，并在数据变化时更新视图。它使视图与模型分离开。 概要 目的：关注点分离\n模型：模型的任务是管理数据。不论数据是来自数据库、API 还是 JSON 对象，模型都要负责管理它们。\n视图:负责显示数据（模型）的用户界面，不包含业务逻辑。\n控制器:它充当视图和模型之间的中介。 它监听由视图触发的事件并查询相同的模型。\n实例：用户通过浏览器（视图）发送请求，服务器端的控制器处理请求，模型进行数据处理。\n当开发大型应用程序，需要清晰分离数据、业务逻辑和用户界面时，考虑使用MVC模式。\n代码 Model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import json class Person(object): def __init__(self, first_name = None, last_name = None): self.first_name = first_name self.last_name = last_name #returns Person name, ex: John Doe def name(self): return (\u0026#34;%s %s\u0026#34; % (self.first_name,self.last_name)) @classmethod #returns all people inside db.txt as list of Person objects def getAll(self): database = open(\u0026#39;db.txt\u0026#39;, \u0026#39;r\u0026#39;) result = [] json_list = json.loads(database.read()) for item in json_list: item = json.loads(item) person = Person(item[\u0026#39;first_name\u0026#39;], item[\u0026#39;last_name\u0026#39;]) result.append(person) return result 视图 它显示模型中获取的所有记录。 视图从不与模型交互； 控制器完成这项工作（与模型和视图通信）。 1 2 3 4 5 6 7 8 9 10 11 12 from model import Person def showAllView(list): print \u0026#39;In our db we have %i users. Here they are:\u0026#39; % len(list) for item in list: print item.name() def startView(): print \u0026#39;MVC - the simplest example\u0026#39; print \u0026#39;Do you want to see everyone in my db?[y/n]\u0026#39; def endView(): print \u0026#39;Goodbye!\u0026#39; 控制器 控制器通过 getAll() 方法与模型交互，该方法获取显示给最终用户的所有记录。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from model import Person import view def showAll(): people_in_db = Person.getAll() return view.showAllView(people_in_db) def start(): view.startView() input = raw_input() if input == \u0026#39;y\u0026#39;: return showAll() else: return view.endView() if __name__ == \u0026#34;__main__\u0026#34;: #running controller function start() 单例模式 单例模式(Singleton pattern)将类的实例化限制为一个对象。 它是一种创建模式，只涉及一个类来创建方法和指定的对象。 这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。 单例模式是一种创建型设计模式，它确保一个类只有一个实例，并提供了一个全局访问点来访问该实例。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Singleton: __instance = None @staticmethod def getInstance(): if Singleton.__instance == None: Singleton() return Singleton.__instance def __init__(self): if Singleton.__instance != None: raise Exception(\u0026#34;This class is a singleton!\u0026#34;) else: Singleton.__instance = self s = Singleton() print s s = Singleton.getInstance() print s s = Singleton.getInstance() print s 工厂模式 工厂模式(Factory Pattern)属于创建模式列表类别。 它提供了创建对象的最佳方式之一。 在工厂模式中，创建对象时不会向客户端公开逻辑并使用公共接口引用新创建的对象。\n它提供了一种创建对象的方式，使得创建对象的过程与使用对象的过程分离。\n工厂模式是使用工厂方法在 Python 中实现的。 当用户调用一个方法时，我们传入一个字符串并将返回值作为一个新对象通过工厂方法实现。 工厂方法中使用的对象类型由方法传递的字符串决定。\n通过使用工厂模式，可以将对象的创建逻辑封装在一个工厂类中，而不是在客户端代码中直接实例化对象，这样可以提高代码的可维护性和可扩展性。\n概要 意图：定义一个创建对象的接口，让其子类决定实例化哪一个具体的类。工厂模式使对象的创建过程延迟到子类。\n何时使用：当我们需要在不同条件下创建不同实例时。\n如何解决：通过让子类实现工厂接口，返回一个抽象的产品。\n缺点：每次增加一个产品时，都需要增加一个具体类和对应的工厂，使系统中类的数量成倍增加，增加了系统的复杂度和具体类的依赖。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Button(object): html = \u0026#34;\u0026#34; def get_html(self): return self.html class Image(BUttton): html = \u0026#34;\u0026lt;img\u0026gt;\u0026lt;/img\u0026gt;\u0026#34; class Input(Button): html = \u0026#34;\u0026lt;input\u0026gt;\u0026lt;/input\u0026gt;\u0026#34; class Flash(Button): html = \u0026#34;\u0026lt;obj\u0026gt;\u0026lt;/obj\u0026gt;\u0026#34; class ButtonFactory(): def create_button(self,typ): targetclass = typ.capitalize() return globals()[targetclass]() button_obj = ButtonFactory() button = [\u0026#39;image\u0026#39;, \u0026#39;input\u0026#39;, \u0026#39;flash\u0026#39;] for b in button: print button_obi.create_button(b).get_html() 这段代码展示了如何使用工厂模式创建不同类型的按钮对象。通过定义一个基类 Button 和多个子类 Image、Input、Flash，以及一个工厂类 ButtonFactory，可以根据传入的类型动态创建相应的按钮对象，并获取其 HTML 表示。\n建造者模式 建造者模式(Builder Pattern)是一种独特的设计模式，它有助于使用简单的对象构建复杂的对象并使用算法方法。 这种设计模式属于创建模式的范畴。 在这种设计模式中，构建器类逐步构建最终对象。 此构建器独立于其他对象。\n概要 产品(Product):要构建的复杂对象。产品类通常包含多个部分或属性。\n抽象建造者(Builder):定义了构建产品的抽象接口，包括构建产品的各个部分的方法。\n具体建造者(Concrete Builder):实现抽象建造者接口，具体确定如何构建产品的各个部分，并负责返回最终构建的产品。\n指导者(Director)：负责调用建造者的方法来构建产品，指导者并不了解具体的构建过程，只关心产品的构建顺序和方式。\n意图：将一个复杂的构建过程与其表示相分离，使得同样的构建过程可以创建不同的表示。\n主要解决：在软件系统中，一个复杂对象的创建通常由多个部分组成，这些部分的组合经常变化，但组合的算法相对稳定。\n实例：去肯德基，汉堡、可乐、薯条、炸鸡翅等是不变的，而其组合是经常变化的，生成出不同的\u0026quot;套餐\u0026quot;。\n如何解决：将变与不变的部分分离开。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class Director: __builder = None def setBuilder(self,builder): self.__builder = builder def getCar(self): car = Car() body = self.__builder.getBody() car.setBody(body) engine = self.__builder.getEngine() car.setEngine(engine) i = 0 while i \u0026lt; 4: wheel = self.__builder.getWheel() car.attachWeel(wheel) i += 1 return car class Car: def __init__(self): self.__wheels = list() self.__engine = None self.__body = None def setBody(self, body): self.__body = body def attachWheel(self, wheel): self.__wheels.append(wheel) def setEngine(self, engine): self.__engine = engine def specification(self): print \u0026#34;body: %s\u0026#34; % self.__body.shape print \u0026#34;engine horsepower: %d\u0026#34; % self.__engine.horsepower print \u0026#34;tire size: %d\\\u0026#39;\u0026#34; % self.__wheels[0].size class Builder: def getWheel(self): pass def getEngine(self): pass def getBody(self): pass class JeepBuilder(Builder): def getWheel(self): wheel = Wheel() wheel.size = 22 return wheel def getEngine(self): engine = Engine() engine.horsepower = 400 return engine def getBody(self): body = Body() body.shape = \u0026#34;SUV\u0026#34; return body class Wheel: size = None class Engine: horsepower = None class Body: shape = None def main(): jeepBuilder = JeepBuilder() director = Director() print \u0026#34;Jeep\u0026#34; director.setBuilder(jeepBuilder) jeep = director.getCar() jeep.specification() print \u0026#34;\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: main() 原型模式 原型模式(Prototype Pattern)有助于隐藏由类创建的实例的复杂性。 现有对象的概念与从头开始创建的新对象的概念不同。\n如果需要，新复制的对象可能会在属性上进行一些更改。 这种方法可以节省用于产品开发的时间和资源。\n概要 意图：使用原型实例指定要创建对象的种类，并通过拷贝这些原型创建新的对象\n主要解决：在运行时动态建立和删除原型。\n如何解决：通过已有的一个原型对象，快速生成与原型对象相同的实例。\n注意事项：与直接实例化类创建新对象不同，原型模式通过拷贝现有对象生成新对象。浅拷贝通过实现 Cloneable 实现，深拷贝通过实现 Serializable 读取二进制流实现。\n原型接口(Prototype Interface)：定义一个用于克隆自身的接口，通常包括一个 clone() 方法。\n具体原型类(Concrete Prototype)：实现原型接口的具体类，负责实际的克隆操作。这个类需要实现 clone() 方法，通常使用浅拷贝或深拷贝来复制自身。\n客户端(Client):使用原型实例来创建新的对象。客户端调用原型对象的 clone() 方法来创建新的对象，而不是直接使用构造函数。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 import copy class Prototype: _type = None _value = None def clone(self): pass def getType(self): return self._type def getValue(self): return self._value class Type1(Prototype): def __init__(self,number): self._type = \u0026#34;Type1\u0026#34; self._value = number def clone(self): return copy.copy(self) class Type2(Prototype): \u0026#34;\u0026#34;\u0026#34; Concrete prototype. \u0026#34;\u0026#34;\u0026#34; def __init__(self, number): self._type = \u0026#34;Type2\u0026#34; self._value = number def clone(self): return copy.copy(self) class ObjectFactory: __type1Value1 = None __type1Value2 = None __type2Value1 = None __type2Value2 = None @staticmethod def initialize(): ObjectFactory.__type1Value1 = Type1(1) ObjectFactory.__type1Value2 = Type1(2) ObjectFactory.__type2Value1 = Type2(1) ObjectFactory.__type2Value2 = Type2(2) @staticmethod def getType1Value1(): return ObjectFactory.__type1Value1.clone() @staticmethod def getType1Value2(): return ObjectFactory.__type1Value2.clone() @staticmethod def getType2Value1(): return ObjectFactory.__type2Value1.clone() @staticmethod def getType2Value2(): return ObjectFactory.__type2Value2.clone() def main(): ObjectFactory.initialize() instance = ObjectFactory.getType1Value1() print \u0026#34;%s: %s\u0026#34; % (instance.getType(), instance.getValue()) instance = ObjectFactory.getType1Value2() print \u0026#34;%s: %s\u0026#34; % (instance.getType(), instance.getValue()) instance = ObjectFactory.getType2Value1() print \u0026#34;%s: %s\u0026#34; % (instance.getType(), instance.getValue()) instance = ObjectFactory.getType2Value2() print \u0026#34;%s: %s\u0026#34; % (instance.getType(), instance.getValue()) if __name__ == \u0026#34;__main__\u0026#34;: main() 外观模式 外观模式(Facade Pattern)为子系统中的一组接口提供统一的接口。 它定义了一个任何子系统都可以使用的更高级别的接口。\n外观模式（Facade Pattern）隐藏系统的复杂性，并向客户端提供了一个客户端可以访问系统的接口。这种类型的设计模式属于结构型模式，它向现有的系统添加一个接口，来隐藏系统的复杂性。\n概要 意图：为一个复杂的子系统提供一个一致的高层接口。这样，客户端代码就可以通过这个简化的接口与子系统交互，而不需要了解子系统内部的复杂性。\n外观(Facade):提供一个简化的接口，封装了系统的复杂性。外观模式的客户端通过与外观对象交互，而无需直接与系统的各个组件打交道\n子系统(Subsystem):由多个相互关联的类组成，负责系统的具体功能。外观对象通过调用这些子系统来完成客户端的请求。\n客户端(Client):使用外观对象来与系统交互，而不需要了解系统内部的具体实现。\n代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class _IgnitionSystem(object): @staticmethod def produce_spark(): return True class _Engine(object): def __init__(self): self.revs_per_minute = 0 def turnon(self): self.revs_per_minute = 2000 def turnoff(self): self.revs_per_minute = 0 class _FuelTank(object): def __init__(self,level=30): self._level = level @property def level(self): return self._level @level.setter def level(self,level): self._level = level class _DashBoardLight(object): def __init__(self, is_on=False): self._is_on = is_on def __str__(self): return self.__class__.__name__ @property def is_on(self): return self._is_on @is_on.setter def is_on(self, status): self._is_on = status def status_check(self): if self._is_on: print(\u0026#34;{}: ON\u0026#34;.format(str(self))) else: print(\u0026#34;{}: OFF\u0026#34;.format(str(self))) class _HandBrakeLight(_DashBoardLight): pass class _FogLampLight(_DashBoardLight): pass class _Dashboard(object): def __init__(self): self.lights = {\u0026#34;handbreak\u0026#34;: _HandBrakeLight(), \u0026#34;fog\u0026#34;: _FogLampLight()} def show(self): for light in self.lights.values(): light.status_check() # Facade class Car(object): def __init__(self): self.ignition_system = _IgnitionSystem() self.engine = _Engine() self.fuel_tank = _FuelTank() self.dashboard = _Dashboard() @property def km_per_litre(self): return 17.0 def consume_fuel(self, km): litres = min(self.fuel_tank.level, km / self.km_per_litre) self.fuel_tank.level -= litres def start(self): print(\u0026#34;\\nStarting...\u0026#34;) self.dashboard.show() if self.ignition_system.produce_spark(): self.engine.turnon() else: print(\u0026#34;Can\u0026#39;t start. Faulty ignition system\u0026#34;) def has_enough_fuel(self, km, km_per_litre): litres_needed = km / km_per_litre if self.fuel_tank.level \u0026gt; litres_needed: return True else: return False def drive(self, km = 100): print(\u0026#34;\\n\u0026#34;) if self.engine.revs_per_minute \u0026gt; 0: while self.has_enough_fuel(km, self.km_per_litre): self.consume_fuel(km) print(\u0026#34;Drove {}km\u0026#34;.format(km)) print(\u0026#34;{:.2f}l of fuel still left\u0026#34;.format(self.fuel_tank.level)) else: print(\u0026#34;Can\u0026#39;t drive. The Engine is turned off!\u0026#34;) def park(self): print(\u0026#34;\\nParking...\u0026#34;) self.dashboard.lights[\u0026#34;handbreak\u0026#34;].is_on = True self.dashboard.show() self.engine.turnoff() def switch_fog_lights(self, status): print(\u0026#34;\\nSwitching {} fog lights...\u0026#34;.format(status)) boolean = True if status == \u0026#34;ON\u0026#34; else False self.dashboard.lights[\u0026#34;fog\u0026#34;].is_on = boolean self.dashboard.show() def fill_up_tank(self): print(\u0026#34;\\nFuel tank filled up!\u0026#34;) self.fuel_tank.level = 100 class CarFacade: def __init__(self): self.car = Car() def start_car(self): self.car.start() def drive_car(self, km=100): self.car.drive(km) def park_car(self): self.car.park() def fill_fuel(self): self.car.fill_up_tank() def switch_fog_lights(self, status): self.car.switch_fog_lights(status) # the main function is the Client def main(): car_facade = CarFacade() car_facade.start_car() car_facade.drive_car(100) car_facade.switch_fog_lights(\u0026#34;ON\u0026#34;) car_facade.park_car() car_facade.fill_fuel() car_facade.start_car() car_facade.drive_car(50) if __name__ == \u0026#34;__main__\u0026#34;: main() 命令模式 命令模式(Command Pattern)在动作之间添加了一个抽象级别，并包含一个调用这些动作的对象。\n在此设计模式中，客户端创建一个命令对象，其中包含要执行的命令列表。 创建的命令对象实现特定接口。\n在软件系统中，\u0026ldquo;行为请求者\u0026quot;与\u0026quot;行为实现者\u0026quot;通常呈现一种\u0026quot;紧耦合\u0026rdquo;。但在某些场合，比如要对行为进行\u0026quot;记录、撤销/重做、事务\u0026quot;等处理，这种无法抵御变化的紧耦合是不合适的。在这种情况下，如何将\u0026quot;行为请求者\u0026quot;与\u0026quot;行为实现者\u0026quot;解耦?将一组行为抽象为对象，实现二者之间的松耦合。这就是命令模式（Command Pattern）。\n概要 意图：将请求封装为一个对象，允许用户使用不同的请求对客户端进行参数化。\n解决的问题:解决在软件系统中请求者和执行者之间的紧耦合问题，特别是在需要对行为进行记录、撤销/重做或事务处理等场景。\n接收者(Receiver):执行命令的实际对象，知道如何执行与请求相关的操作，实际执行命令的对象。\n命令（Command）：定义执行命令的接口。定义了执行操作的接口，通常包含一个 execute 方法，用于调用具体的操作。\n具体命令(ConcreteCommand)：实现了命令接口，负责执行具体的操作。它通常包含了对接收者的引用，通过调用接收者的方法来完成请求的处理。\n调用者（Invoker）：发送命令的对象，它包含了一个命令对象并能触发命令的执行。调用者并不直接处理请求，而是通过将请求传递给命令对象来实现。\n客户端(Client):创建具体命令对象并设置其接收者，将命令对象交给调用者执行。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def demo(a,b,c): print \u0026#39;a:\u0026#39;,a print \u0026#39;b:\u0026#39;,b print \u0026#39;c:\u0026#39;,c class Command: def __init__(self, cmd, *args): self._cmd=cmd self._args=args def __call__(self, *args): return apply(self._cmd, self._args+args) cmd = Command(dir,__builtins__) print cmd() cmd = Command(demo,1,2) cmd(3) 适配器模式 适配器模式(Adapter pattern)充当两个不兼容接口之间的桥梁。 这种类型的设计模式属于结构模式，因为这种模式结合了两个独立接口的能力。\n此模式涉及单个类，负责连接独立或不兼容接口的功能。 现实生活中的例子可能是读卡器，它充当存储卡和笔记本电脑之间的适配器。 将存储卡插入读卡器，将读卡器插入笔记本电脑，这样就可以通过笔记本电脑读取存储卡。\n适配器设计模式有助于让类协同工作。 它根据需要将一个类的接口转换为另一个接口。 该模式包括一个物种形成和多态性，它命名一个名称和多个形式。 假设可以根据收集的要求使用形状类。\n适配器模式有两种类型\n对象适配器模式：这种设计模式依赖于对象实现。 因此，它被称为对象适配器模式。\n类适配器模式：这是实现适配器设计模式的另一种方式。 该模式可以使用多重继承来实现。\n概要 目标接口（Target）：定义客户需要的接口。\n适配者类（Adaptee）：定义一个已经存在的接口，这个接口需要适配。\n适配器类（Adapter）：实现目标接口，并通过组合或继承的方式调用适配者类中的方法，从而实现目标接口。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class EuropeanSocketInterface: def voltage(self): pass def live(self): pass def neutral(self): pass def earth(self): pass # Adaptee class Socket(EuropeanSocketInterface): def voltage(self): return 230 def live(self): return 1 def neutral(self): return -1 def earth(self): return 0 # Target interface class USASocketInterface: def voltage(self): pass def live(self): pass def neutral(self): pass # The Adapter class Adapter(USASocketInterface): __socket = None def __init__(self, socket): self.__socket = socket def voltage(self): return 110 def live(self): return self.__socket.live() def neutral(self): return self.__socket.neutral() # Client class ElectricKettle: __power = None def __init__(self, power): self.__power = power def boil(self): if self.__power.voltage() \u0026gt; 110: print \u0026#34;Kettle on fire!\u0026#34; else: if self.__power.live() == 1 and \\ self.__power.neutral() == -1: print \u0026#34;Coffee time!\u0026#34; else: print \u0026#34;No power.\u0026#34; def main(): # Plug in socket = Socket() adapter = Adapter(socket) kettle = ElectricKettle(adapter) # Make coffee kettle.boil() return 0 if __name__ == \u0026#34;__main__\u0026#34;: main() 装饰器模式 装饰器模式（Decorator Pattern）允许用户在不改变其结构的情况下向现有对象添加新功能。 这种类型的设计模式属于结构模式，因为这种模式充当现有类的包装器。\n这个模式创建了一个装饰器类，它包装了原始类并提供了额外的功能来保持类方法签名的完整性。\n装饰器模式的动机是动态地附加对象的附加职责。\n代码 以下代码简单演示了如何在Python中实现装饰器设计模式。 该插图涉及以类形式展示一家咖啡店。 创建的咖啡类是一个抽象类，这意味着它不能被实例化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import six from abc import ABCMeta @six.add_metaclass(ABCMeta) class Abstract_Coffee(object): def get_cost(self): pass def get_ingredients(self): pass def get_tax(self): return 0.1*self.get_cost() class Concrete_Coffee(Abstract_Coffee): def get_cost(self): return 1.00 def get_ingredients(self): return \u0026#39;coffee\u0026#39; @six.add_metaclass(ABCMeta) class Abstract_Coffee_Decorator(Abstract_Coffee): def __init__(self,decorated_coffee): self.decorated_coffee = decorated_coffee def get_cost(self): return self.decorated_coffee.get_cost() def get_ingredients(self): return self.decorated_coffee.get_ingredients() class Sugar(Abstract_Coffee_Decorator): def __init__(self,decorated_coffee): Abstract_Coffee_Decorator.__init__(self,decorated_coffee) def get_cost(self): return self.decorated_coffee.get_cost() def get_ingredients(self): return self.decorated_coffee.get_ingredients() + \u0026#39;, sugar\u0026#39; class Milk(Abstract_Coffee_Decorator): def __init__(self,decorated_coffee): Abstract_Coffee_Decorator.__init__(self,decorated_coffee) def get_cost(self): return self.decorated_coffee.get_cost() + 0.25 def get_ingredients(self): return self.decorated_coffee.get_ingredients() + \u0026#39;, milk\u0026#39; class Vanilla(Abstract_Coffee_Decorator): def __init__(self,decorated_coffee): Abstract_Coffee_Decorator.__init__(self,decorated_coffee) def get_cost(self): return self.decorated_coffee.get_cost() + 0.75 def get_ingredients(self): return self.decorated_coffee.get_ingredients() + \u0026#39;, vanilla\u0026#39; 代理模式 代理模式(Proxy Pattern)包含一个称为\u0026quot;Proxy\u0026quot;代理的新对象，以代替称为\u0026quot;真实主体\u0026quot;的现有对象。 真实主体创建的代理对象必须在同一接口上，这样客户端就不会知道代理被用来代替真实对象。 客户端向代理产生的请求通过真实主体传递。代理模式(Proxy Pattern)包含一个称为\u0026quot;Proxy\u0026quot;代理的新对象，以代替称为\u0026quot;真实主体\u0026quot;的现有对象。 真实主体创建的代理对象必须在同一接口上，这样客户端就不会知道代理被用来代替真实对象。 客户端向代理产生的请求通过真实主体传递。\n在代理模式（Proxy Pattern）中，一个类代表另一个类的功能，这种类型的设计模式属于结构型模式。\n代理模式通过引入一个代理对象来控制对原对象的访问。代理对象在客户端和目标对象之间充当中介，负责将客户端的请求转发给目标对象，同时可以在转发请求前后进行额外的处理。\n在代理模式中，我们创建具有现有对象的对象，以便向外界提供功能接口。\n概要 增加中间层：创建一个代理类，作为真实对象的中间层。 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Image: def __init__(self,filename): self._filename = filename def load_image_from_disk( self ): print(\u0026#34;loading \u0026#34; + self._filename ) def display_image( self ): print(\u0026#34;display \u0026#34; + self._filename) class Proxy: def __init__( self, subject ): self._subject = subject self._proxystate = None class ProxyImage(Proxy): def display_image( self ): if self._proxystate == None: self._subject.load_image_from_disk() self._proxystate = 1 print(\u0026#34;display \u0026#34; + self._subject._filename ) proxy_image1 = ProxyImage( Image(\u0026#34;HiRes_10Mb_Photo1\u0026#34;) ) proxy_image2 = ProxyImage( Image(\u0026#34;HiRes_10Mb_Photo2\u0026#34;) ) proxy_image1.display_image() # loading necessary proxy_image1.display_image() # loading unnecessary proxy_image2.display_image() # loading necessary proxy_image2.display_image() # loading unnecessary proxy_image1.display_image() # loading unnecessary 责任链模式 责任链模式(Chain of Responsibility)是一种设计模式。在责任链模式里，很多对象由每一个对象对其下家的引用而连接起来形成一条链。请求在这个链上传递，直到链上的某一个对象决定处理此请求。发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织和分配责任。\n责任链模式用于在软件中实现松散耦合，其中来自客户端的指定请求通过包含在其中的对象链传递。 它有助于构建对象链。 请求从一端进入，从一个对象移动到另一个对象。\n此模式允许对象在不知道哪个对象将处理请求的情况下发送命令。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class ReportFormat(object): PDF = 0 TEXT = 1 class Report(object): def __init__(self, format_): self.title = \u0026#39;Monthly report\u0026#39; self.text = [\u0026#39;Things are going\u0026#39;, \u0026#39;really, really well.\u0026#39;] self.format_ = format_ class Handler(object): def __init__(self): self.nextHandler = None def handle(self, request): self.nextHandler.handle(request) class PDFHandler(Handler): def handle(self, request): if request.format_ == ReportFormat.PDF: self.output_report(request.title, request.text) else: super(PDFHandler, self).handle(request) def output_report(self, title, text): print \u0026#39;\u0026lt;html\u0026gt;\u0026#39; print \u0026#39; \u0026lt;head\u0026gt;\u0026#39; print \u0026#39; \u0026lt;title\u0026gt;%s\u0026lt;/title\u0026gt;\u0026#39; % title print \u0026#39; \u0026lt;/head\u0026gt;\u0026#39; print \u0026#39; \u0026lt;body\u0026gt;\u0026#39; for line in text: print \u0026#39; \u0026lt;p\u0026gt;%s \u0026#39; % line print \u0026#39; \u0026lt;/body\u0026gt;\u0026#39; print \u0026#39;\u0026lt;/html\u0026gt;\u0026#39; class TextHandler(Handler): def handle(self, request): if request.format_ == ReportFormat.TEXT: self.output_report(request.title, request.text) else: super(TextHandler, self).handle(request) def output_report(self, title, text): print 5*\u0026#39;*\u0026#39; + title + 5*\u0026#39;*\u0026#39; for line in text: print line class ErrorHandler(Handler): def handle(self, request): print \u0026#34;Invalid request\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: report = Report(ReportFormat.TEXT) pdf_handler = PDFHandler() text_handler = TextHandler() pdf_handler.nextHandler = text_handler text_handler.nextHandler = ErrorHandler() pdf_handler.handle(report) 观察者模式 在观察者模式(Observer Pattern)中，对象表示为等待事件触发的观察者。 一旦指定的事件发生，观察者就会附加到主题上。 当事件发生时，主体告诉观察者它已经发生。\n观察者模式是一种行为型设计模式，它定义了一种一对多的依赖关系，当一个对象的状态发生改变时，其所有依赖者都会收到通知并自动更新。\n当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知依赖它的对象。观察者模式属于行为型模式。\n概要 实例：拍卖系统：拍卖师作为主题，竞价者作为观察者，拍卖价格更新时通知所有竞价者。\n主题（Subject）：也称为被观察者或可观察者，它是具有状态的对象，并维护着一个观察者列表。主题提供了添加、删除和通知观察者的方法。\n观察者（Observer）：观察者是接收主题通知的对象。观察者需要实现一个更新方法，当收到主题的通知时，调用该方法进行更新操作\n具体主题（Concrete Subject）：具体主题是主题的具体实现类。它维护着观察者列表，并在状态发生改变时通知观察者\n具体观察者（Concrete Observer）：具体观察者是观察者的具体实现类。它实现了更新方法，定义了在收到主题通知时需要执行的具体操作。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 import threading import time class Subject: def __init__(self): self._observers = [] def attach(self, observer): if observer not in self._observers: self._observers.append(observer) def detach(self, observer): try: self._observers.remove(observer) except ValueError: pass def notify(self, message): for observer in self._observers: observer.update(message) class Downloader(threading.Thread, Subject): def __init__(self): threading.Thread.__init__(self) Subject.__init__(self) self.i = 0 def run(self): print(\u0026#39;downloading\u0026#39;) for i in range(1, 5): self.i = i time.sleep(2) print(\u0026#39;unfunf\u0026#39;) self.notify(f\u0026#39;Download progress: {self.i}\u0026#39;) self.notify(\u0026#39;Download complete\u0026#39;) print(\u0026#39;hello world\u0026#39;) class Observer: def update(self, message): pass class Worker(threading.Thread, Observer): def __init__(self, subject): threading.Thread.__init__(self) self.subject = subject self.subject.attach(self) def run(self): for i in range(1, 5): print(f\u0026#39;worker running: {i} ({self.subject.i})\u0026#39;) time.sleep(1) self.subject.detach(self) print(\u0026#39;done\u0026#39;) def update(self, message): print(f\u0026#39;Worker received update: {message}\u0026#39;) t = Downloader() t.start() time.sleep(1) t1 = Worker(t) t1.start() t2 = Worker(t) t2.start() t3 = Worker(t) t3.start() 状态模式 它为状态机提供了一个模块，该模块使用子类来实现，由指定的状态机类派生。这些方法是独立于状态的，并引起使用装饰器声明的转换。\n在状态模式中，我们创建表示各种状态的对象和一个行为随着状态对象改变而改变的 context 对象。\n状态模式允许对象在内部状态改变时改变其行为，使得对象在不同的状态下有不同的行为表现。通过将每个状态封装成独立的类，可以避免使用大量的条件语句来实现状态切换。\n概要 意图： 允许一个对象在其内部状态改变时改变其行为，看起来就像是改变了其类一样。\n实例：篮球运动员状态：运动员可以有正常、不正常和超常等状态。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class ComputerState(object): name = \u0026#34;state\u0026#34; allowed = [] def switch(self, state): \u0026#34;\u0026#34;\u0026#34; Switch to new state \u0026#34;\u0026#34;\u0026#34; if state.name in self.allowed: print \u0026#39;Current:\u0026#39;,self,\u0026#39; =\u0026gt; switched to new state\u0026#39;,state.name self.__class__ = state else: print \u0026#39;Current:\u0026#39;,self,\u0026#39; =\u0026gt; switching to\u0026#39;,state.name,\u0026#39;not possible.\u0026#39; def __str__(self): return self.name class Off(ComputerState): name = \u0026#34;off\u0026#34; allowed = [\u0026#39;on\u0026#39;] class On(ComputerState): \u0026#34;\u0026#34;\u0026#34; State of being powered on and working \u0026#34;\u0026#34;\u0026#34; name = \u0026#34;on\u0026#34; allowed = [\u0026#39;off\u0026#39;,\u0026#39;suspend\u0026#39;,\u0026#39;hibernate\u0026#39;] class Suspend(ComputerState): \u0026#34;\u0026#34;\u0026#34; State of being in suspended mode after switched on \u0026#34;\u0026#34;\u0026#34; name = \u0026#34;suspend\u0026#34; allowed = [\u0026#39;on\u0026#39;] class Hibernate(ComputerState): \u0026#34;\u0026#34;\u0026#34; State of being in hibernation after powered on \u0026#34;\u0026#34;\u0026#34; name = \u0026#34;hibernate\u0026#34; allowed = [\u0026#39;on\u0026#39;] class Computer(object): \u0026#34;\u0026#34;\u0026#34; A class representing a computer \u0026#34;\u0026#34;\u0026#34; def __init__(self, model=\u0026#39;HP\u0026#39;): self.model = model # State of the computer - default is off. self.state = Off() def change(self, state): \u0026#34;\u0026#34;\u0026#34; Change state \u0026#34;\u0026#34;\u0026#34; self.state.switch(state) if __name__ == \u0026#34;__main__\u0026#34;: comp = Computer() comp.change(On) comp.change(Off) comp.change(On) comp.change(Suspend) comp.change(Hibernate) comp.change(On) comp.change(Off) 策略模式 策略模式(Strategy Pattern)是一种行为模式。 策略模式的主要目标是使客户能够选择不同的算法或程序来完成指定的任务。 不同的算法可以换入和换出，而不会为上述任务带来任何复杂性。\n概要 意图：将每个算法封装起来，使它们可以互换使用。\n实例：旅行方式选择：骑自行车、坐汽车等，每种方式都是一个可替换的策略。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import types class StrategyExample: def __init__(self, func = None): self.name = \u0026#39;Strategy Example 0\u0026#39; if func is not None: self.execute = types.MethodType(func, self) def execute(self): print(self.name) def execute_replacement1(self): print(self.name + \u0026#39;from execute 1\u0026#39;) def execute_replacement2(self): print(self.name + \u0026#39;from execute 2\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: strat0 = StrategyExample() strat1 = StrategyExample(execute_replacement1) strat1.name = \u0026#39;Strategy Example 1\u0026#39; strat2 = StrategyExample(execute_replacement2) strat2.name = \u0026#39;Strategy Example 2\u0026#39; strat0.execute() strat1.execute() strat2.execute() 模板模式 模板模式(Template Pattern)使用抽象操作在基类中定义基本算法，其中子类覆盖具体行为。 模板模式将算法的轮廓保存在一个单独的方法中。 此方法称为模板方法 以下是模板模式的不同特点\n它定义了算法在操作中的骨架 它包括重新定义算法某些步骤的子类。 概要 意图：在父类中定义了算法的骨架，并允许子类在不改变算法结构的前提下重定义算法的某些特定步骤。\n实例：地基、走线、水管等步骤相同，后期建筑如加壁橱、栅栏等步骤不同。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class MakeMeal: def prepare(self): pass def cook(self): pass def eat(self): pass def go(self): self.prepare() self.cook() self.eat() class MakePizza(MakeMeal): def prepare(self): print \u0026#34;Prepare Pizza\u0026#34; def cook(self): print \u0026#34;Cook Pizza\u0026#34; def eat(self): print \u0026#34;Eat Pizza\u0026#34; class MakeTea(MakeMeal): def prepare(self): print \u0026#34;Prepare Tea\u0026#34; def cook(self): print \u0026#34;Cook Tea\u0026#34; def eat(self): print \u0026#34;Eat Tea\u0026#34; makePizza = MakePizza() makePizza.go() print 25*\u0026#34;+\u0026#34; makeTea = MakeTea() makeTea.go() 享元模式 享元模式(Flyweight Pattern)属于结构设计模式类别。 它提供了一种减少对象数量的方法。 它包括有助于改进应用程序结构的各种功能。 享元对象最重要的特征是不可变的。 这意味着它们一旦构造就无法修改。 该模式使用 HashMap 来存储引用对象。\n享元模式是一种软件设计模式。它使用共享物件，用来尽可能减少内存使用量以及分享资讯给尽可能多的相似物件；它适合用于只是因重复而导致使用无法令人接受的大量内存的大量物件。通常物件中的部分状态是可以分享。常见做法是把它们放在外部数据结构，当需要使用时再将它们传递给享元。\n概要 意图：通过共享对象来减少创建大量相似对象时的内存消耗。\n实例：数据库连接池：数据库连接被复用，避免频繁创建和销毁连接。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ComplexGenetics(object): def __init__(self): pass def genes(self, gene_code): return \u0026#34;ComplexPatter[%s]TooHugeinSize\u0026#34; % (gene_code) class Families(object): family = {} def __new__(cls, name, family_id): try: id = cls.family[family_id] except KeyError: id = object.__new__(cls) cls.family[family_id] = id return id def set_genetic_info(self, genetic_info): cg = ComplexGenetics() self.genetic_info = cg.genes(genetic_info) def get_genetic_info(self): return (self.genetic_info) def test(): data = ((\u0026#39;a\u0026#39;, 1, \u0026#39;ATAG\u0026#39;), (\u0026#39;a\u0026#39;, 2, \u0026#39;AAGT\u0026#39;), (\u0026#39;b\u0026#39;, 1, \u0026#39;ATAG\u0026#39;)) family_objects = [] for i in data: obj = Families(i[0], i[1]) obj.set_genetic_info(i[2]) family_objects.append(obj) for i in family_objects: print \u0026#34;id = \u0026#34; + str(id(i)) print i.get_genetic_info() print \u0026#34;similar id\u0026#39;s says that they are same objects \u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: test() 抽象工厂模式 抽象工厂模式(Abstract Factory Pattern)也称为工厂的工厂。\n在抽象工厂模式中，接口是负责创建一个相关对象的工厂，不需要显式指定它们的类。每个生成的工厂都能按照工厂模式提供对象。\n抽象工厂模式提供了一种创建一系列相关或相互依赖对象的接口，而无需指定具体实现类。通过使用抽象工厂模式，可以将客户端与具体产品的创建过程解耦，使得客户端可以通过工厂接口来创建一族产品。\n抽象工厂模式相对于工厂方法模式来说，就是工厂方法模式是针对一个产品系列的，而抽象工厂模式是针对多个产品系列的，即工厂方法模式是一个产品系列一个工厂类，而抽象工厂模式是多个产品系列一个工厂类。在抽象工厂模式中，客户端不再负责对象的创建，而是把这个责任丢给了具体的工厂类，客户端只负责对对象的调用，从而明确了各个类的职责。并且当一系列相互关联的产品被设计到一个工厂类里后，客户端的调用将会变得非常简单，而且，如果要更换这一系列的产品，则只需要更换一个工厂类即可。\n如果客户端需要创建一些产品结构，而这些产品结构又分别属于不同的产品类别，则可以使用抽象工厂模式，抽象工厂模式中抽象工厂类负责定义创建对象的接口，具体这一系列对象的创建工作由实现抽象工厂的具体工厂类来完成。\n概要 意图：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们的具体类。\n实例：假设有不同类型的衣柜，每个衣柜（具体工厂）只能存放一类衣服（成套的具体产品），如商务装、时尚装等。每套衣服包括具体的上衣和裤子（具体产品）。所有衣柜都是衣柜类（抽象工厂）的具体实现，所有上衣和裤子分别实现上衣接口和裤子接口（抽象产品）。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class Window: __toolkit = \u0026#34;\u0026#34; __purpose = \u0026#34;\u0026#34; def __init__(self, toolkit, purpose): self.__toolkit = toolkit self.__purpose = purpose def getToolkit(self): return self.__toolkit def getType(self): return self.__purpose class GtkToolboxWindow(Window): def __init__(self): Window.__init__(self, \u0026#34;Gtk\u0026#34;, \u0026#34;ToolboxWindow\u0026#34;) class GtkLayersWindow(Window): def __init__(self): Window.__init__(self, \u0026#34;Gtk\u0026#34;, \u0026#34;LayersWindow\u0026#34;) class GtkMainWindow(Window): def __init__(self): Window.__init__(self, \u0026#34;Gtk\u0026#34;, \u0026#34;MainWindow\u0026#34;) class QtToolboxWindow(Window): def __init__(self): Window.__init__(self, \u0026#34;Qt\u0026#34;, \u0026#34;ToolboxWindow\u0026#34;) class QtLayersWindow(Window): def __init__(self): Window.__init__(self, \u0026#34;Qt\u0026#34;, \u0026#34;LayersWindow\u0026#34;) class QtMainWindow(Window): def __init__(self): Window.__init__(self, \u0026#34;Qt\u0026#34;, \u0026#34;MainWindow\u0026#34;) # Abstract factory class class UIFactory: def getToolboxWindow(self): pass def getLayersWindow(self): pass def getMainWindow(self): pass class GtkUIFactory(UIFactory): def getToolboxWindow(self): return GtkToolboxWindow() def getLayersWindow(self): return GtkLayersWindow() def getMainWindow(self): return GtkMainWindow() class QtUIFactory(UIFactory): def getToolboxWindow(self): return QtToolboxWindow() def getLayersWindow(self): return QtLayersWindow() def getMainWindow(self): return QtMainWindow() if __name__ == \u0026#34;__main__\u0026#34;: gnome = True kde = not gnome if gnome: ui = GtkUIFactory() elif kde: ui = QtUIFactory() toolbox = ui.getToolboxWindow() layers = ui.getLayersWindow() main = ui.getMainWindow() print \u0026#34;%s:%s\u0026#34; % (toolbox.getToolkit(), toolbox.getType()) print \u0026#34;%s:%s\u0026#34; % (layers.getToolkit(), layers.getType()) print \u0026#34;%s:%s\u0026#34; % (main.getToolkit(), main.getType()) 面向对象模式 迭代器模式 迭代器模式提供一种方法顺序访问一个聚合对象中的各个元素，而又不暴露其内部的表示。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import time def fib(): a,b=0,1 while True: yield b a,b=b,a+b g = fib() try: for e in g: print(e) time.sleep(1) except KeyboardInterrupt: print(\u0026#34;Calculation stopped\u0026#34;) ","date":"2024-10-27T00:00:00Z","image":"https://a-b-ab.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/c68fxf_hu5a96542783fbe1a1942f34aaf96ce728_991087_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"设计模式"},{"content":"基于github和vercel部署自定义域名网站 假设你已经可以在本地运行个人网站\n其实就是hugo主题文件配置好后：\nhugo server -D\n就可以了，接下来，如果项目根文件有public文件夹，直接删了\n然后执行\nhugo\n这样就会生成public文件夹，public文件夹包含了你网站的所有静态资源，如 HTML 文件、CSS 文件、JavaScript 文件和图片等，这些文件是网站的前端资源，浏览器会请求这些文件来渲染和显示网页内容\n也就是说你在github创建仓库后，把public文件夹提交到仓库，启用github page就可以将你的网站进行托管了，就是别人可以通过url访问你的个人网站\n创建github仓库 注册github账号，在页面左侧有个new点击，接下来你会看到Repository name,建议按GitHub账户名+github.io填写，然后仓库选择公开(默认)，然后点击就创建仓库。\n创建仓库后，复制你的仓库地址。在public启用终端，执行以下命令\ngit init\ngit add .\ngit commit -m \u0026ldquo;信息(随意)\u0026rdquo;\ngit branch -M main\ngit remote add origin \u0026lt;远程仓库地址\u0026gt;\ngit push -u origin main\n刷新一下你的远程仓库里面就有public文件了\n进行github page托管 如图：在仓库主页点击settings然后进入pages，看到图中的branch默认以main作为部署分支，点击save然后过一会在页面可以看到visit site点击后就可以跳转到你的网站了，别人可以通过这个url来访问你的网站了\n后续提交 改动文件后，在项目根目录执行\nhugo\n然后进入public文件执行\ngit add .\ngit commit -m \u0026ldquo;try\u0026rdquo;\ngit push\n就可以了\n进行vercel托管 vercel类似于github page，但远比github page强大，速度也快得多得多，而且将Github授权给vercel后，可以达到最优雅的发布体验，只需将代码轻轻一推，项目就自动更新部署了\n先进入vercel官网,注册账户,建议使用github账号关联\n点击 add new或者import project，然后import对应的你想进行托管的github仓库\n因为你的public已经是静态文件了，可以直接\n然后点击visit就可以看到vercel托管的网站了\n自定义域名 首先你得买个域名先，这一步以后写，国内的域名需要备案，我买的是国外的域名，用的是cloudflare\n点击你的vercel对应托管的网站\n点击 Domains在输入框输入你要绑定的自定义域名，然后点击add,完成后。\n进入你管理域名的软件，进行域名解析，就可以了 域名解析用cname，然后名称是自定义的域名，后面的是你在vercel中add操作得到的域名，保存就可以了\n","date":"2024-10-19T00:00:00Z","image":"https://a-b-ab.github.io/p/%E5%9F%BA%E4%BA%8Egithub%E5%92%8Cvercel%E9%83%A8%E7%BD%B2%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9F%9F%E5%90%8D%E7%BD%91%E7%AB%99/6015cf7ecba254fdaf9793afbffab6b2_huabd41559441e7fad3c84db969d05be77_2108767_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/%E5%9F%BA%E4%BA%8Egithub%E5%92%8Cvercel%E9%83%A8%E7%BD%B2%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9F%9F%E5%90%8D%E7%BD%91%E7%AB%99/","title":"基于github和vercel部署自定义域名网站"},{"content":"Linux 常用命令 帮助命令 man:查看命令帮助 help:查看Linux内置命令 文件和目录操作命令 ls:列出目录的内容及其内容属性信息 cd:从当前工作目录切换到指定的工作目录 cp:复制文件或目录 find:查找目录及目录下的文件 mkdir:创建目录 mv:移动或重命名文件 pwd:显示当前工作目录的绝对路径 rename:用于重命名文件 rm:删除一个或多个文件或目录 rmdir:删除空目录 touch:创建新的空文件，改变已有文件的时间戳属性 tree:以树形结构显示目录下的内容 basename:显示文件名或目录名 dirname:显示文件或目录路径 chattr:改变文件的扩展属性 lsattr:查看文件的扩展属性 file:显示文件的类型 md5sum:计算和校验文件的MD5值 查看文件及内容处理命令 cat:用于连接多个文件并且打印到屏幕输出或重定向到指定文件中 tac:反向显示文件内容 more:分页显示文件内容 less:分页显示文件内容，more的相反用法 head:显示文件内容的头部 tail:显示文件内容的尾部 cut:将文件的每一行按指定分隔符分割并输出 split:分割文件为不同的小片段 paste:按行合并文件内容 sort:对文件的文本内容排序 uniq:去除重复行 wc:统计文件的行数，单词数或字节数 iconv:转换文件的编码格式 dos2unix:将DOS格式文件转换成UNIX格式 diff: 比较文件的差异 vimdiff:命令行可视化文件比较工具 rev:反向输出文件内容 grep/egrep:过滤字符串 join:按两个文件的相同字段合并 tr:替换或删除字符 vi/vim:命令行文本编辑器 文件压缩及解压缩命令 tar:打包压缩 unzip:解压文件 gzip:gzip压缩工具 zip:压缩工具 信息显示命令 uname:显示操作系统相关信息的命令 hostname:显示或者设置当前系统的主机名 dmesg:显示开机信息，用于诊断系统故障 uptime:显示系统运行时间及负载 stat:显示文件或文件系统的状态 du:计算磁盘空间使用情况 df:报告文件系统磁盘空间的使用情况 top:实时显示系统资源使用情况 free:查看系统内存 date:显示与设置系统时间 cal:查看日历等时间信息 搜索文件命令 which:查找二进制命令，按环境变量PATH路径查找 find:从磁盘遍历文件或目录 whereis:查找二进制命令，按环境变量PATH路径查找 locate:从数据库查找命令，使用update更新库 用户管理命令 useradd:添加用户 usermod:修改系统已经存在的用户属性 userdel:删除用户 groupadd:添加用户组 passwd:修改用户密码 change:修改用户密码有效期限 id:查看用户的uid，gid及归属的用户组 su:切换用户身份 visudo:编辑/etc/sudoers 文件的专属命令 sudo:以另一个用户身份(默认root用户)执行事先在sudoers文件允许的命令 基础网络操作命令 telnet:使用TELNET 协议远程登录 ssh:使用SSH加密协议远程登录 scp:用于不同主机之间复制文件 wget:命令行下载文件 ping:测试主机之间网络的连通性 route:显示和设置linux系统的路由表 ifconfig:查看，配置，启用或禁用网络接口的命令 ifup:启动网卡 ifdown:关闭网卡 netstat:查看网络状态 ss:查看网络状态 深入网络操作命令 nmap:网络扫描命令 lsof:列举系统中已经被打开的文件 mail:发送和接收邮件 mutt:邮件管理命令 nslookup:交互式查询互联网DNS服务器命令 dig:查找DNS解析过程 host:查询DNS的命令 traceroute:追踪数据传输路由状况 tcpdump:命令行的抓包工具 有关磁盘与文件系统的命令 mount:挂载文件系统 umount:卸载文件系统 fsck:检查并修复Linux文件系统 dd:转换或复制文件 dumpe2fs:导出ext2/ext3/ext4文件系统信息 dump:ext2/3/4文件系统备份工具 fdisk:磁盘分区命令，适用于2TB以下磁盘分区 parted:磁盘分区命令，没有磁盘大小限制，常用于2TB以下磁盘分区 mkfs:格式化创建Linux文件系统 partprobe:更新内核的硬盘分区表信息 e2fsck:检查ext2/ext3/ext4类型文件系统 mkswap:创建linux交换分区 swapon:启用交换分区 swqpoff:关闭交换分区 sync:将内存缓冲区的数据写入磁盘 resize2fs:调整ext2/ext3/ext4文件系统大小 系统权限及用户授权相关命令 chmod:改变文件或目录权限 chown:改变文件或目录的属主和属组 chgrp:更改文件用户组 umask:显示或设置权限掩码 查看系统用户登录信息的命令 whoami:显示当前有效的用户名称，相当于执行id -un命令 who:显示目前登录系统的用户信息 w:显示已经登录系统的用户列表，并显示用户正在执行的指令 last:显示登入系统的用户 lastlog:显示系统中所有用户最近一次登录信息 users:显示当前登录系统的所有用户的用户列表 finger:查找并显示用户信息 内置命令及其它 echo:打印变量，或直接输出指定的字符串 printf:将结果格式化输出到标准输出 rpm:管理rpm包的命令 yum:自动化简单化管理rpm包的命令 watch:周期性的执行给定的命令，并将命令的输出以全屏方式显示 alias:设置系统别名 unalias:取消系统别名 date:查看或设置系统时间 clear:清屏 history:查看命令执行的历史记录 eject:弹出光驱 time:计算命令执行时间 nc:功能强大的网络工具 xargs:将标准输入转换成命令行参数 exec:调用并执行指令的命令 export:设置或显示环境变量 unset:删除变量或函数 type:用于判断另外一个命令是否是内置命令 bc:命令行科学计算器 系统管理与性能监视命令 chkconfig:管理linux系统开机启动项 vmstat:虚拟内存统计 mpstat:显示各个可用CPU的状态统计 iostat:统计系统IO sqr:全面获取系统的CPU，运行队列，磁盘I/O，分页(交换区)，内存，CPU中断和网络等性能数据 ipcs:用于报告linux中进程间通信设施的状态，显示的信息包括消息列表，共享内存和信号量的信息 ipcrm:用来删除一个或更多的消息队列，信号量或者共享内存标识 strace:用于诊断，调试linux用户空间跟踪器，我们用它来监控用户空间进程和内核的交互，比如系统调用，信号传递，进程状态变更等 ltrace:命令会跟踪进程的库函数调用，它会显现出那个库函数被调用 关机/重启/注销和查看系统信息的命令 shutdown:关机 halt:关机 poweroff:关闭电源 logout:退出当前登录的Shell exit:退出当前登录的Shell Ctrl+d:退出当前登录的Shell的快捷键 进程管理相关命令 bg:将一个在后台暂停的命令，变成继续执行(在后台执行) fg:将后台中的命令调至前台继续执行 jobs:查看当前有多少在后台运行的命令 kill:终止进程 killall:通过进程名终止进程 pkill:通过进程名终止进程 crontab:定时任务命令 ps:显示进程的快照 pstree:树形显示进程 nice/renice:调整程序运行的优先级 nohup:忽略挂起信号运行指定的命令 pgrep:查找匹配条件的进程 runlevel:查看系统当前运行级别 init:切换运行级别 service:启动，停止，重新启动和关闭系统服务，还可以显示所有系统服务的当前状态 ","date":"2024-10-18T00:00:00Z","image":"https://a-b-ab.github.io/p/linux%E5%91%BD%E4%BB%A4%E7%AE%80%E7%95%A5/92a192cfd865a37877f69c14707d073f0d5f0d2e2cd2f3-uVUe18_hua1015fee6750167c8d18a12ae10b99e3_2937587_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/linux%E5%91%BD%E4%BB%A4%E7%AE%80%E7%95%A5/","title":"Linux命令简略"},{"content":"前言 相信大家遇到过掉网卡驱动的情况，情况就是网络选项都没有了，WIFI图标都没有了\n救急 既然网络选项没有了，那我们要是急需上网怎么办，可以用数据线连接手机和笔记本,在手机设置里打开个人热点，打开USB共享这样你的笔记本就可以上网了。不过这个方法只适合临时，毕竟每次这样很麻烦。\n重启大法： 无脑重启，偶尔可以解决问题。 重启大法好啊！！！\n驱动重启或重装 打开设备管理器,查看网络设配器，图中我选中的就是无线网卡驱动，如果有个黄色感叹号就是驱动没有正常运行。 右键对应的驱动 可以先更新驱动程序试试 或者直接禁用设备，然后启用设备 如果都不行卸载设备 然后去对应电脑官网下载一个对应的无线网卡驱动 然后重启 注册表 键盘按win+r，弹出运行窗口 输入“redegit”，进入注册表， 删除HKEY_CLASSES_ROOT\\CLSID{3d09c1ca-2bcc-40b7-b9bb-3f3ec143a87b}，然后网卡禁用再启用。 硬件问题 掉网卡有多种可能，有可能是一些其他驱动冲突。有时出现掉网卡的状况可能是因为温度过高，硬件的自我保护机制，强制停止运行。这种情况很有可能是我们的散热系统里面进了灰尘，导致散热系统不能完全发挥性能。也有可能是网卡实体位置不对等等。。。\n如果出现这些问题去线下店看看喽\n线下店的老板告诉我，2022款的拯救者y7000p有这通病，可能是散热不良导致的~~\n","date":"2024-10-16T00:00:00Z","image":"https://a-b-ab.github.io/p/%E8%81%94%E6%83%B3%E6%8E%89%E7%BD%91%E5%8D%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/3168604b7ecc090e38e308737e9e2427_hu4d4c90b36ef2280206c7178ae8b45eca_1803425_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/%E8%81%94%E6%83%B3%E6%8E%89%E7%BD%91%E5%8D%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"联想掉网卡解决方案"},{"content":"高级特性 迭代 只要是可迭代对象，无论有无下标，都可以迭代，比如dict就可以迭代\n那么，如何判断一个对象是可迭代对象呢？方法是通过collections.abc模块的Iterable类型判断：\n1 2 from collections.abc import Iterable isinstance(\u0026#39;abc\u0026#39;,Iterable) Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身：\n1 2 for i,value in enumerate([\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;]): print(i,value) 列表生成式 1 2 [ x * x for x in range(1,11)] \u0026gt; [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 1 2 [ x * x for x in range(1,11) if x%2 == 0] \u0026gt; [4, 16, 36, 64, 100] 1 2 [ m + b for m in \u0026#39;ABC\u0026#39; for n in \u0026#39;XYZ\u0026#39;] \u0026gt; [\u0026#39;AX\u0026#39;, \u0026#39;AY\u0026#39;, \u0026#39;AZ\u0026#39;, \u0026#39;BX\u0026#39;, \u0026#39;BY\u0026#39;, \u0026#39;BZ\u0026#39;, \u0026#39;CX\u0026#39;, \u0026#39;CY\u0026#39;, \u0026#39;CZ\u0026#39;] 运用列表生成式，可以写出非常简洁的代码。例如，列出当前目录下的所有文件和目录名，可以通过一行代码实现：\n1 2 3 import os [d for d in os.listdir(\u0026#39;.\u0026#39;)] #os.listdir可以列出文件和目录 \u0026gt; [\u0026#39;.emacs.d\u0026#39;, \u0026#39;.ssh\u0026#39;, \u0026#39;.Trash\u0026#39;, \u0026#39;Adlm\u0026#39;, \u0026#39;Applications\u0026#39;, \u0026#39;Desktop\u0026#39;, \u0026#39;Documents\u0026#39;, \u0026#39;Downloads\u0026#39;, \u0026#39;Library\u0026#39;, \u0026#39;Movies\u0026#39;, \u0026#39;Music\u0026#39;, \u0026#39;Pictures\u0026#39;, \u0026#39;Public\u0026#39;, \u0026#39;VirtualBox VMs\u0026#39;, \u0026#39;Workspace\u0026#39;, \u0026#39;XCode\u0026#39;] 最后把一个list中所有的字符串变成小写：\n1 2 3 L = [\u0026#39;Hello\u0026#39;, \u0026#39;World\u0026#39;, \u0026#39;IBM\u0026#39;, \u0026#39;Apple\u0026#39;] [s.lower() for s in L] \u0026gt; [\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;, \u0026#39;ibm\u0026#39;, \u0026#39;apple\u0026#39;] 1 2 [x if x % 2 == 0 else -x for x in range(1,11)] \u0026gt; [-1, 2, -3, 4, -5, 6, -7, 8, -9, 10] 可见，在一个列表生成式中，for前面的if \u0026hellip; else是表达式，而for后面的if是过滤条件，不能带else。\n生成器 如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。\n要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator：\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; L = [x * x for x in range(10)] \u0026gt;\u0026gt;\u0026gt; L [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] \u0026gt;\u0026gt;\u0026gt; g = (x * x for x in range(10)) \u0026gt;\u0026gt;\u0026gt; g \u0026lt;generator object \u0026lt;genexpr\u0026gt; at 0x1022ef630\u0026gt; 如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值\n当然，上面这种不断调用next(g)实在是太变态了，正确的方法是使用for循环，因为generator也是可迭代对象：\n1 2 3 g = (x * x for x in range(10)) for n in g: print(n) 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易：\n1 2 3 4 5 6 7 def fib(max): n,a,b = 0,0,1 while n \u0026lt; max: print(b) a,b = b,a+b n = n + 1 return \u0026#39;done\u0026#39; 仔细观察，可以看出，fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。 也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator函数，只需要把print(b)改为yield b就可以了：\n1 2 3 4 5 6 7 def fib(max): n,a,b = 0,0,1 while n \u0026lt; max: yield b a,b = b,a+b n = n+1 return \u0026#39;done\u0026#39; 这就是定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator函数，调用一个generator函数将返回一个generator：\n1 2 3 \u0026gt;\u0026gt;\u0026gt; f = fib(6) \u0026gt;\u0026gt;\u0026gt; f \u0026lt;generator object fib at 0x104feaaa0\u0026gt; 这里，最难理解的就是generator函数和普通函数的执行流程不一样。普通函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 举个简单的例子，定义一个generator函数，依次返回数字1，3，5：\n1 2 3 4 5 6 7 def odd(): print(\u0026#39;step 1\u0026#39;) yield 1 print(\u0026#39;step 2\u0026#39;) yield(3) print(\u0026#39;step 3\u0026#39;) yield(5) 调用该generator函数时，首先要生成一个generator对象，然后用next()函数不断获得下一个返回值：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt;\u0026gt;\u0026gt; o = odd() \u0026gt;\u0026gt;\u0026gt; next(o) step 1 1 \u0026gt;\u0026gt;\u0026gt; next(o) step 2 3 \u0026gt;\u0026gt;\u0026gt; next(o) step 3 5 \u0026gt;\u0026gt;\u0026gt; next(o) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration 可以看到，odd不是普通函数，而是generator函数，在执行过程中，遇到yield就中断，下次又继续执行。执行3次yield后，已经没有yield可以执行了，所以，第4次调用next(o)就报错。\n请务必注意：调用generator函数会创建一个generator对象，多次调用generator函数会创建多个相互独立的generator。\n迭代器 我们已经知道，可以直接作用于for循环的数据类型有以下几种：\n一类是集合数据类型，如list、tuple、dict、set、str等；\n一类是generator，包括生成器和带yield的generator function。\n这些可以直接作用于for循环的对象统称为可迭代对象：Iterable。\n可以使用isinstance()判断一个对象是否是Iterable对象：\n可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator\n生成器都是Iterator对象，但list、dict、str虽然是Iterable，却不是Iterator。\n把list、dict、str等Iterable变成Iterator可以使用iter()函数：\n1 2 3 isinstance(iter([]),Iterator) isinstance(iter(\u0026#39;abc\u0026#39;),Iterator) 这是因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。\nIterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。\n凡是可作用于for循环的对象都是Iterable类型；\n凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列；\n集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。\nPython的for循环本质上就是通过不断调用next()函数实现的\n函数式编程 函数是Python内建支持的一种封装，我们通过把大段代码拆成函数，通过一层一层的函数调用，就可以把复杂任务分解成简单的任务，这种分解可以称之为面向过程的程序设计。函数就是面向过程的程序设计的基本单元。\n函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。\n函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！\nPython对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。\n高阶函数 变量可以指向函数 abs(-10)是函数调用，而abs是函数本身。\n要获得函数调用结果，我们可以把结果赋值给变量：\n1 2 3 \u0026gt;\u0026gt;\u0026gt; x = abs(-10) \u0026gt;\u0026gt;\u0026gt; x 10 但是，如果把函数本身赋值给变量呢？\n1 2 3 f = abs f \u0026lt;built-in function abs\u0026gt; 函数本身也可以赋值给变量，即：变量可以指向函数。\n函数名也是变量 函数名其实就是指向函数的变量！\n注：由于abs函数实际上是定义在import builtins模块中的，所以要让修改abs变量的指向在其它模块也生效，要用import builtins; builtins.abs = 10。\n传入函数 既然变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。\nmap/reduce map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。\n1 2 3 4 5 6 def f(x): return x*x r = map(f,[1,2,3,4,5,6,7,8,9]) list(r) [1, 4, 9, 16, 25, 36, 49, 64, 81] reduce把一个函数作用在一个序列[x1, x2, x3, \u0026hellip;]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是： 比方说对一个序列求和，就可以用reduce实现：\n再看reduce的用法。reduce把一个函数作用在一个序列[x1, x2, x3, \u0026hellip;]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是：\nreduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4)\n1 2 3 4 5 6 from functools import reduce def add(x,y): return x + y reduce(add,[1,3,5,7,9]) \u0026gt; 25 这个例子本身没多大用处，但是，如果考虑到字符串str也是一个序列，对上面的例子稍加改动，配合map()，我们就可以写出把str转换为int的函数\n1 2 3 4 5 6 7 8 9 from functools import reduce def fn(x,y): return x*10+y def char2num(s): digits = {\u0026#39;0\u0026#39;: 0, \u0026#39;1\u0026#39;: 1, \u0026#39;2\u0026#39;: 2, \u0026#39;3\u0026#39;: 3, \u0026#39;4\u0026#39;: 4, \u0026#39;5\u0026#39;: 5, \u0026#39;6\u0026#39;: 6, \u0026#39;7\u0026#39;: 7, \u0026#39;8\u0026#39;: 8, \u0026#39;9\u0026#39;: 9} return digits[s] reduce(fn,map(char2sum,\u0026#39;13579\u0026#39;)) 整理成一个str2int的函数就是：\n1 2 3 4 5 6 7 8 from functools import reduce DIGITS = {\u0026#39;0\u0026#39;: 0, \u0026#39;1\u0026#39;: 1, \u0026#39;2\u0026#39;: 2, \u0026#39;3\u0026#39;: 3, \u0026#39;4\u0026#39;: 4, \u0026#39;5\u0026#39;: 5, \u0026#39;6\u0026#39;: 6, \u0026#39;7\u0026#39;: 7, \u0026#39;8\u0026#39;: 8, \u0026#39;9\u0026#39;: 9} def str2int(s): def fn(s,y): return x*10+y def char2num(s): return DIGITS[s] return reduce(fn,map(char2num,s)) 还可以用lambda函数进一步简化成\n1 2 3 4 5 6 7 8 9 from functools import reduce DIGITS = {\u0026#39;0\u0026#39;: 0, \u0026#39;1\u0026#39;: 1, \u0026#39;2\u0026#39;: 2, \u0026#39;3\u0026#39;: 3, \u0026#39;4\u0026#39;: 4, \u0026#39;5\u0026#39;: 5, \u0026#39;6\u0026#39;: 6, \u0026#39;7\u0026#39;: 7, \u0026#39;8\u0026#39;: 8, \u0026#39;9\u0026#39;: 9} def char2num(s): return DIGITS[s] def str2int(s): return reduce(lambda x,y:x * 10 + y,map(char2num,s)) map用于将一个函数作用于一个序列，以此得到另一个序列；\nreduce用于将一个函数依次作用于上次计算的结果和序列的下一个元素，以此得到最终结果。\nfilter Python内建的filter()函数用于过滤序列。\n和map()类似，filter()也接收一个函数和一个序列。和map()不同的是，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。\n把一个序列中的空字符串删掉，可以这么写：\n1 2 3 4 5 def not_empty(s): return s and s.strip() list(filter(not_empty,[\u0026#39;A\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;B\u0026#39;, None, \u0026#39;C\u0026#39;, \u0026#39; \u0026#39;])) \u0026gt; 结果: [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;] 注意到filter()函数返回的是一个Iterator，也就是一个惰性序列，所以要强迫filter()完成计算结果，需要用list()函数获得所有结果并返回list。\nfilter()的作用是从一个序列中筛出符合条件的元素。由于filter()使用了惰性计算，所以只有在取filter()结果的时候，才会真正筛选并每次返回下一个筛出的元素。\nsorted sorted()函数也是一个高阶函数，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序：\nsorted([36,5,-12,9,-21],key=abs) [5, 9, -12, -21, 36]\nkey指定的函数将作用于list的每一个元素上，并根据key函数返回的结果进行排序。对比原始的list和经过key=abs处理过的list：\n返回函数 高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。\n我们来实现一个可变参数的求和。通常情况下，求和的函数是这样定义的：\n1 2 3 4 5 def calc_sum(*args): ax = 0 for n in args: ax = ax + n return ax 但是，如果不需要立刻求和，而是在后面的代码中，根据需要再计算怎么办？可以不返回求和的结果，而是返回求和的函数：\n1 2 3 4 5 6 7 def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax + n return ax return sum 当我们调用lazy_sum()时，返回的并不是求和结果，而是求和函数：\n1 2 3 \u0026gt;\u0026gt;\u0026gt; f = lazy_sum(1, 3, 5, 7, 9) \u0026gt;\u0026gt;\u0026gt; f \u0026lt;function lazy_sum.\u0026lt;locals\u0026gt;.sum at 0x101c6ed90\u0026gt; 调用函数f时，才真正计算求和的结果：\nf() 25\n在这个例子中，我们在函数lazy_sum中又定义了函数sum，并且，内部函数sum可以引用外部函数lazy_sum的参数和局部变量，当lazy_sum返回函数sum时，相关参数和变量都保存在返回的函数中，这种称为“闭包（Closure）”的程序结构拥有极大的威力。\n请再注意一点，当我们调用lazy_sum()时，每次调用都会返回一个新的函数，即使传入相同的参数：\n闭包 注意到返回的函数在其定义内部引用了局部变量args，所以，当一个函数返回了一个函数后，其内部的局部变量还被新函数引用，所以，闭包用起来简单，实现起来可不容易。\n另一个需要注意的问题是，返回的函数并没有立刻执行，而是直到调用了f()才执行。我们来看一个例子：\n1 2 3 4 5 6 7 8 9 def count(): fs = [] for i in range(1,4): def f(): return i*i fs.append(f) return fs f1,f2,f3 = count() 1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; f1() 9 \u0026gt;\u0026gt;\u0026gt; f2() 9 \u0026gt;\u0026gt;\u0026gt; f3() 9 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量\n如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变：\n1 2 3 4 5 6 7 8 9 def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1,4): fs.append(f(i)) #f(i)立刻被执行，因此i的当前值被传入f() return fs f1, f2, f3 = count() f1() 1 f2() 4 f3() 9\nnonlocal 使用闭包，就是内层函数引用了外层函数的局部变量。如果只是读外层变量的值，我们会发现返回的闭包函数调用一切正常：\n1 2 3 4 5 6 7 8 9 10 def inc(): x = 0 def fn(): # 仅读取x的值 return x + 1 return f = inc() print(f()) print(f()) 但是，如果对外层变量赋值，由于Python解释器会把x当作函数fn()的局部变量，它会报错：\n1 2 3 4 5 6 7 8 9 10 11 def inc(): x = 0 def fn(): # nonlocal x x = x+1 return x return fn f = inc() print(f()) # 1 print(f()) # 2 原因是x作为局部变量并没有初始化，直接计算x+1是不行的。但我们其实是想引用inc()函数内部的x，所以需要在fn()函数内部加一个nonlocal x的声明。加上这个声明后，解释器把fn()的x看作外层函数的局部变量，它已经被初始化了，可以正确计算x+1。\n使用闭包时，对外层变量赋值前，需要先使用nonlocal声明该变量不是当前函数的局部变量。\n一个函数可以返回一个计算结果，也可以返回一个函数。\n返回一个函数时，牢记该函数并未执行，返回函数中不要引用任何可能会变化的变量。\n总结来说，闭包使得 counter 函数能够记住它定义时所在的作用域内的变量 x 的内存位置。当 createCounter 执行完毕后，虽然它的栈帧被销毁了，但是 x 的内存位置仍然被 counter 记住，因此 counter 可以继续访问并修改 x 的值。这种机制保证了 x 的状态能够在多次调用 counter 之间保持一致。\n匿名函数 1 2 list(map(lamdba x:x*x,[1, 2, 3, 4, 5, 6, 7, 8, 9])) \u0026gt; [1, 4, 9, 16, 25, 36, 49, 64, 81] 关键字lambda表示匿名函数，冒号前面的x表示函数参数。 匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数：\n装饰器 由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。\n函数对象有一个__name__属性（注意：是前后各两个下划线），可以拿到函数的名字：\n现在，假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。\n本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下：\n1 2 3 4 5 def log(func): def wrapper(*args,**kw): print(\u0026#39;call %s():\u0026#39; % func.__name__) return func(*args,**kw) return wrapper 把@log放到now()函数的定义处，相当于执行了语句：\nnow = log(now)\n如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写出来会更复杂。比如，要自定义log的文本\n1 2 3 4 5 6 7 def log(text): def decorator(func): def wrapper(*args,**kw): print(\u0026#39;%s %s():\u0026#39; %(text,func.__name__)) return fun(*args,**kw) return wrapper return decorator 这个3层嵌套的decorator用法如下：\n1 2 3 @log(\u0026#39;execute\u0026#39;) def now(): print(\u0026#34;2024-6-1\u0026#34;) now() execute now(): 2024-6-1\n和两层嵌套的decorator相比，3层嵌套的效果是这样的\nnow = log(\u0026rsquo;execute\u0026rsquo;)(now)\n我们来剖析上面的语句，首先执行log(\u0026rsquo;execute\u0026rsquo;)，返回的是decorator函数，再调用返回的函数，参数是now函数，返回值最终是wrapper函数\n以上两种decorator的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有__name__等属性，但你去看经过decorator装饰之后的函数，它们的__name__已经从原来的\u0026rsquo;now\u0026rsquo;变成了\u0026rsquo;wrapper\u0026rsquo;：\n因为返回的那个wrapper()函数名字就是\u0026rsquo;wrapper\u0026rsquo;，所以，需要把原始函数的__name__等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错。 不需要编写wrapper.name = func.__name__这样的代码，Python内置的functools.wraps就是干这个事的，所以，一个完整的decorator的写法如下：\n1 2 3 4 5 6 7 8 import functools def long(func): @functools.wraps(func) def wrapper(*args,**kw): print(\u0026#39;call %s():\u0026#39;%func.__name__) return func(*args,**kw) return wrapper 或者针对带参数的decorator：\n1 2 3 4 5 6 7 8 9 10 import functools def log(text): def decorator(func): @functools.wraps(func) def wrapper(*args,**kw): print(\u0026#39;%s %s():\u0026#39;%(text,func.__name__)) return func(*args,**kw) return wrapper return decorator 在面向对象（OOP）的设计模式中，decorator被称为装饰模式。OOP的装饰模式需要通过继承和组合来实现，而Python除了能支持OOP的decorator外，直接从语法层次支持decorator。Python的decorator可以用函数实现，也可以用类实现。\n偏函数 但int()函数还提供额外的base参数，默认值为10。如果传入base参数，就可以做N进制的转换：\nint(\u0026lsquo;12345\u0026rsquo;,base=8)\nfunctools.partial就是帮助我们创建一个偏函数的，不需要我们自己定义int2()，可以直接使用下面的代码创建一个新的函数int2：\n1 2 3 import functools int2 = functools.partial(int,base=2) int2(\u0026#39;2100000\u0026#39;) 所以，简单总结functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。\n最后，创建偏函数时，实际上可以接收函数对象、*args和**kw这3个参数，当传入：\n1 int2 = functools.partial(int,base=2) 实际上固定了int()函数的关键字参数base，也就是： int2(\u0026lsquo;10010\u0026rsquo;) 相当于：\nkw = { \u0026lsquo;base\u0026rsquo;: 2 } int(\u0026lsquo;10010\u0026rsquo;, **kw)\n当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。\n模块 我们以内建的sys模块为例，编写一个hello的模块：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # !/usr/bin/env python3 # -*- coding:utf-8 -*- \u0026#39;a test module\u0026#39; __author__ = \u0026#39;Crow\u0026#39; import sys def test(): args = sys.argv if len(args) == 1: print(Hello, world!) if len(args) == 2: print(\u0026#39;Hello,%s!\u0026#39;% args[1]) else: print(\u0026#39;Too many arguments!\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: test() 导入sys模块后，我们就有了变量sys指向该模块，利用sys这个变量，就可以访问sys模块的所有功能。\nsys模块有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称，例如：\n运行python3 hello.py获得的sys.argv就是[\u0026lsquo;hello.py\u0026rsquo;]；\n运行python3 hello.py Michael获得的sys.argv就是[\u0026lsquo;hello.py\u0026rsquo;, \u0026lsquo;Michael\u0026rsquo;]\n使用模块 作用域 正常的函数和变量名是公开的（public），可以被直接引用，比如：abc，x123，PI等；\n类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如上面的__author__，__name__就是特殊变量，hello模块定义的文档注释也可以用特殊变量__doc__访问，我们自己的变量一般不要用这种变量名；\n类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，__abc等；\n外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public\n默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中：\n如果我们要添加自己的搜索目录，有两种方法：\n一是直接修改sys.path，添加要搜索的目录：\nimport sys sys.path.append(\u0026rsquo;/Users/michael/my_py_scripts\u0026rsquo;) 这种方法是在运行时修改，运行结束后失效。\n第二种方法是设置环境变量PYTHONPATH，该环境变量的内容会被自动添加到模块搜索路径中。设置方式与设置Path环境变量类似。注意只需要添加我们自己的搜索路径，Python本身的搜索路径不受影响。\n面向对象编程 OOP把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数\n类和实例 注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。\n有了__init__方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去：\n访问实例 注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。\n有了__init__方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去：\n有些时候，你会看到以一个下划线开头的实例变量名，比如_name，这样的实例变量外部是可以访问的，但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。\n双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问__name是因为Python解释器对外把__name变量改成了_Student__name，所以，仍然可以通过_Student__name来访问__name变量：\n但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把__name改成不同的变量名。\n总的来说就是，Python本身没有任何机制阻止你干坏事，一切全靠自觉。\n表面上看，外部代码“成功”地设置了__name变量，但实际上这个__name变量和class内部的__name变量不是一个变量！内部的__name变量已经被Python解释器自动改成了_Student__name，而外部代码给bart新增了一个__name变量。不信试试：\n继承和多态 当子类和父类都存在相同的run()方法时，我们说，子类的run()覆盖了父类的run()，在代码运行的时候，总是会调用子类的run()。这样，我们就获得了继承的另一个好处：多态。\n要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。我们定义的数据类型和Python自带的数据类型，比如str、list、dict没什么两样：\n多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思：\n静态语言vs动态语言 对于静态语言（例如Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用run()方法。\n对于Python这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个run()方法就可以了：\n这就是动态语言的“鸭子类型”，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。\n动态语言的鸭子类型特点决定了继承不像静态语言那样是必须的。\n获取对象信息 如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法：\n仅仅把属性和方法列出来是不够的，配合getattr()、setattr()以及hasattr()，我们可以直接操作一个对象的状态：\n一个正确的用法的例子如下：\n1 2 3 4 def readImage(fp): if hasattr(fp,\u0026#39;read\u0026#39;): return readData(fp) return None 假设我们希望从文件流fp中读取图像，我们首先要判断该fp对象是否存在read方法，如果存在，则该对象是一个流，如果不存在，则无法读取。hasattr()就派上了用场。\n请注意，在Python这类动态语言中，根据鸭子类型，有read()方法，不代表该fp对象就是一个文件流，它也可能是网络流，也可能是内存中的一个字节流，但只要read()方法返回的是有效的图像数据，就不影响读取图像的功能。\n实例属性和类属性 由于Python是动态语言，根据类创建的实例可以任意绑定属性。\n给实例绑定属性的方法是通过实例变量，或者通过self变量：\n1 2 3 4 5 6 class Student(object): def __init__(self,name): self.name = name s = Student(\u0026#39;Bob\u0026#39;) s.score = 90 但是，如果Student类本身需要绑定一个属性呢？可以直接在class中定义属性，这种属性是类属性，归Student类所有：\n1 2 class Student(object): name = \u0026#39;Student\u0026#39; 实例属性属于各个实例所有，互不干扰；\n类属性属于类所有，所有实例共享一个属性；\n不要对实例属性和类属性使用相同的名字，否则将产生难以发现的错误。\n面向对象高级编程 使用__slot__ 为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的__slots__变量，来限制该class实例能添加的属性：\n1 2 class Student(object): __slot__ = (\u0026#39;name\u0026#39;,\u0026#39;age\u0026#39;) # 用tuple定义允许绑定的属性名称 使用__slots__要注意，slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的： 除非在子类中也定义__slots，这样，子类实例允许定义的属性就是自身的__slots__加上父类的__slots__。\n使用@property 还记得装饰器（decorator）可以给函数动态加上功能吗？对于类的方法，装饰器一样起作用。Python内置的@property装饰器就是负责把一个方法变成属性调用的：\n1 2 3 4 5 6 7 8 9 10 11 12 class Student(object): @property def score(self): return self._score @score.setter def score(self,value): if not isinstance(value,int): raise ValueError(\u0026#39;score must be an integer!\u0026#39;) if value \u0026lt; 0 or value \u0026gt; 100: raise ValueError(\u0026#39;score must between 0 ~ 100!\u0026#39;) self._score = value @property的实现比较复杂，我们先考察如何使用。把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值，于是，我们就拥有一个可控的属性操作：\n注意到这个神奇的@property，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter和setter方法来实现的。\n还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性：\n1 2 3 4 5 6 7 8 9 10 11 12 class Student(object): @propery def birth(self): return self._birth @birth.setter def birth(self,value): self._birth = value @propery def age(self): return 2015 - self.birth 上面的birth是可读写属性，而age就是一个只读属性，因为age可以根据birth和当前时间计算出来。\n要特别注意：属性的方法名不要和实例变量重名。\n属性方法名和实例变量重名，会造成递归调用，导致栈溢出报错！\n多重继承 MixIn 在设计类的继承关系时，通常，主线都是单一继承下来的，例如，Ostrich继承自Bird。但是，如果需要“混入”额外的功能，通过多重继承就可以实现，比如，让Ostrich除了继承自Bird外，再同时继承Runnable。这种设计通常称之为MixIn。\n为了更好地看出继承关系，我们把Runnable和Flyable改为RunnableMixIn和FlyableMixIn。类似的，你还可以定义出肉食动物CarnivorousMixIn和植食动物HerbivoresMixIn，让某个动物同时拥有好几个MixIn：\n1 2 class Dog(Mammal,RunnableMixIn,CarnivorousMixIn): pass MixIn的目的就是给一个类增加多个功能，这样，在设计类的时候，我们优先考虑通过多重继承来组合多个MixIn的功能，而不是设计多层次的复杂的继承关系。\nPython自带的很多库也使用了MixIn。举个例子，Python自带了TCPServer和UDPServer这两类网络服务，而要同时服务多个用户就必须使用多进程或多线程模型，这两种模型由ForkingMixIn和ThreadingMixIn提供。通过组合，我们就可以创造出合适的服务来。\n比如，编写一个多进程模式的TCP服务，定义如下：\n1 2 class MyTCPServer(TCPServer, ForkingMixIn): pass 编写一个多线程模式的UDP服务，定义如下：\n1 2 class MyUDPServer(UDPServer, ThreadingMixIn): pass 如果你打算搞一个更先进的协程模型，可以编写一个CoroutineMixIn：\n1 2 class MyTCPServer(TCPServer, CoroutineMixIn): pass 由于Python允许使用多重继承，因此，MixIn就是一种常见的设计。\n只允许单一继承的语言（如Java）不能使用MixIn的设计。\n定制类 看到类似__slots__这种形如__xxx__的变量或者函数名就要注意，这些在Python中是有特殊用途的。\n__slots__我们已经知道怎么用了，len()方法我们也知道是为了能让class作用于len()函数。\n除此之外，Python的class中还有许多这样有特殊用途的函数，可以帮助我们定制类\nstr 这是因为直接显示变量调用的不是__str__()，而是__repr__()，两者的区别是__str__()返回用户看到的字符串，而__repr__()返回程序开发者看到的字符串，也就是说，repr()是为调试服务的。\n解决办法是再定义一个__repr__()。但是通常__str__()和__repr__()代码都是一样的，所以，有个偷懒的写法：\n1 2 3 4 5 6 class Student(object): def __init__(self, name): self.name = name def __str__(self): return \u0026#39;Student object (name=%s)\u0026#39; % self.name __repr__ = __str__ iter 如果一个类想被用于for \u0026hellip; in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的__next__()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。\n我们以斐波那契数列为例，写一个Fib类，可以作用于for循环：\n1 2 3 4 5 6 7 8 9 10 11 12 class Fib(object): def __init__(self): self.a,self.b = 0,1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，故返回自己 def __next__(self): self.a,self.b = self.b,self.a + self.b # 计算下一个值 if self.a \u0026gt; 100000: raise StopIteration return self.a # 返回下一个值 getitem 要表现得像list那样按照下标取出元素，需要实现__getitem__()方法：\n1 2 3 4 5 6 class Fib(object): def __getitem__(self,n): a,b = 1,1 for x in range(n): a,b = b, a+b return a 但是list有个神奇的切片方法：\nlist(range(100))[5:10] [5, 6, 7, 8, 9] 对于Fib却报错。原因是__getitem__()传入的参数可能是一个int，也可能是一个切片对象slice，所以要做判断：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Fib(object): def __getitem__(self,n): if isinstance(n,int): # n是索引 a,b = 1,1 for x in range(n): a,b = b,a+b return a if isinstance(n.slice): # n是切片 start = n.start stop = n.stop if start is None: start = 0 a,b = 1,1 L = [] for x in range(stop): if x \u0026gt;= start: L.append(a) a,b = b,a+b return L 此外，如果把对象看成dict，getitem()的参数也可能是一个可以作key的object，例如str。\n与之对应的是__setitem__()方法，把对象视作list或dict来对集合赋值。最后，还有一个__delitem__()方法，用于删除某个元素。\n总之，通过上面的方法，我们自己定义的类表现得和Python自带的list、tuple、dict没什么区别，这完全归功于动态语言的“鸭子类型”，不需要强制继承某个接口。\ngetattr 要避免这个错误，除了可以加上一个score属性外，Python还有另一个机制，那就是写一个__getattr__()方法，动态返回一个属性。修改如下：\n1 2 3 4 5 6 7 class Student(object): def __init__(self): self.name = \u0026#39;Michael\u0026#39; def __getattr__(self, attr): if attr==\u0026#39;score\u0026#39;: return 99 s = Student() s.name \u0026lsquo;Michael\u0026rsquo; s.score 99\n返回函数也是完全可以的：\n1 2 3 4 class Student(object): def __getattr__(self, attr): if attr==\u0026#39;age\u0026#39;: return lambda: 25 只是调用方式要变为：\ns.age() 25\n注意，只有在没有找到属性的情况下，才调用__getattr__，已有的属性，比如name，不会在__getattr__中查找。\n此外，注意到任意调用如s.abc都会返回None，这是因为我们定义的__getattr__默认返回就是None。要让class只响应特定的几个属性，我们就要按照约定，抛出AttributeError的错误：\n这实际上可以把一个类的所有属性和方法调用全部动态化处理了，不需要任何特殊手段。\n这种完全动态调用的特性有什么实际作用呢？作用就是，可以针对完全动态的情况作调用。\n举个例子：\n现在很多网站都搞REST API，比如新浪微博、豆瓣啥的，调用API的URL类似：\nhttp://api.server/user/friends http://api.server/user/timeline/list 如果要写SDK，给每个URL对应的API都写一个方法，那得累死，而且，API一旦改动，SDK也要改。\n利用完全动态的__getattr__，我们可以写出一个链式调用：\n1 2 3 4 5 6 7 8 9 10 11 class Chain(object): def __init__(self,path=\u0026#39;\u0026#39;) self._path = path def __getattr__(self,path): return Chain(\u0026#39;%s%s\u0026#39; %(self._path,path)) def __str__(self): return self._path __repr__ = __str__ 这样，无论API怎么变，SDK都可以根据URL实现完全动态的调用，而且，不随API的增加而改变！\n还有些REST API会把参数放到URL中，比如GitHub的API：\nGET /users/:user/repos 调用时，需要把:user替换为实际用户名。如果我们能写出这样的链式调用：\nChain().users(\u0026lsquo;michael\u0026rsquo;).repos 就可以非常方便地调用API了。有兴趣的童鞋可以试试写出来。\ncall 一个对象实例可以有自己的属性和方法，当我们调用实例方法时，我们用instance.method()来调用。能不能直接在实例本身上调用呢？在Python中，答案是肯定的。\n任何类，只需要定义一个__call__()方法，就可以直接对实例进行调用。请看示例\n1 2 3 4 5 6 class Student(object): def __init__(self,name): self.name = name def __call__(self): print(\u0026#39;My name is %s.\u0026#39;%self.name) 调用方式如下：\ns = Student(\u0026lsquo;Michael\u0026rsquo;) s() # self参数不要传入 My name is Michael.\ncall()还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样，所以你完全可以把对象看成函数，把函数看成对象，因为这两者之间本来就没啥根本的区别。\n如果你把对象看成函数，那么函数本身其实也可以在运行期动态创建出来，因为类的实例都是运行期创建出来的，这么一来，我们就模糊了对象和函数的界限。\n那么，怎么判断一个变量是对象还是函数呢？其实，更多的时候，我们需要判断一个对象是否能被调用，能被调用的对象就是一个Callable对象，比如函数和我们上面定义的带有__call__()的类实例：\ncallable(Student()) True callable(max) True callable([1, 2, 3]) False callable(None) False callable(\u0026lsquo;str\u0026rsquo;) False\n通过callable()函数，我们就可以判断一个对象是否是“可调用”对象。\n使用枚举类 更好的方法是为这样的枚举类型定义一个class类型，然后，每个常量都是class的一个唯一实例。Python提供了Enum类来实现这个功能\n1 2 3 from enum import Enum Month = Enum(\u0026#39;Month\u0026#39;,(\u0026#39;Jan\u0026#39;, \u0026#39;Feb\u0026#39;, \u0026#39;Mar\u0026#39;, \u0026#39;Apr\u0026#39;, \u0026#39;May\u0026#39;, \u0026#39;Jun\u0026#39;, \u0026#39;Jul\u0026#39;, \u0026#39;Aug\u0026#39;, \u0026#39;Sep\u0026#39;, \u0026#39;Oct\u0026#39;, \u0026#39;Nov\u0026#39;, \u0026#39;Dec\u0026#39;)) 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，或者枚举它的所有成员：\n1 2 for name,member in Month.__members__.items(): print(name,\u0026#39;=\u0026gt;\u0026#39;,member,\u0026#39;,\u0026#39;,member.value) value属性则是自动赋给成员的int常量，默认从1开始计数。\n如果需要更精确地控制枚举类型，可以从Enum派生出自定义类：\n1 2 3 4 5 6 7 8 9 10 11 from enum import Enum,unique @unique class Weekday(Enum): Sun = 0 # Sun的value被设定为0 Mon = 1 Tue = 2 Wed = 3 Thu = 4 Fri = 5 Sat = 6 @unique装饰器可以帮助我们检查保证没有重复值。\n访问这些枚举类型可以有若干种方法：\nday1 = Weekday.Mon print(day1) Weekday.Mon print(Weekday.Tue) Weekday.Tue print(Weekday[\u0026lsquo;Tue\u0026rsquo;]) Weekday.Tue print(Weekday.Tue.value) 2 print(day1 == Weekday.Mon) True print(day1 == Weekday.Tue) False print(Weekday(1)) Weekday.Mon print(day1 == Weekday(1)) True Weekday(7) Traceback (most recent call last): \u0026hellip; ValueError: 7 is not a valid Weekday for name, member in Weekday.members.items(): \u0026hellip; print(name, \u0026lsquo;=\u0026gt;\u0026rsquo;, member) \u0026hellip; Sun =\u0026gt; Weekday.Sun Mon =\u0026gt; Weekday.Mon Tue =\u0026gt; Weekday.Tue Wed =\u0026gt; Weekday.Wed Thu =\u0026gt; Weekday.Thu Fri =\u0026gt; Weekday.Fri Sat =\u0026gt; Weekday.Sat\n可见，既可以用成员名称引用枚举常量，又可以直接根据value的值获得枚举常量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from enum import Enum,unique class Gender(Enum): Male = 0 Female = 1 class Student(object): def __init__(self,name,gender): self.name = name self.gender = gender # 测试 bart = Student(\u0026#39;Bart\u0026#39;,Gender.Male) if bart.gender == Gender.Male: print(\u0026#39;测试通过!\u0026#39;) else: print(\u0026#39;测试失败!\u0026#39;) Enum可以把一组相关常量定义在一个class中，且class不可变，而且成员可以直接\n使用元类 动态语言和静态语言最大的不同，就是函数和类的定义，不是编译时定义的，而是运行时动态创建的。\ntype() 比方说我们要定义一个Hello的class，就写一个hello.py模块：\n1 2 3 class Hello(object): def Hello(self,name=\u0026#39;world\u0026#39;): print(\u0026#39;Hello,%s.\u0026#39;%name) 当Python解释器载入hello模块时，就会依次执行该模块的所有语句，执行结果就是动态创建出一个Hello的class对象，\n我们说class的定义是运行时动态创建的，而创建class的方法就是使用type()函数。\ntype()函数既可以返回一个对象的类型，又可以创建出新的类型，比如，我们可以通过type()函数创建出Hello类，而无需通过class Hello(object)\u0026hellip;的定义\n要创建一个class对象，type()函数依次传入3个参数：\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt;\u0026gt;\u0026gt; def fn(self, name=\u0026#39;world\u0026#39;): # 先定义函数 ... print(\u0026#39;Hello, %s.\u0026#39; % name) ... \u0026gt;\u0026gt;\u0026gt; Hello = type(\u0026#39;Hello\u0026#39;, (object,), dict(hello=fn)) # 创建Hello class \u0026gt;\u0026gt;\u0026gt; h = Hello() \u0026gt;\u0026gt;\u0026gt; h.hello() Hello, world. \u0026gt;\u0026gt;\u0026gt; print(type(Hello)) \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; print(type(h)) \u0026lt;class \u0026#39;__main__.Hello\u0026#39;\u0026gt; class的名称； 继承的父类集合，注意Python支持多重继承，如果只有一个父类，别忘了tuple的单元素写法； class的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。\n通过type()函数创建的类和直接写class是完全一样的，因为Python解释器遇到class定义时，仅仅是扫描一下class定义的语法，然后调用type()函数创建出class。\n正常情况下，我们都用class Xxx\u0026hellip;来定义类，但是，type()函数也允许我们动态创建出类来，也就是说，动态语言本身支持运行期动态创建类，这和静态语言有非常大的不同，要在静态语言运行期创建类，必须构造源代码字符串再调用编译器，或者借助一些工具生成字节码实现，本质上都是动态编译，会非常复杂。\nmetaclass 除了使用type()动态创建类以外，要控制类的创建行为，还可以使用metaclass。\nmetaclass，直译为元类，简单的解释就是：\n当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。\n但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。\n连接起来就是：先定义metaclass，就可以创建类，最后创建实例。\n所以，metaclass允许你创建类或者修改类。换句话说，你可以把类看成是metaclass创建出来的“实例”。\nmetaclass是Python面向对象里最难理解，也是最难使用的魔术代码。正常情况下，你不会碰到需要使用metaclass的情况，所以，以下内容看不懂也没关系，因为基本上你不会用到\n我们先看一个简单的例子，这个metaclass可以给我们自定义的MyList增加一个add方法：\n定义ListMetaclass，按照默认习惯，metaclass的类名总是以Metaclass结尾，以便清楚地表示这是一个metaclass：\n1 2 3 4 5 # metaclass 是类的模板，所以必须从`type`类型派生 class ListMetaclass(type): def __new__(cls,name,bases,attrs): attrs[\u0026#39;add\u0026#39;] = lambda self,value:self.append(value) return type.__new__(cls,name,bases,attrs) 有了ListMetaclass，我们在定义类的时候还要指示使用ListMetaclass来定制类，传入关键字参数metaclass：\n1 2 class MyList(list,metaclass=ListMetabclass): pass 当我们传入关键字参数metaclass时，魔术就生效了，它指示Python解释器在创建MyList时，要通过ListMetaclass.new()来创建，在此，我们可以修改类的定义，比如，加上新的方法，然后，返回修改后的定义。\nnew()方法接收到的参数依次是：\n当前准备创建的类的对象； 类的名字； 类继承的父类集合； 类的方法集合。\n动态修改有什么意义？直接在MyList定义中写上add()方法不是更简单吗？正常情况下，确实应该直接写，通过metaclass修改纯属变态。\n但是，总会遇到需要通过metaclass修改类定义的。ORM就是一个典型的例子。\nORM全称“Object Relational Mapping”，即对象-关系映射，就是把关系数据库的一行映射为一个对象，也就是一个类对应一个表，这样，写代码更简单，不用直接操作SQL语句。\n要编写一个ORM框架，所有的类都只能动态定义，因为只有使用者才能根据表的结构定义出对应的类来。\n让我们来尝试编写一个ORM框架。\n编写底层模块的第一步，就是先把调用接口写出来。比如，使用者如果使用这个ORM框架，想定义一个User类来操作对应的数据库表User，我们期待他写出这样的代码：\n1 2 3 4 5 6 7 8 9 10 11 class User(Model): # 定义类的属性到列的映F射 id = IntegerField(\u0026#39;id\u0026#39;) name = StringField(\u0026#39;username\u0026#39;) email = StringField(\u0026#39;email\u0026#39;) password = StringField(\u0026#39;password\u0026#39;) # 创建一个实例 u = User(id=12345,name=\u0026#39;Michael\u0026#39;,email=\u0026#39;test@orm.org\u0026#39;,password=\u0026#39;my-pwd\u0026#39;) # 保存到数据库 u.save() 其中，父类Model和属性类型StringField、IntegerField是由ORM框架提供的，剩下的魔术方法比如save()全部由父类Model自动完成。虽然metaclass的编写会比较复杂，但ORM的使用者用起来却异常简单。\n现在，我们就按上面的接口来实现该ORM。\n首先来定义Field类，它负责保存数据库表的字段名和字段类型：\n1 2 3 4 5 6 7 8 class Field(object): def __init__(self,name,column_type): self.name = name self.column_type = column_type def __str__(self): return \u0026#39;\u0026lt;%s:%s\u0026gt;\u0026#39; % (self.__class__.__name__,self.name) 在Field的基础上，进一步定义各种类型的Field，比如StringField，IntegerField等等：\n1 class StringField(Field) ","date":"2024-10-05T00:00:00Z","image":"https://a-b-ab.github.io/p/python%E7%AC%94%E8%AE%B0%E5%BB%96%E9%9B%AA%E5%B3%B0/39ddbb84c4a998da1962a60addcfed6c201d30f8_hu5d83a0257e79ca84b15f1817bf0faaaa_2937694_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/python%E7%AC%94%E8%AE%B0%E5%BB%96%E9%9B%AA%E5%B3%B0/","title":"Python笔记(廖雪峰)"},{"content":"概述 Redis 是一个开源的使用 ANSI C 语言编写、遵守 BSD 协议、支持网络、可基于内存、分布式、可选持久性的键值对(Key-Value)存储数据库，并提供多种语言的 API。\nRedis（Remote Dictionary Server）是一个开源的内存数据库，遵守 BSD 协议，它提供了一个高性能的键值（key-value）存储系统，常用于缓存、消息队列、会话存储等应用场景。\n数据结构 string：字符串 hash：散列 list：列表 set：集合 sorted set:有序集合 redis配置 Redis 的配置文件位于 Redis 安装目录下，文件名为 redis.conf 你可以通过修改 redis.conf 文件或使用 CONFIG set 命令来修改配置。\n参数说明 port 6379:指定 Redis 监听端口，默认端口为 6379 databases 16:设置数据库的数量，默认数据库为0，可以使用SELECT 命令在连接上指定数据库id dbfilename dump.rdb:指定本地数据库文件名，默认值为 dump.rdb dir ./:指定本地数据库存放目录 requirepass foobared:设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 AUTH 命令提供密码，默认关闭 数据类型 string string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。 string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据，比如jpg图片或者序列化的对象。 string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。\n常用命令 SET key value：设置键的值。 GET key：获取键的值 INCR key：将键的值加 1 DECR key：将键的值减 1 APPEND key value：将值追加到键的值之后 Hash Redis hash 是一个键值(key=\u0026gt;value)对集合，类似于一个小型的 NoSQL 数据库。 Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 每个哈希最多可以存储 2^32 - 1 个键值对。\n常用命令 HSET key field value：设置哈希表中字段的值。 HGET key field：获取哈希表中字段的值。 HGETALL key：获取哈希表中所有字段和值。 HDEL key field：删除哈希表中的一个或多个字段。 List Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 列表最多可以存储 2^32 - 1 个元素。\n常用命令 LPUSH key value：将值插入到列表头部。 RPUSH key value：将值插入到列表尾部 LPOP key：移出并获取列表的第一个元素 RPOP key：移出并获取列表的最后一个元素。 LRANGE key start stop：获取列表在指定范围内的元素。 Set Redis 的 Set 是 string 类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。\n常用命令 SADD key value：向集合添加一个或多个成员。 SREM key value：移除集合中的一个或多个成员。 SMEMBERS key：返回集合中的所有成员。 SISMEMBER key value：判断值是否是集合的成员。 zset(sorted set:有序集合) Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。\n常用命令 ZADD key score value：向有序集合添加一个或多个成员，或更新已存在成员的分数。 ZRANGE key start stop [WITHSCORES]：返回指定范围内的成员。 ZREM key value：移除有序集合中的一个或多个成员。 ZSCORE key value：返回有序集合中，成员的分数值。 zadd命令 添加元素到集合，元素在集合中存在则更新对应score\nzadd key score member\n其他高级数据类似 HyperLogLog 用于基数估计算法的数据结构。 常用于统计唯一值的近似值。\nBitmaps 位数组，可以对字符串进行位操作。 常用于实现布隆过滤器等位操作。\nGeospatial Indexes 处理地理空间数据，支持地理空间索引和半径查询。\nStreams 日志数据类型，支持时间序列数据。 用于消息队列和实时数据处理\nredis命令 Redis 命令用于在 redis 服务上执行操作 要在 redis 服务上执行命令需要一个 redis 客户端。Redis 客户端在我们之前下载的的 redis 的安装包中\n语法 Redis 客户端的基本语法为：\n$ redis-cli\n实例 以下实例讲解了如何启动 redis 客户端： 启动 redis 服务器，打开终端并输入命令 redis-cli，该命令会连接本地的 redis 服务。\n$ redis-cli redis 127.0.0.1:6379\u0026gt; redis 127.0.0.1:6379\u0026gt; PING\nPONG\n在远程服务上执行命令 如果需要在远程 redis 服务上执行命令，同样我们使用的也是 redis-cli 命令。\n语法 $ redis-cli -h host -p port -a password\n实例 以下实例演示了如何连接到主机为 127.0.0.1，端口为 6379 ，密码为 mypass 的 redis 服务上\n$redis-cli -h 127.0.0.1 -p 6379 -a \u0026ldquo;mypass\u0026rdquo; redis 127.0.0.1:6379\u0026gt; redis 127.0.0.1:6379\u0026gt; PING\nPONG\nredis键(key) Redis 键命令用于管理 redis 的键。\n语法 Redis 键命令的基本语法如下:\nredis 127.0.0.1:6379\u0026gt; COMMAND KEY_NAME\n实例 redis 127.0.0.1:6379\u0026gt; SET runoobkey redis OK redis 127.0.0.1:6379\u0026gt; DEL runoobkey (integer) 1\n在以上实例中 DEL 是一个命令， runoobkey 是一个键。 如果键被删除成功，命令执行后输出 (integer) 1，否则将输出 (integer) 0\nredis keys命令 del key：该命令用于在 key 存在时删除 key。 dump key:序列化给定 key ，并返回被序列化的值。 exists key:检查给定 key 是否存在。 expire key seconds：检查给定 key 是否存在。 expire key timestamp:EXPIREAT 的作用和 EXPIRE 类似，都用于为 key 设置过期时间。 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp)。 pexpire key milliseconds：设置 key 的过期时间以毫秒计。 pexpireat key milliseconds-timestamp：设置 key 过期时间的时间戳(unix timestamp) 以毫秒计 keys pattern：查找所有符合给定模式( pattern)的 key 。 move key db：将当前数据库的 key 移动到给定的数据库 db 当中。 persist key：移除 key 的过期时间，key 将持久保持。 pttl key：以毫秒为单位返回 key 的剩余的过期时间。 ttl key：以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 randomkey：从当前数据库中随机返回一个 key 。 rename key newkey：仅当 newkey 不存在时，将 key 改名为 newkey 。 renamenx key newkey：迭代数据库中的数据库键。 scan cursor [match pattern][count count]： type key:返回 key 所储存的值的类型。 redis 字符串 语法 redis 127.0.0.1:6379\u0026gt; COMMAND KEY_NAME\n实例 redis 127.0.0.1:6379\u0026gt; SET runoobkey redis OK redis 127.0.0.1:6379\u0026gt; GET runoobkey \u0026ldquo;redis\u0026rdquo;\n常用命令 set key value:设置指定key的值 get key:获取指定 key 的值。 getrange key start end：返回 key 中字符串值的子字符 getset key value:将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 getbit key offset:对 key 所储存的字符串值，获取指定偏移量上的位(bit)。 mget key1[key2]:获取所有(一个或多个)给定 key 的值。 setbit key offset value:对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。 setex key seconds value:将值 value 关联到 key ，并将 key 的过期时间设为 seconds (以秒为单位)。 setnx key value:只有在 key 不存在时设置 key 的值。 setrange key offset value:用 value 参数覆写给定 key 所储存的字符串值，从偏移量 offset 开始。 strlen key:返回 key 所储存的字符串值的长度。 mset key value [key value]：同时设置一个或多个 key-value 对。 msetnx key value [key value]:同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 psetex key milliseconds value:这个命令和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间，而不是像 SETEX 命令那样，以秒为单位 incr key:将 key 中储存的数字值增一 incrby key increment:将 key 所储存的值加上给定的增量值（increment）。 incrbyfloat key increment:将 key 所储存的值加上给定的浮点增量值（increment） decr key:将 key 中储存的数字值减一。 decrby key decrement:key 所储存的值减去给定的减量值（decrement） 。 append key value:如果 key 已经存在并且是一个字符串， APPEND 命令将指定的 value 追加到该 key 原来值（value）的末尾。 redis哈希 Redis hash 是一个 string 类型的 field（字段） 和 value（值） 的映射表，hash 特别适合用于存储对象。 Redis 中每个 hash 可以存储 232 - 1 键值对（40多亿）。\n实例 127.0.0.1:6379\u0026gt; HMSET runoobkey name \u0026ldquo;redis tutorial\u0026rdquo; description \u0026ldquo;redis basic commands for caching\u0026rdquo; likes 20 visitors 23000 OK 127.0.0.1:6379\u0026gt; HGETALL runoobkey\n\u0026ldquo;name\u0026rdquo; \u0026ldquo;redis tutorial\u0026rdquo; \u0026ldquo;description\u0026rdquo; \u0026ldquo;redis basic commands for caching\u0026rdquo; \u0026ldquo;likes\u0026rdquo; \u0026ldquo;20\u0026rdquo; \u0026ldquo;visitors\u0026rdquo; \u0026ldquo;23000\u0026rdquo; 在以上实例中，我们设置了 redis 的一些描述信息(name, description, likes, visitors) 到哈希表的 runoobkey 中。\n命令 hdel key field1[field2]：删除一个或多个哈希表字段 hexists key field:查看哈希表 key 中，指定的字段是否存在 hget key field:获取存储在哈希表中指定字段的值。 hgetall key:获取在哈希表中指定 key 的所有字段和值 hincrby key field increment:为哈希表 key 中的指定字段的整数值加上增量 increment 。 hincrbyfloat key field increment:为哈希表 key 中的指定字段的浮点数值加上增量 increment 。 hkeys key:获取哈希表中的所有字段 hlen key:获取哈希表中字段的数量 hmget key field1[field2]:获取所有给定字段的值 hmset key field1 value1[field1 value2]:同时将多个 field-value (域-值)对设置到哈希表 key 中。 hset key field value:将哈希表 key 中的字段 field 的值设为 value 。 hsetnx key field value:只有在字段 field 不存在时，设置哈希表字段的值。 hvals key:获取哈希表中所有值 hscan key cursor[matchb pattern][count count]:迭代哈希表中的键值对。 redis列表 Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边） 一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过40亿个元素)。\n命令 blpop key1[key2] timeout:移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 brpop key1[key2] timeout:移出并获取列表的最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 brpoplpush source destination timeout：从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它； 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 lindex key index:通过索引获取列表中的元素 linsert key before|after pivot value:在列表的元素前或者后插入元素 lien key:获取列表长度 lpop key:移出并获取列表的第一个元素 lpush key value1[value2]：将一个或多个值插入到列表头部 lpushx key value:将一个值插入到已存在的列表头部 lrange key start stop:获取列表指定范围内的元素 lrem key count value：移除列表元素 lset key index value:通过索引设置列表元素的值 ltrim key start stop:对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除 rpop key:移除列表的最后一个元素，返回值为移除的元素 rpoplpush source destination:移除列表的最后一个元素，并将该元素添加到另一个列表并返回 rpush key value1[value2]:在列表中添加一个或多个值到列表尾部 rpushx key value:为已存在的列表添加值 redis 集合 Redis 的 Set 是 String 类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。 集合对象的编码可以是 intset 或者 hashtable。 Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。\n命令 sadd key member1：向集合添加一个或多个成员 scard key:获取集合的成员数 sdiff key1:返回第一个集合与其他集合之间的差异。 sdiffstore destination key1:返回给定所有集合的差集并存储在 destination 中 sinter key1:返回给定所有集合的交集 sinterstore destination key1:返回给定所有集合的交集并存储在 destination 中 sismember key member:判断 member 元素是否是集合 key 的成员 smembers key：返回集合中的所有成员 smove source destination member：将 member 元素从 source 集合移动到 destination 集合 spop key:移除并返回集合中的一个随机元素 srandmember key[count]:返回集合中一个或多个随机数 srem key member1[member2]:移除集合中一个或多个成员 sunion key1:返回所有给定集合的并集 sunionstore deswwtination key1:所有给定集合的并集存储在 destination 集合中 sscan key cursor [match pattern][count count]:迭代集合中的元素 redis 有序集合（sorted set） Redis 有序集合和集合一样也是 string 类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个 double 类型的分数。redis 正是通过分数来为集合中的成员进行从小到大的排序。 有序集合的成员是唯一的,但分数(score)却可以重复。\n实例 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 1 redis (integer) 1 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 2 mongodb (integer) 1 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 3 mysql (integer) 1 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 3 mysql (integer) 0 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 4 mysql (integer) 0 redis 127.0.0.1:6379\u0026gt; ZRANGE runoobkey 0 10 WITHSCORES\n\u0026ldquo;redis\u0026rdquo; \u0026ldquo;1\u0026rdquo; \u0026ldquo;mongodb\u0026rdquo; \u0026ldquo;2\u0026rdquo; \u0026ldquo;mysql\u0026rdquo; \u0026ldquo;4\u0026rdquo; 命令 ZADD key score1 member1 [score2 member2]:向有序集合添加一个或多个成员，或者更新已存在成员的分数 ZCARD key:获取有序集合的成员数 ZCOUNT key min max:计算在有序集合中指定区间分数的成员数 ZINCRBY key increment member:有序集合中对指定成员的分数加上增量 increment ZINTERSTORE destination numkeys key [key \u0026hellip;]:计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 destination 中 ZLEXCOUNT key min max:在有序集合中计算指定字典区间内成员数量 ZRANGE key start stop [WITHSCORES]:通过索引区间返回有序集合指定区间内的成员 ZRANGEBYLEX key min max [LIMIT offset count]:通过字典区间返回有序集合的成员 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT]:通过分数返回有序集合指定区间内的成员 ZRANK key member:返回有序集合中指定成员的索引 ZREM key member [member \u0026hellip;]:移除有序集合中的一个或多个成员 ZREMRANGEBYLEX key min max:移除有序集合中给定的字典区间的所有成员 ZREMRANGEBYRANK key start stop:移除有序集合中给定的排名区间的所有成员 ZREMRANGEBYSCORE key min max:移除有序集合中给定的分数区间的所有成员 ZREVRANGE key start stop [WITHSCORES]:返回有序集中指定区间内的成员，通过索引，分数从高到低 ZREVRANGEBYSCORE key max min [WITHSCORES]:返回有序集中指定分数区间内的成员，分数从高到低排序 ZREVRANK key member:返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 ZSCORE key member:返回有序集中，成员的分数值 ZUNIONSTORE destination numkeys key [key \u0026hellip;]:计算给定的一个或多个有序集的并集，并存储在新的 key 中 ZSCAN key cursor [MATCH pattern] [COUNT count]:迭代有序集合中的元素（包括元素成员和元素分值） redis hyperloglog Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。\n什么是基数 比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。\n命令 PFADD key element [element \u0026hellip;]:添加指定元素到 HyperLogLog 中。 PFCOUNT key [key \u0026hellip;]:返回给定 HyperLogLog 的基数估算值。 PFMERGE destkey sourcekey [sourcekey \u0026hellip;]:将多个 HyperLogLog 合并为一个 HyperLogLog redis 发布订阅 Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。\n实例 以下实例演示了发布订阅是如何工作的，需要开启两个 redis-cli 客户端。 在我们实例中我们创建了订阅频道名为 runoobChat:\nredis 127.0.0.1:6379\u0026gt; SUBSCRIBE runoobChat\nReading messages\u0026hellip; (press Ctrl-C to quit)\n\u0026ldquo;subscribe\u0026rdquo; \u0026ldquo;runoobChat\u0026rdquo; (integer) 1 现在，我们先重新开启个 redis 客户端，然后在同一个频道 runoobChat 发布两次消息，订阅者就能接收到消息。\nredis 127.0.0.1:6379\u0026gt; PUBLISH runoobChat \u0026ldquo;Redis PUBLISH test\u0026rdquo;\n(integer) 1\nredis 127.0.0.1:6379\u0026gt; PUBLISH runoobChat \u0026ldquo;Learn redis by runoob.com\u0026rdquo;\n(integer) 1\n#订阅者的客户端会显示如下消息\n\u0026ldquo;message\u0026rdquo; \u0026ldquo;runoobChat\u0026rdquo; \u0026ldquo;Redis PUBLISH test\u0026rdquo; \u0026ldquo;message\u0026rdquo; \u0026ldquo;runoobChat\u0026rdquo; \u0026ldquo;Learn redis by runoob.com\u0026rdquo; 命令 PSUBSCRIBE pattern [pattern \u0026hellip;]：订阅一个或多个符合给定模式的频道。 PUBSUB subcommand [argument [argument \u0026hellip;]]：查看订阅与发布系统状态。 PUBLISH channel message：将信息发送到指定的频道。 PUNSUBSCRIBE [pattern [pattern \u0026hellip;]]：退订所有给定模式的频道。 SUBSCRIBE channel [channel \u0026hellip;]：订阅给定的一个或多个频道的信息。 UNSUBSCRIBE [channel [channel \u0026hellip;]]：指退订给定的频道 redis事务 Redis 事务可以一次执行多个命令， 并且带有以下三个重要的保证：\n批量操作在发送 EXEC 命令前被放入队列缓存 收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行 在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。 一个事务从开始到执行会经历以下三个阶段\n开始事务 命令入队 执行事务 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。 事务可以理解为一个打包的批量执行脚本，但批量指令并非原子化的操作，中间某条指令的失败不会导致前面已做指令的回滚，也不会造成后续的指令不做。\n命令 DISCARD:取消事务，放弃执行事务块内的所有命令 EXEC:执行所有事务块内的命令。 MULTI:标记一个事务块的开始 UNWATCH:取消 WATCH 命令对所有 key 的监视。 WATCH key [key \u0026hellip;]:监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断 redis脚本 redis连接 Redis 连接命令主要是用于连接 redis 服务。\n实例 以下实例演示了客户端如何通过密码验证连接到 redis 服务，并检测服务是否在运行：\nredis 127.0.0.1:6379\u0026gt; AUTH \u0026ldquo;password\u0026rdquo; OK redis 127.0.0.1:6379\u0026gt; PING PONG\n命令 AUTH password：验证密码是否正确 ECHO message：打印字符串 PING：查看服务是否运行 QUIT：关闭当前连接 SELECT index：切换到指定的数据库 redis服务器 Redis 服务器命令主要是用于管理 redis 服务。\nredis GEO reids stream Redis Stream 是 Redis 5.0 版本新增加的数据结构。 Redis Stream 主要用于消息队列（MQ，Message Queue），Redis 本身是有一个 Redis 发布订阅 (pub/sub) 来实现消息队列的功能，但它有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。 简单来说发布订阅 (pub/sub) 可以分发消息，但无法记录历史消息。 而 Redis Stream 提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失 Redis Stream 的结构如下所示，它有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容\n每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。\nredis客户端连接 Redis 通过监听一个 TCP 端口或者 Unix socket 的方式来接收来自客户端的连接，当一个连接建立后，Redis 内部会进行以下一些操作：\n首先，客户端 socket 会被设置为非阻塞模式，因为 Redis 在网络事件处理上采用的是非阻塞多路复用模型。 然后为这个 socket 设置 TCP_NODELAY 属性，禁用 Nagle 算法 然后创建一个可读的文件事件用于监听这个客户端 socket 的数据发送 最大连接数、 在 Redis2.4 中，最大连接数是被直接硬编码在代码里面的，而在2.6版本中这个值变成可配置的。 maxclients 的默认值是 10000，你也可以在 redis.conf 中对这个值进行修改。\n命令 CLIENT LIST：返回连接到 redis 服务的客户端列表 CLIENT SETNAME：设置当前连接的名称 CLIENT GETNAME：获取通过 CLIENT SETNAME 命令设置的服务名称 CLIENT PAUSE：挂起客户端连接，指定挂起的时间以毫秒计 CLIENT KILL：关闭客户端连接 ","date":"2024-06-20T00:00:00Z","image":"https://a-b-ab.github.io/p/redis%E7%AC%94%E8%AE%B0/e0f12bb2ec86a77c779b904b79e29ca5_huc4709ea6de29232b489089a69fca4caa_1596933_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/redis%E7%AC%94%E8%AE%B0/","title":"Redis笔记"},{"content":"调试 VS Code 的内置调试器有助于加速编辑、编译和调试循环。\n调试器扩展 VS Code 具有对 Node.js 运行时的内置调试支持，可以调试 JavaScript、TypeScript 或转译为 JavaScript 的任何其他语言。\n若要调试其他语言和运行时（包括 PHP、Ruby、Go、C#、Python、C++、PowerShell 等），请在 VS Code Marketplace 中查找扩展，或在顶级“运行”菜单中选择“安装其他调试器”。\nLaunch.json属性 在为属性指定值后，可以使用 IntelliSense （Ctrl+Space） 查看可用属性的列表。launch.jsontype\n以下属性对于每个启动配置都是必需的：\ntype: 用于此启动配置的调试器类型。每个已安装的调试扩展都引入了一种类型：例如，用于内置 Node 调试器，或者用于 PHP 和 Go 扩展。 request: 此启动配置的请求类型。目前，并受支持。launch attach name:\u0026ldquo;Python: Current File\u0026rdquo;: 这个字段指定了配置的名称，它将在 VS Code 的调试启动配置下拉菜单中显示。这里的名称是 \u0026ldquo;Python: Current File\u0026rdquo;，意味着这个配置是用来调试当前打开的 Python 文件 ","date":"2024-05-26T00:00:00Z","image":"https://a-b-ab.github.io/p/vscode%E7%AE%80%E5%8D%95%E8%B0%83%E8%AF%95/f5a3e854330c8e49bae0774f3de85ba3_hu8bb523bd56250c856ec56200cac9d5d5_2634381_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/vscode%E7%AE%80%E5%8D%95%E8%B0%83%E8%AF%95/","title":"Vscode简单调试"},{"content":"介绍 YAML 是 \u0026ldquo;YAML Ain\u0026rsquo;t a Markup Language\u0026rdquo;（YAML 不是一种标记语言）的递归缩写。在开发的这种语言时，YAML 的意思其实是：\u0026ldquo;Yet Another Markup Language\u0026rdquo;（仍是一种标记语言）。\nYAML 的语法和其他高级语言类似，并且可以简单表达清单、散列表，标量等数据形态。它使用空白符号缩进和大量依赖外观的特色，特别适合用来表达或编辑数据结构、各种配置文件、倾印调试内容、文件大纲（例如：许多电子邮件标题格式和YAML非常接近）。\nYAML 的配置文件后缀为 .yml，如：runoob.yml 。\n基本语法 大小写敏感 使用缩进表示层级关系 缩进不允许使用tab，只允许空格 缩进的空格数不重要，只要相同层级的元素左对齐即可 #表示注释 数据类型 对象：键值对的机会，又称为映射/哈希/字典 数组：一组按次序排列的值，又称为序列/表 纯量：单个的，不可再分的值 YAML对象 对象键值对使用冒号结构表示key：value，冒号后面再加一个空格 也可以使用key：{key1：value1,key2：value2,\u0026hellip;} 还可以用缩进表示层级关系\nkey: child-key: value child-key2: value2\n较为复杂的对象格式，可以使用问号加一个空格代表一个复杂的 key，配合一个冒号加一个空格代表一个 value\n?\n- complexkey1 - complexkey2 : - complexvalue1 - complexvalue2\n意思即对象的属性是一个数组[complexkey1,complexkey2],对应的值也是一个数组 [complexvalue1,complexvalue2]\nYAML数组 以 - 开头的行表示构成一个属组\nA B C 数据结构的子成员是一个数据，则可以在该项下面缩进一个空格 A B C 一个相对复杂的例子： companies: - id: 1 name: company1 price: 200W - id: 2 name: company2 price: 500W\n意思是 companies 属性是一个数组，每一个数组元素又是由 id、name、price 三个属性构成。\n数组也可以使用流式(flow)的方式表示：\ncompanies: [{id: 1,name: company1,price: 200W},{id: 2,name: company2,price: 500W}]\r复合结构 数组和对象可以构成复合结构 languages:\nRuby Perl Python websites: YAML: yaml.org Ruby: ruby-lang.org Python: python.org Perl: use.perl.org 转换为json为： { languages: [ \u0026lsquo;Ruby\u0026rsquo;, \u0026lsquo;Perl\u0026rsquo;, \u0026lsquo;Python\u0026rsquo;], websites: { YAML: \u0026lsquo;yaml.org\u0026rsquo;, Ruby: \u0026lsquo;ruby-lang.org\u0026rsquo;, Python: \u0026lsquo;python.org\u0026rsquo;, Perl: \u0026lsquo;use.perl.org\u0026rsquo; } }\n纯量 纯量是最基本的，不可再分的值，包括\n字符串 布尔值 整数 浮点数 Null 时间 日期 使用一个例子来快速了解纯量的基本使用： boolean: - TRUE #true,True都可以 - FALSE #false，False都可以 float: - 3.14 - 6.8523015e+5 #可以使用科学计数法 int: - 123 - 0b1010_0111_0100_1010_1110 #二进制表示 null: nodeName: \u0026rsquo;node\u0026rsquo; parent: ~ #使用~表示null string: - 哈哈 - \u0026lsquo;Hello world\u0026rsquo; #可以使用双引号或者单引号包裹特殊字符 - newline newline2 #字符串可以拆成多行，每一行会被转化成一个空格 date: - 2018-02-17 #日期必须使用ISO 8601格式，即yyyy-MM-dd datetime: - 2018-02-17T15:02:31+08:00 #时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区\n引用 \u0026amp; 锚点和 * 别名，可以用来引用 defaults: \u0026amp;defaults adapter: postgres host: localhost\ndevelopment: database: myapp_development \u0026laquo;: *defaults\ntest: database: myapp_test \u0026laquo;: *defaults\n相当于： defaults: adapter: postgres host: localhost\ndevelopment: database: myapp_development adapter: postgres host: localhost\ntest: database: myapp_test adapter: postgres host: localhost\n\u0026amp; 用来建立锚点，\u0026laquo; 表示合并到当前数据，*用来引用锚点\n下面是另一个例子:\n\u0026amp;showell Steve Clark Brian Oren *showell 转为 JavaScript 代码如下: [ \u0026lsquo;Steve\u0026rsquo;, \u0026lsquo;Clark\u0026rsquo;, \u0026lsquo;Brian\u0026rsquo;, \u0026lsquo;Oren\u0026rsquo;, \u0026lsquo;Steve\u0026rsquo; ]\n","date":"2024-05-26T00:00:00Z","image":"https://a-b-ab.github.io/p/yaml%E7%AC%94%E8%AE%B0/8d65db2f2003a7a63c85ae9e4edb4f2d_huef99a8c02e9a198b17e0ba13beef0c8d_3590075_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/yaml%E7%AC%94%E8%AE%B0/","title":"Yaml笔记"},{"content":"概述 SQLAlchemy SQL 工具包和对象关系映射器是一套用于处理数据库和 Python 的综合工具\nSQLAlchemy 最重要的两个面向用户的部分是对象关系映射器 (ORM)和Core。\nCore 包含了 SQLAlchemy 的 SQL 和数据库集成以及描述服务的广度，其中最突出的是SQL 表达式语言。\nSQL 表达式语言是一个独立于 ORM 包的工具包，它提供了一个由可组合对象表示的 SQL 表达式构建系统，然后可以在特定事务的范围内针对目标数据库“执行”这些表达式，返回结果集。通过传递表示这些语句的 SQL 表达式对象以及表示要与每个语句一起使用的参数的字典，可以实现插入、更新和删除（即 DML）。\n统一教程 SQLAlchemy 被呈现为两个不同的 API，一个建立在另一个之上。这些 API 被称为Core 和ORM。\nSQLAlchemy Core 是 SQLAlchemy 作为“数据库工具包”的基础架构。该库提供了管理数据库连接、与数据库查询和结果交互以及以编程方式构建 SQL 语句的工具。\n主要仅限 Core 的部分不会引用 ORM。这些部分中使用的 SQLAlchemy 结构将从 sqlalchemy 命名空间导入。作为主题分类的额外指示，它们还将在右侧包含一个深蓝色边框。在使用 ORM 时，这些概念仍然在起作用，但在用户代码中不那么显式。ORM 用户应该阅读这些部分，但不要期望直接使用这些 API 来编写以 ORM 为中心的代码。\nSQLAlchemy ORM 建立在 Core 之上，提供可选的对象关系映射功能。ORM 提供了一个额外的配置层，允许用户定义的 Python 类被映射到数据库表和其他结构，以及一种称为会话的对象持久化机制。然后，它扩展了 Core 级别的 SQL 表达式语言，允许以用户定义的对象来编写和调用 SQL 查询。\n建立连接 - 引擎 任何 SQLAlchemy 应用程序的开始都是一个称为 Engine 的对象。此对象充当连接到特定数据库的中心源，既提供一个工厂，又提供一个称为 连接池 的存储空间，用于这些数据库连接。引擎通常是仅为特定数据库服务器创建一次的全局对象，并且使用 URL 字符串进行配置，该 URL 字符串将描述它如何连接到数据库主机或后端\n1 2 from sqlalchemy import create_engine engine = create_engine(\u0026#34;sqlite+pysqlite:///:memory:\u0026#34;, echo=True) 使用事务和DBAPI 获取连接 从用户角度来看，Engine 对象的唯一目的是提供一个连接到数据库的连接单元，称为 Connection。在直接使用 Core 时，Connection 对象是所有与数据库交互的方式。由于 Connection 代表一个针对数据库的开放资源，我们希望始终将对该对象的使用的范围限制在特定上下文中，而最好的方法是使用 Python 上下文管理器形式，也称为 with 语句\n1 2 3 4 5 from sqlalchemy import text with engine.connect() as conn: result = conn.execute(text(\u0026#34;select \u0026#39;hello world\u0026#39;\u0026#34;)) print(result.all()) 提交更改 我们发出了两个通常是事务性的 SQL 语句，一个“CREATE TABLE”语句 [1] 和一个参数化的“INSERT”语句（上面的参数化语法将在下面几节中讨论，在 发送多个参数 中）。由于我们希望在块内提交所做的工作，因此我们调用 Connection.commit() 方法来提交事务。在我们在块内调用此方法后，我们可以继续运行更多 SQL 语句，如果我们选择，我们可以再次调用 Connection.commit() 来处理后续语句。SQLAlchemy 将这种风格称为边走边提交。\n还有一种提交数据的方式，即我们可以预先将“连接”块声明为事务块。对于这种操作模式，我们使用 Engine.begin() 方法获取连接，而不是使用 Engine.connect() 方法。此方法将同时管理 Connection 的范围，并将所有内容包含在一个事务中，并在成功执行块后执行 COMMIT，或在出现异常时执行 ROLLBACK。这种方式被称为 一次开始\n使用数据库元数据 在完成引擎和 SQL 执行后，我们准备开始使用 SQLAlchemy。SQLAlchemy Core 和 ORM 的核心元素是 SQL 表达式语言，它允许以流畅、可组合的方式构建 SQL 查询。这些查询的基础是表示数据库概念（如表和列）的 Python 对象。这些对象统称为 数据库元数据。\nSQLAlchemy 中最常见的数据库元数据基础对象是 MetaData、Table 和 Column\n使用Table对象设置MetaData 当我们使用关系型数据库时，数据库中用于存储数据的基本结构称为表。在 SQLAlchemy 中，数据库“表”最终由一个名为 Table 的 Python 对象表示。\n要开始使用 SQLAlchemy 表达式语言，我们需要构建表示我们想要操作的所有数据库表的 Table 对象。Table 是以编程方式构建的，可以通过直接使用 Table 构造函数，也可以通过使用 ORM Mapped 类（稍后在 使用 ORM 声明式表单定义表元数据 中描述）间接构建。还可以选择从现有数据库加载部分或全部表信息，称为 反射。\n无论使用哪种方法，我们总是从一个集合开始，这个集合将是我们放置称为 MetaData 对象的表的集合。这个对象本质上是一个围绕 Python 字典的 facade，它存储一系列以字符串名称为键的 Table 对象。虽然 ORM 提供了一些关于从哪里获取这个集合的选项，但我们始终可以选择直接创建一个，它看起来像\n1 2 from sqlalchemy import MetaData metadata_obj = MetaData() 一旦我们有了 MetaData 对象，我们就可以声明一些 Table 对象\n对于整个应用程序使用单个 MetaData 对象是最常见的情况，它被表示为应用程序中某个位置的模块级变量，通常位于“models”或“dbschema”类型的包中。 MetaData 通常通过以 ORM 为中心的 registry 或 Declarative Base 基类访问，因此同一个 MetaData 在 ORM 和 Core 声明的 Table 对象之间共享。\n声明简单约束 将DDL发射到数据库 我们构建了一个对象结构，它表示数据库中的两个数据库表，从根 MetaData 对象开始，然后进入两个 Table 对象，每个对象都包含一个 Column 和 Constraint 对象的集合。这个对象结构将成为我们今后使用 Core 和 ORM 执行的大多数操作的核心。\n使用ORM声明式表单定义表元数据 Table 对象，它是在构建 SQL 表达式时 SQLAlchemy 最终引用数据库表的底层机制。如前所述，SQLAlchemy ORM 为 Table 声明过程提供了一个外观，称为 声明式表。声明式表过程实现了与上一节中相同的目标，即构建 Table 对象，但在这个过程中，它还为我们提供了其他东西，称为 ORM 映射类，或简称为“映射类”。映射类是使用 ORM 时 SQL 最常见的基石，在现代 SQLAlchemy 中，它也可以非常有效地与以 Core 为中心的用法结合使用\n使用 ORM 时，声明 Table 元数据的过程通常与声明 映射 类相结合。映射类是我们想要创建的任何 Python 类，它将具有与数据库表中的列关联的属性。虽然实现这一点的方式有很多种，但最常见的风格被称为 声明式，它允许我们同时声明用户定义的类和 Table 元数据。\n建立声明式基类 当使用 ORM 时，MetaData 集合仍然存在，但它本身与一个仅限 ORM 的结构相关联，通常被称为 声明式基类。获取新的声明式基类的最便捷方式是创建一个新的类，该类是 SQLAlchemy DeclarativeBase 类的子类。\n1 2 3 from sqlalchemy.orm import DeclarativeBase class Base(DeclarativeBase): pass 上面，Base 类是我们所说的声明式基类。当我们创建新的类作为 Base 的子类时，结合适当的类级别指令，它们将在类创建时被建立为新的 ORM 映射类，每个类通常（但并非总是）引用一个特定的 Table 对象。\n声明式基类引用一个 MetaData 集合，该集合会自动为我们创建，假设我们没有从外部提供。这个 MetaData 集合可以通过 DeclarativeBase.metadata 类级别属性访问。当我们创建新的映射类时，它们将分别引用此 MetaData 集合中的一个 Table。\n声明式基类也指代一个名为 registry 的集合，它是 SQLAlchemy ORM 中的中心“映射配置”单元。虽然很少直接访问，但此对象是映射配置过程的核心，因为一组 ORM 映射类将通过此注册表相互协调。与 MetaData 一样，我们的声明式基类也为我们创建了一个 registry（同样可以传递我们自己的 registry），我们可以通过 DeclarativeBase.registry 类变量访问它。\n声明映射类 在建立了 Base 类之后，我们现在可以根据新的类 User 和 Address，为 user_account 和 address 表定义 ORM 映射类。我们将在下面说明最现代的声明式形式，它由 PEP 484 类型注解驱动，使用特殊类型 Mapped，它指示将属性映射为特定类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from typing import List from typing import Optional from sqlalchemy.orm import Mapped from sqlalchemy.orm import mapped_column from sqlalchemy.orm import relationship class User(Base): __tablename__ = \u0026#34;user_account\u0026#34; id: Mapped[int] = mapped_column(primary_key=True) name: Mapped[str] = mapped_column(String(30)) fullname: Mapped[Optional[str]] addresses: Mapped[List[\u0026#34;Address\u0026#34;]] = relationship(back_populates=\u0026#34;user\u0026#34;) def __repr__(self) -\u0026gt; str: return f\u0026#34;User(id={self.id!r}, name={self.name!r}, fullname={self.fullname!r})\u0026#34; class Address(Base): __tablename__ = \u0026#34;address\u0026#34; id: Mapped[int] = mapped_column(primary_key=True) email_address: Mapped[str] user_id = mapped_column(ForeignKey(\u0026#34;user_account.id\u0026#34;)) user: Mapped[User] = relationship(back_populates=\u0026#34;addresses\u0026#34;) def __repr__(self) -\u0026gt; str: return f\u0026#34;Address(id={self.id!r}, email_address={self.email_address!r})\u0026#34; 每个类都引用一个 Table 对象，该对象是在声明式映射过程中生成的，其名称是通过将字符串分配给 DeclarativeBase.tablename 属性来确定的。创建类后，此生成的 Table 可从 DeclarativeBase.table 属性获得。\n如前所述，这种形式被称为 声明式表配置。几种替代声明风格中的一种将让我们直接构建 Table 对象，并将其 分配 给 DeclarativeBase.table。这种风格被称为 带有命令式表的声明式。\n为了在 Table 中指示列，我们使用 mapped_column() 结构，并结合基于 Mapped 类型的类型注解。此对象将生成 Column 对象，这些对象将应用于 Table 的构建。\n对于具有简单数据类型且没有其他选项的列，我们可以仅指示 Mapped 类型注解，使用简单的 Python 类型，如 int 和 str 来表示 Integer 和 String。在声明式映射过程中如何解释 Python 类型是开放式的；有关背景信息，请参阅部分 使用带注解的声明式表（mapped_column() 的类型注解形式） 和 自定义类型映射。\n根据是否存在 Optional[] 类型注解（或其等效形式， | None 或 Union[, None]），可以将列声明为“可为空”或“不可为空”。mapped_column.nullable 参数也可以显式使用（并且不必与注解的可选性匹配）。\n使用显式类型注解是完全可选的。我们也可以使用 mapped_column() 而不使用注解。使用这种形式时，我们将使用更明确的类型对象，例如 Integer 和 String，以及根据需要在每个 mapped_column() 结构中使用 nullable=False。\n另外两个属性，User.addresses 和 Address.user，定义了一种不同类型的属性，称为 relationship()，它具有与所示类似的注解感知配置样式。relationship() 结构将在 使用 ORM 相关对象 中更详细地讨论。\n如果我们没有声明自己的 init() 方法，这些类将自动获得一个 init() 方法。此方法的默认形式接受所有属性名称作为可选关键字参数。\nsandy = User(name=\u0026ldquo;sandy\u0026rdquo;, fullname=\u0026ldquo;Sandy Cheeks\u0026rdquo;) 为了自动生成一个功能齐全的 init() 方法，该方法提供位置参数以及具有默认关键字值的参数，可以在 声明式数据类映射 中介绍的数据类功能中使用。当然，始终可以选择使用显式的 init() 方法。\n添加了 repr() 方法，以便我们获得可读的字符串输出；这些方法不需要在这里。与 init() 一样，可以使用 数据类 功能自动生成 repr() 方法。\n表反射 表反射是指通过读取数据库的当前状态来生成Table和相关对象的过程\n作为反射的示例，我们将创建一个新的Table对象，它代表我们在本文档前面部分手动创建的some_table对象。执行此操作的方式有很多种，但最基本的方式是构造一个Table对象，给出表的名称和它将所属的MetaData集合，然后不是指定单个Column和Constraint对象，而是使用Engine传递目标Table.autoload_with参数。\n在整个过程中，some_table对象现在包含有关表中存在的Column对象的信息，并且该对象的使用方式与我们显式声明的Table完全相同。\n使用数据 使用insert 使用select 使用update 使用delete ","date":"2024-05-05T00:00:00Z","image":"https://a-b-ab.github.io/p/sqlalchemy%E7%AC%94%E8%AE%B0/bebc542adb28a4465a71050225954ca26cb14e96_hu6113bd237f7c83c4f01a2bc42a08bf1d_4278078_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/sqlalchemy%E7%AC%94%E8%AE%B0/","title":"SQLAlchemy笔记"},{"content":"快速开始 基于Ruia快速实现一个以Hacker News为目标的爬虫 本文主要通过对hacker News的爬取示例来展示如何使用ruia。\n第一步：定义item item的目的是定义目标网站你需要爬取的数据，此时，爬虫的目标数据就是页面中的title和url，怎么提取数据，ruia的field类提供了以下三种方式提取目标数据\nxpath re css selector 本教程爬虫例子都默认使用css selector的规则来提取目标数据\n规则确定后，就可以用 item来实现一个针对目标数据的 orm，创建文件 items.py\n1 2 3 4 5 6 from ruia import AttrField,TextField,Item class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;,attr=\u0026#39;href\u0026#39;) 这段代码含义是：针对我们提取的目标html，我们定义了一个HackerNewsItem类，其包含了两个field\ntitle：直接从文本提取 url：从属性提取 而 target_item是什么。对于一个 Item类来说，当其定义好网页目标数据后，ruia提供两种方式进行获取 Item\nget_item：获取网页的单目标，比如目标网页的标题，此时无需定义target_item get_items：获取网页的多目标，比如当前目标网页Hacker News中的title和url一共有30个，这时就必须定义target_item来寻找多个目标块；target_item的作用就是针对这样的工作而诞生的，开发者只要定义好这个属性（此时Ruia会自动获取网页中30个target_item），然后每个target_item里面包含的title和url就会被提取出来 第二步：测试Item Ruia为了方便扩展以及自由地组合使用，本身各个模块之间耦合度是极低的，每个模块都可以在你的项目中单独使用；你甚至只使用ruia.Item、Ruia.TextField和ruia.AttrField来编写一个简单的爬虫\n第三步：编写Spider Ruia.Spider是Ruia框架里面的核心控制类\n控制目标网页的请求 Ruia.Request和 Ruia.Response 可加载自定义钩子,插件以及相关配置等,让开发效率更高 开发者实现 HackerNewsSpider 必须是 Spider的子类，代码出现的两个方法 Spider内置：\nparse：此方法是Spider的入口，每一个start_urls的响应必然会被parse方法捕捉并执行； process_item:此方法作用是抽离出对Item提取结果的处理过程，比如这里会接受自定义Item类作为输入，然后进行处理持久化到文件。 第四步：运行Start 如果你想在异步函数里面调用，执行**await HackerNewsSpider.start()**即可\n第五步：扩展 Middleware的目的是对每次请求前后进行一番处理 Ruia已经专门编写了一个名为 ruia-ua的插件来为开发者提升效率。示例代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from ruia import AttrField, TextField, Item, Spider from ruia_ua import middleware class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;, attr=\u0026#39;href\u0026#39;) class HackerNewsSpider(Spider): start_urls = [\u0026#39;https://news.ycombinator.com/news?p=1\u0026#39;, \u0026#39;https://news.ycombinator.com/news?p=2\u0026#39;] async def parse(self, response): # Do something... print(response.url) if __name__ == \u0026#39;__main__\u0026#39;: HackerNewsSpider.start(middleware=middleware) MongoDB 数据持久化，如果想将数据持久化到数据库（MongoDB）中，该怎么做？此时就到了凸显Ruia插件优势的时候了，你只需要安装 ruia-motor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from ruia_motor import RuiaMotorInsert, init_spider from ruia import AttrField, Item, Spider, TextField class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#34;tr.athing\u0026#34;) title = TextField(css_select=\u0026#34;a.storylink\u0026#34;) url = AttrField(css_select=\u0026#34;a.storylink\u0026#34;, attr=\u0026#34;href\u0026#34;) class HackerNewsSpider(Spider): start_urls = [f\u0026#34;https://news.ycombinator.com/news?p={index}\u0026#34; for index in range(3)] concurrency = 3 # aiohttp_kwargs = {\u0026#34;proxy\u0026#34;: \u0026#34;http://0.0.0.0:1087\u0026#34;} async def parse(self, response): async for item in HackerNewsItem.get_items(html=await response.text()): yield RuiaMotorInsert(collection=\u0026#34;news\u0026#34;, data=item.results) async def init_plugins_after_start(spider_ins): spider_ins.mongodb_config = {\u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 27017, \u0026#34;db\u0026#34;: \u0026#34;ruia_motor\u0026#34;} init_spider(spider_ins=spider_ins) if __name__ == \u0026#34;__main__\u0026#34;: HackerNewsSpider.start(after_start=init_plugins_after_start) 入门指南 概览 Ruia是一个基于 asyncio和 aiohttp的异步爬虫框架，理念：\n更少的代码：能通用的功能就插件化，让开发者直接引用即可 更快的速度：由异步驱动 安装 定义Item 运行 Spider 基础概念 Request Request的主要作用是方便处理网络请求，最终返回一个 Request对象\n主要提供的方法有\nRequest().fetch:请求一个网页资源，可以单独使用 Request().fetch_callback:为 Spider类提供的核心方法 核心参数 url:请求的资源连接 method:请求的方法 callback:回调函数 headers:请求头 load_js:目标网页是否需要加载js metadata:跨请求传递的一些数据 request_config:请求配置 request_session: aiohttp的请求 session aiohttp-kwargs:请求目标资源可定义的其他参数 Request通过对 aiohttp和 pyppeteer的封装来实现对网页资源的异步请求\nResponse Respoonse的目的是返回一个统一切友好的响应对象\nurl:请求的资源连接 metadata：跨请求传递的一些数据 html：源网站返回的资源数据 cookies：网站cookie history：访问历史 headers：请求头 status：请求状态码 Item Item的主要作用是定义以及通过一定的规则提取网页中的目标数据，它主要提供了\nget_item:针对页面单目标数据进行提取 get_items:针对页面多目标数据进行提取 Core arguments html:网页源码 url：网页连接 html_etree：etree._Element对象 不论是源网站链接或者网站HTML源码，甚至是经过lxml处理过的etree._Element对象，Item能接收这三种类型的输入并进行处理\n有时你会遇见这样一种情况，例如爬取Github的Issue时，你会发现一个Issue可能对应多个Tag。 这时，将Tag作为一个独立的Item来提取是不划算的， 我们可以使用Field字段的many=True参数，使这个字段返回一个列表。\n最终 Item类会将输入最终转化为etree._Element对象进行处理，然后利用元类的思想将每一个 Field构造的属性计算为原网页上对应的真实数据\nSelector Selector通过Field类实现，为开发者提供了CSS Selector和XPath两种方式提取目标数据，具体由下面两个类实现：\nAttrField(BaseField):提取网页标签的属性数据 TextField(BaseField):提取网页标签的text属性 核心参数 所有 Field共有的参数\ndefault：str；设置默认值，建议定义，否则找不到字段时会报错 many：bool，返回值将是一个列表 AttrField TextField HtmlField共有参数\ncss_select:str，利用 CSS Select提取目标数据 xpath_select：str，利用 XPath提取目标数据 AttrField需要一个额外的参数\nattr：目标标签属性 RegexField需要一个额外的参数\nre_select:str,正则表达式字符串 定好CSS Selector或XPath规则，然后利用lxml实现对目标html进行目标数据的提取\nSpider Spider是爬虫程序的入口，它将Item,Middleware,Request,等模块组合在一起，从而为你构造一个稳健的爬虫程序。你只需要关注以下两个函数：\nSpider.start：爬虫的启动函数 parse：爬虫的第一层解析函数，继承 Spider的子类必须实现这个函数 核心参数： Spider.start的参数\nafter_start:爬虫启动后的钩子函数 before_stop:爬虫启动前的钩子函数 middleware:中间件类，可以是一个中间件 **Middleware()**实例，也可以是一组 **Middleware()**实例组成的列表 loop:事件循环 Spider会自动读取start_urls列表里面的请求链接，然后维护一个异步队列，使用生产消费者模式进行爬取，爬虫程序一直循环直到没有调用函数为止\nMiddleware Middleware的主要作用是在进行一个请求的前后进行一些处理，比如监听请求或者响应：\nMiddleware().request:在请求前处理一些事情 Middleware().response:在请求后处理一些事情 使用中间件有两点需要注意，一个是处理函数需要带上特定的参数，第二个是不需要返回值，具体使用如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from ruia import Middleware middleware = Middleware() @middleware.request async def print_on_request(spider_ins, request): \u0026#34;\u0026#34;\u0026#34; 每次请求前都会调用此函数 request: Request类的实例对象 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;request: print when a request is received\u0026#34;) @middleware.response async def print_on_response(spider_ins, request, response): \u0026#34;\u0026#34;\u0026#34; 每次请求后都会调用此函数 request: Request类的实例对象 response: Response类的实例对象 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;response: print when a response is received\u0026#34;) Middleware通过装饰器来实现对函数的回调，从而让开发者可以优雅的实现中间件功能，Middleware类中的两个属性request_middleware和response_middleware分别维护着一个队列来处理开发者定义的处理函数\n","date":"2024-04-30T00:00:00Z","image":"https://a-b-ab.github.io/p/ruia%E7%AC%94%E8%AE%B0/Picgo202403181915672_hu489bb48f574fc8adb34c90b5762fb388_69560_120x120_fill_q75_box_smart1.jpeg","permalink":"https://a-b-ab.github.io/p/ruia%E7%AC%94%E8%AE%B0/","title":"Ruia笔记"},{"content":"Nginx Nginx（发音为“engine X”）是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP代理服务器。它因其稳定性、丰富的功能集、简单的配置以及低资源消耗而广受欢迎。Nginx 的主要作用包括但不限于以下几个方面：\nWeb服务器：Nginx可以作为静态网页的服务器，直接处理并返回用户请求的静态文件（如HTML、CSS、JavaScript、图片等）。由于Nginx对静态文件的处理非常高效，因此它非常适合作为静态资源的服务器。\n反向代理服务器：Nginx最常用的功能之一是作为反向代理服务器。反向代理服务器接收来自客户端的请求，然后将这些请求转发给后端服务器（如Apache、Tomcat等），并将从后端服务器得到的响应返回给客户端。这种方式可以隐藏后端服务器的IP地址，增强安全性，同时实现负载均衡，将请求分发到多个后端服务器上，提高网站的并发处理能力。\n负载均衡器：Nginx支持多种负载均衡算法（如轮询、最少连接、IP哈希等），可以根据后端服务器的负载情况、响应时间等因素，智能地将请求分发到不同的服务器上，以实现资源的合理利用和负载均衡。\nHTTP缓存：Nginx支持HTTP缓存，可以缓存静态内容，减少服务器的请求处理次数和响应时间，提高网站的访问速度。\n支持SSL/TLS协议：Nginx支持SSL/TLS协议，可以提供安全的HTTPS服务，保护用户数据在传输过程中的安全。\n模块化设计：Nginx采用模块化设计，具有高度的可扩展性。用户可以根据需要安装不同的模块，以实现特定的功能，如HTTP/2支持、WebSocket支持、邮件代理服务等。\n高并发处理能力：Nginx采用异步非阻塞的事件驱动模型，能够处理大量的并发连接，非常适合作为高并发网站的服务器。\n资源消耗低：Nginx在资源消耗方面表现优秀，即使在处理大量并发连接时，也能保持较低的CPU和内存使用率。\n综上所述，Nginx因其高性能、稳定性、丰富的功能集和易用性，在Web服务器、反向代理、负载均衡、HTTP缓存等方面发挥着重要作用，是现代互联网架构中不可或缺的一部分。\nnginx 有一个主进程（Master）和几个工作进程（Worker）。主进程的主要目的是读取和评估配置，并维护工作进程。工作进程对请求进行处理。nginx 采用了基于事件模型和依赖于操作系统的机制来有效地在工作进程之间分配请求。工作进程的数量可在配置文件中定义，并且可以针对给定的配置进行修改，或者自动调整到可用 CPU 内核的数量\n配置文件决定了 nginx 及其模块的工作方式。默认情况下，配置文件名为 nginx.conf，并放在目录 /usr/local/nginx/conf，/etc/nginx 或 /usr/local/etc/nginx 中。\n启动，停止和重新加载配置 要启动 nginx，需要运行可执行文件。nginx 启动之后，可以通过调用可执行文件附带 -s 参数 来控制它。 使用以下语法：\nnginx -s 信号\n信号可能是以下之一：\nstop - 立即关闭\nquit - 正常关闭\nreload - 重新加载配置文件\nreopen - 重新打开日志文件\n例如，要等待工作进程处理完当前的请求才停止 nginx 进程，可以执行以下命令：\nnginx -s quit\n这个命令的执行用户应该是与启动nginx用户是一致的\n在将重新加载配置的命令发送到 nginx 或重新启动之前，配置文件所做的内容更改将不会生效。要重新加载配置，请执行：\nnginx -s reload\n一旦主进程（Master）收到要重新加载配置（reload）的信号，它将检查新配置文件的语法有效性，并尝试应用其中提供的配置。如果成功，主进程将启动新的工作进程（Worker），并向旧工作进程发送消息，请求它们关闭。否则，主进程回滚更改，并继续使用旧配置。旧工作进程接收到关闭命令后，停止接受新的请求连接，并继续维护当前请求，直到这些请求都被处理完成之后，旧工作进程将退出\n可以借助 Unix 工具（如 kill 工具）将信号发送到 nginx 进程，信号直接发送到指定进程 ID 的进程。默认情况下，nginx 主进程的进程 ID 是写入在 /usr/local/nginx/logs 或 /var/run 中的 nginx.pid 文件中。例如，如果主进程 ID 为 1628，则发送 QUIT 信号让 nginx 正常平滑关闭，可执行\nkill -s QUIT 1628\n获取所有正在运行的 nginx 进程列表，可以使用 ps 命令，如下\nps -ax | grep nginx\n配置文件结构 nginx 是由配置文件中指定的指令控制模块组成。指令可分为简单指令和块指令。一个简单的指令是由空格分隔的名称和参数组成，并以分号 ; 结尾。块指令具有与简单指令相同的结构，但不是以分号结尾，而是以大括号{}包围的一组附加指令结尾。如果块指令的大括号内部可以有其它指令，则称这个块指令为上下文（例如：events，http，server 和 location）。\n配置文件中被放置在任何上下文之外的指令都被认为是主上下文 main。events 和 http 指令在主 main 上下文中，server 在 http 中，location 又在 server 中。\n井号 # 之后的行的内容被视为注释。\n提供静态内容服务 Web 服务器的一个重要任务是提供文件（比如图片或者静态 HTML 页面）服务。您将实现一个示例，根据请求，将提供来自不同的本地目录的文件： /data/www（可能包含 HTML 文件）和 /data/images（包含图片）。这需要编辑配置文件，在 http 中配置一个包含两个 location 块的 server 块指令。\n首先，创建 /data/www 目录将包含任何文本内容的 index.html 文件放入其中，创建 /data/images 目录然后放一些图片进去。\n其次，打开这个配置文件， 默认配置文件已经包含几个服务器块示例，大部分是已经注释掉的。现在注释掉这些块并且启动一个新的 server 块。\nhttp {\rserver {\r}\r}\r通常，配置文件可以包含几个由监听 listen 端口和服务器域名 server names 区分的 server 块指令 distinguished。一旦 nginx 决定由哪个 server 来处理请求，它会根据 server 块中定义的 location 指令的参数来检验请求头中指定的URI\n添加如下 location 块指令到 server 块指令中\nlocation / {\rroot /data/www;\r}\r该 location 块指令指定 / 前缀与请求中的 URI 相比较。对于匹配的请求，URI 将被添加到根指令 root 中指定的路径，即 /data/ www，以形成本地文件系统上所请求文件的路径。如果有几个匹配上的 location 块指令，nginx 将选择具有最长前缀的 location 块。上面的位置块提供最短的前缀，长度为 1，因此只有当所有其它 location 块不能匹配时，才会使用该块。\n接下来，添加第二个 location 指令快：\nlocation /images/ {\rroot /data;\r}\r以 /images/ 为开头的请求将会被匹配上（虽然 location / 也能匹配上此请求，但是它的前缀更短）\n最后，server 块指令应如下所示：\nserver {\rlocation / {\rroot /data/www;\r}\rlocation /images/ {\rroot /data;\r}\r}\r这已经是一个监听标准 80 端口并且可以在本地机器上通过 http://localhost/ 地址来访问的有效配置。响应以 /images/ 开头的URI请求，服务器将从 /data/images 目录发送文件。例如，响应http://localhost/images/example.png 请求，nginx 将发送 /data/images/example.png 文件。如果此文件不存在，nginx 将发送一个404错误响应。不以 / images/ 开头的 URI 的请求将映射到 /data/www 目录。例如，响应 http://localhost/some/example.html 请求，nginx 将发送 /data/www/some/example.html 文件。\n如果运行的效果没有在预期之中，您可以尝试从 /usr/local/nginx/logs 或 /var/log/ nginx 中的 access.log 和 error.log 日志文件中查找原因。\n设置一个简单的代理服务器 nginx 的一个常见用途是作为一个代理服务器，作用是接收请求并转发给被代理的服务器，从中取得响应，并将其发送回客户端。\n我们将配置一个基本的代理服务器，它为图片请求提供的文件来自本地目录，并将所有其它请求发送给代理的服务器。在此示例中，两个服务器在单个 nginx 实例上定义。\n首先，通过向 nginx 的配置文件添加一个 server 块来定义代理服务器，其中包含以下内容\nserver {\rlisten 8080;\rroot /data/up1;\rlocation / {\r}\r}\r这是一个监听 8080 端口的简单服务器（以前，由于使用了标准 80 端口，所以没有指定 listen 指令），并将所有请求映射到本地文件系统上的 /data/up1 目录。创建此目录并将 index.html 文件放入其中。请注意，root 指令位于 server 上下文中。当选择用于处理请求的 location 块自身不包含 root 指令时，将使用此 root 指令。\n接下来，在上一节中的服务器配置基础上进行修改，使其成为代理服务器配置。在第一个 location 块中，使用参数指定的代理服务器的协议，域名和端口（在本例中为 http://localhost:8080）放置在 proxy_pass 指令处：\nserver {\rlocation / {\rproxy_pass http://localhost:8080;\r}\rlocation /images/ {\rroot /data;\r}\r}\r我们将修改使用了 /images/ 前缀将请求映射到 /data/images 目录下的文件的第二个 location 块，使其与附带常见的图片文件扩展名的请求相匹配。修改后的 location 块如下所示：\nlocation ~ \\.(gif|jpg|png)$ {\rroot /data/images;\r}\r该参数是一个正则表达式，匹配所有以.gif，.jpg 或 .png 结尾的 URI。正则表达式之前应该是 ~。相应的请求将映射到 /data/images 目录。\n当 nginx 选择一个 location 块来提供请求时，它首先检查指定前缀的 location 指令，记住具有最长前缀的 location，然后检查正则表达式。如果与正则表达式匹配，nginx 会选择此 location，否则选择更早之前记住的那一个。\n代理服务器的最终配置如下：\nserver { location / { proxy_pass http://localhost:8080/; }\nlocation ~ \\.(gif|jpg|png)$ {\rroot /data/images;\r}\r}\n此 server 将过滤以 .gif，.jpg 或 .png 结尾的请求，并将它们映射到 /data/images 目录（通过向 root 指令的参数添加 URI），并将所有其它请求传递到上面配置的代理服务器。\n设置FastCGI代理 nginx 可被用于将请求路由到运行了使用各种框架和 PHP 等编程语言构建的应用程序的 FastCGI 服务器。\n与 FastCGI 服务器协同工作的最基本的 nginx 配置是使用 fastcgi_pass 指令而不是 proxy_pass 指令，以及 fastcgi_param 指令来设置传递给 FastCGI 服务器的参数。假设 FastCGI 服务器可以在 localhost:9000 上访问。以上一节的代理配置为基础，用 fastcgi_pass 指令替换 proxy_pass 指令，并将参数更改为 localhost:9000。在 PHP 中，SCRIPT_FILENAME 参数用于确定脚本名称，QUERY_STRING 参数用于传递请求参数。最终的配置将是：\nserver { location / { fastcgi_pass localhost:9000; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param QUERY_STRING $query_string; }\nlocation ~ \\.(gif|jpg|png)$ {\rroot /data/images;\r}\r}\n这里设置一个 server，将除了静态图片请求之外的所有请求路由到通过 FastCGI 协议在 localhost:9000 上运行的代理服务器。\n","date":"2024-04-26T00:00:00Z","image":"https://a-b-ab.github.io/p/nginx%E7%AC%94%E8%AE%B0/f48d72dbe89cb374fafa2450e23b0dc409e89c86_hue2c1746dcbde90ad93215d14797d57fb_786639_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/nginx%E7%AC%94%E8%AE%B0/","title":"Nginx笔记"},{"content":"Python模块和包管理 模块和包 在 Python 中每一个 .py 都可以视作一个模块（module），而每一个包含 init.py 的目录则可以视作包（packge）。\n如下所示，packs 因为包含 init.py 而可以看做是一个包，而 packs/one.py 则是一个模块\n├── main.py\r├── packs\r│ ├── __init__.py\r│ └── one.py\r└── readme.md\r在 one.py 中简单定义了一个函数 One：\ndef One():\rprint(\u0026quot;module one\u0026quot;)\rdef OneOne():\rprint(\u0026quot;module one/one\u0026quot;)\r如果我们想要在main.py中使用的话，那么可以直接import，这种方式也称之为“模块导入”\nimport packs\rpacks.one.One()\r这里使用的同时加上了模块名称，当然可以省略模块名，而这种方式称之为“成员导入”，这种方式比之前一种方式要快一些\nfrom packs.one import One\rOne()\r当我们运行后就会打印 module one，另外也可以看到在 packs 目录下生成一个 pycache 的新目录，这是编译后中间文件，可以提升模块载入速度。\n如果当前模块有多个 One 名称导入的话，可以使用别名进行区分。如果有多个导入名称的话可以使用逗号进行分隔。\nfrom packs.one import One as Alias, OneOne\rAlias()\rOneOne()\r如果要在一行导入的名称过多，也可以分行写\nfrom packs.one import One\rfrom packs.one import OneOne\rOne()\rOneOne()\r当然也可以全部导入到当前模块，不过注意这种方式可能存在命名冲突，并且在一些 linter 工具下会提示必须使用全部的导入名称。\nfrom packs.one import *\rOne()\rOneOne()\r除了这种全局导入外，还可以在局部作用域导入。\ndef main():\rfrom packs.one import One\rOne()\rmain()\rinit文件 这里的__init__.py文件每当导入当前包的模块的时候就会运行一次\n现在在 packs/init.py 中加入一行 print(\u0026ldquo;packs one imported\u0026rdquo;) 的语句，然后运行，可以发现 init.py 是首先运行的\n根据这个特点，我们可以再 init.py 输入导出的模块，外部使用的就不需要很长的导入路径了。\n修改 packs/__init__py 为如下所示：\nfrom .one import One\rprint(\u0026quot;packs one imported\u0026quot;)\r那么在 main.py 中就可以这样使用：\nfrom packs import One\rOne()\r或者\nimport packs\rpacks.One()\r在深入一些，我们在 packs 文件夹下新建一个 two 包，然后修改 main.py 并导入这个包，类如下面\n$ tree .\r.\r├── main.py\r├── packs\r│ ├── __init__.py\r│ ├── one.py\r│ └── two\r│ ├── __init__.py\r│ └── two.py\r└── readme.md\r2 directories, 6 files\r$ cat packs/two/__init__.py\rprint(\u0026quot;packs two imported\u0026quot;)\r$ cat packs/two/two.py\rdef Two():\rprint(\u0026quot;module two\u0026quot;)\r$ cat main.py\rfrom packs.two.two import Two\rTwo()\r可以看到两个 init 都被运行了，但是我们还不能使用 packs.one.One 这个函数，因为在 main.py 并没有导入这个名称。\n相对路径和绝对路径引用 上面使用类似这种带有相对路径的导入路径from .XXX，这种代表从当前的xx模块中导入名称，如果想要在packs/two/two.py 中使用上一层的 packs/one.py，就可以使用from ..one import One的方式\n# packs/two/__init__.py\rfrom ..one import One\rOne()\r以此类推，那么 \u0026hellip; 代表更上一级。\n但是这种方式还是有问题的，如果项目深度太大就容易写太多 .，还有一种方式就是绝对路径引用，这里的绝对路径是指相对项目根目录而言的，比如上述例子，那么就要修改为：\n# packs/two/__init__.py\rfrom packs.one import One\rOne()\r那引用当前目录的模块必须使用相对路径了，比如上述例子:\n# packs/two/__init__.py\rfrom packs.one import One\rfrom .two import Two\rOne()\r注意，这里不能是 from two import Two 的形式！这个也好理解，因为绝对路径不是 . 开始的，如果相对路径不使用 . 开始，那么就得从项目根目录开始找了。\n当然绝对路径的模块，就有一个 base 路径，所有的文件都是相对此 base 目录，如果在 IDE 中直接打开这里的模块，模块的根目录就是当前模块，显然就会提示找不到了对应的模块了。\n模块搜索顺序 自己写的包名肯定可能和第三方或者标准库同名，不过这种同名通常没有问题。因为python会优先在当前目录搜索然后在环境变量的搜索路径，之后才是标准库和第三方包\n这个和linux$PATH的环境变量一样，按照顺序来搜索，一旦导入每个模块就有全局的命名空间，第二层再次加载就会使用缓存\n这个路径搜索方式和 nodejs 有些区别，nodejs 是一旦同名，优先标准库，如果自定义一个 http 模块，那么永远不会被加载。\npip包管理工具 Python之所以受欢迎不光是因为它简单易学，更重要的是它有成千上万的宝藏库。这些库相当于是已经集成好的工具，只要安装就能在Python里使用。它们可以处理各式各样的问题，无需你再造轮子，而且随着社区的不断更新维护，有些库越来越强大，几乎能媲美企业级应用。那么这些工具库怎么下载安装呢？它们被放在一个统一的“仓库”里，名叫PyPi（Python Package Index），所有的库安装都是从这里调度。有了仓库之后，还需要有管理员，pip就是这样一个角色。\npip是一个工具，用它可以来管理 Python 标准库中其他的包，允许你安装和管理不属于 Python 标准库的其它软件包，其提供了对 Python 包的查找、下载、安装、卸载等功能。总的来说，pip的Python第三方库的大管家，搞懂它，会让你省很多事。从Python 3 \u0026gt;= Python 3.4 、Python2 \u0026gt;= Python2.7.9 版本开始，pip默认包含在Python的安装程序中，在安装Python时将会自动被安装，省事方便。\n*作者：独泪了无痕 链接\n包管理 # 安装\r# 最新版本\rpip install Django\r# 指定版本号\rpip install Django==2.0.0\r# 最小版本\rpip install 'Django\u0026gt;=2.0.0'\r# 升级包\rpip install --upgrade Django\r# 卸载包\rpip uninstall SomePackage\r# 搜索包\rpip search SomePackage\r# 显示安装包信息\rpip show\r# 查看指定包的详细信息\rpip show -f SomePackage\r# 列出已安装的包\rpip list\r# 查看可升级的包\rpip list -o\r包依赖项 pip freeze \u0026gt; requirement.txt # 锁版本\rpip install -r requirement.txt # 指定安装版本\rpip install --user install black # 安装到用户级目录\r使用镜像 pip install -r requirements.txt \\\n\u0026ndash;index-url=https://mirrors.aliyun.com/pypi/simple/ \\\n\u0026ndash;extra-index-url https://pypi.mirrors.ustc.edu.cn/simple/\n这条命令的意思是从阿里云的镜像源和中国科技大学的镜像源下载 requirements.txt 文件中列出的 Python 包，并安装到当前的 Python 环境中。这种方式可以提高包下载的速度，特别是在网络环境不稳定或者网络延迟较大的情况下。\n配置pip镜像 对于windows系统，在C:\\Users\\文件夹下的用户目录（例如如果当前用户是Administrator则是C:\\Users\\Administrator）下创建pip文件夹，然后再在此文件夹下创建pip.ini文件，在文件中写入一下内容：\n[global]\r# 添加默认下载地址，以阿里云源为例\rindex-url = https://mirrors.aliyun.com/pypi/simple/\r[install]\r# 此参数是为了避免麻烦，否则使用的时候可能会提示不受信任\rtrusted-host = mirrors.aliyun.com\r配置完成后在通过 pip config list 查看pip配置。\n升级pip pip install -U pip 或者sudo easy_install --upgrade pip\rPyenv 我们经常会遇到这样的开发需求，比如你手头有多个开发项目，其中项目A要求用python3.7，项目B需要用python3.6，有要求项目A和项目B依赖包相互独立，互不干扰。为了满足这样的开发需求，我们需要在自己的电脑上安装多个Python版本，并且项目之间进行环境隔离。因此，我们要想运行这些项目，在工作电脑上就要安装不同版本的Python。pyenv 是Python版本管理工具，通过系统修改环境变量来实现Python不同版本的切换，利用它可以在同一台电脑上安装多个版本的Python，设置目录级别的Python，还能创建和管理vitual python enviroments。而且所有的设置都是用户级别的操作，不需要sudo命令。\n安装或升级pyenv 在 Windows 系统下安装 pyenv 需要借助 pyenv-win。pyenv-win 是 pyenv 的一个移植版本，专门针对 Windows 平台。下面是详细的安装步骤：\n使用 Git 安装 克隆 pyenv-win 仓库：git clone https://github.com/pyenv-win/pyenv-win.git %USERPROFILE%.pyenv\n配置环境变量 打开系统的环境变量设置界面： 右键点击“此电脑”或“我的电脑”，选择“属性”。 点击“高级系统设置”。 在“系统属性”窗口中，点击“环境变量”。 在“系统变量”部分，找到并编辑 Path 变量，添加以下路径\n%USERPROFILE%\\.pyenv\\pyenv-win\\bin\r%USERPROFILE%\\.pyenv\\pyenv-win\\shims\r保存更改并关闭所有窗口。\n验证安装 pyenv \u0026ndash;version\n使用pyenv安装python 列出所有可用的 Python 版本 pyenv install \u0026ndash;list\n安装指定版本的 Python，例如 3.9.7： pyenv install 3.9.7\n设置全局默认的 Python 版本 pyenv global 3.9.7\n验证 Python 版本 python \u0026ndash;version\nPipenv Pipenv 是 Python 官方推荐的包管理工具，它综合了 virtualenv、pip 和 pyenv 三者的功能，你可以使用 pipenv 这一个工具来安装、卸载、跟踪和记录依赖性，并创建、使用和组织你的虚拟环境。\n安装和升级pipenv pip install pipenv\rpip install --upgrade pipenv\r为项目建立虚拟环境 进入到项目目录中，通过下面的指令为项目创建虚拟环境 $ mkdir pipenv_demo $ cd pipenv_demo $ pipenv \u0026ndash;python 3.9.9 Creating a virtualenv for this project… Pipfile: /Users/dllwh/work/pipenv_demo/Pipfile Using /Users/dllwh/.pyenv/versions/3.9.9/bin/python3 (3.9.9) to create virtualenv… ⠙ Creating virtual environment\u0026hellip;Using base prefix \u0026lsquo;/Users/dllwh/.pyenv/versions/3.7.7\u0026rsquo; New python executable in /Users/dllwh/.local/share/virtualenvs/pipenv_demo-RYMSREda/bin/python3 Also creating executable in /Users/dllwh/.local/share/virtualenvs/pipenv_demo-RYMSREda/bin/python Installing setuptools, pip, wheel\u0026hellip; done. Running virtualenv with interpreter /Users/dllwh/.pyenv/versions/3.7.7/bin/python3\n✔ Successfully created virtual environment!\rVirtualenv location: /Users/dllwh/.local/share/virtualenvs/pipenv_demo-RYMSREda\r上面的操作，给pipenv_demo这个项目初始化了一个 Python 3.9.9 的虚拟环境，并在项目录下生成一个项目依赖包文件 Pipefile。如果系统中没有 3.9.8 版本的Python，pipenv 会调用 pyenv 来安装对应的 Python 的版本。默认地，虚拟环境会创建在 ~/.local/share/virtualenvs目录里面。我们也可以通过 pipenv \u0026ndash;venv查看项目的虚拟环境目录。可以通过 pipenv \u0026ndash;rm 删除虚拟环境。\n用Pipenv管理依赖包 pipenv使用 Pipfile 和 Pipfile.lock 来管理依赖包，并且在使用pipenv添加或删除包时，自动维护 Pipfile 文件，同时生成 Pipfile.lock 来锁定安装包的版本和依赖信息。相比pip需要手动维护requirements.txt 中的安装包和版本，具有很大的进步。\n安装依赖包 为项目安装依赖包到虚拟环境中，使每个项目拥有相互独立的依赖包，是非常不错的Python的开发实践。安装依赖包到虚拟环境中的方法： pipenv install pytest\n执行完上面的命令后，检查一下是否安装成功： pipenv graph\n观察项目的根目录下，又多了一个 Pipfile.lock 文件。这两个文件记录了此项目的依赖包，这两个文件的区别是 Pipfile 中安装的包不包含包的具体版本号，而Pipfile.lock 是包含包的具体的版本号的。如果不想产生 Pipfile.lock 文件，在安装依赖包的时候，加上 –skip-lock 选项即可。 在使用pipenv的时候，常常会安装过程比较慢，这个是因为pipenv创建的 Pipfile 中默认的Pypi源是python官方的 pypi.python.org/simple。\n删除依赖包 如果是要删除虚拟环境中的第三方包，执行： pipenv uninstall pytest\n安装项目所有的依赖包 用git管理项目时候，要把Pipfile和Pipfile.lock加入版本跟踪。这样clone了这个项目的同学，只需要执行：\npipenv install\r就可以安装所有的Pipfile中 [packages]部分列出来的包了，并且自动为项目在自己电脑上创建了虚拟环境。\n安装pipefile.lock中的依赖包 上面的方法都是安装Pipfile中列出来的第三方包的最新版本，如果是想安装Pipfile.lock中固定版本的第三方依赖包，需要执行：\npipenv install --ignore-pipfile\r安装requirements.txt里面的依赖包 如果项目之前使用requirements.txt来管理依赖的，那么使用pipenv安装所有依赖可以采用类似pip的方法\npipenv install -r requirements.txt\n使用虚拟环境开发 虚拟环境创建好了之后，就可以在里面进行开发了。如果在命令行下开发，则在项目目录下执行 pipenv shell，就进入到了虚拟环境中，在这个环境中，已经包含安装过的所有依赖包了，接下来就可以利用这些依赖包进行开发工作了。如果是用Pycharm进行开发，就更简单了，直接用Pycharm打开项目即可。可以从Pycharm中的左侧导航栏里面看到External Libraries显示的是虚拟环境中的Python解释器了。 在虚拟环境中执行开发好的程序，有两种方式，一种是前面提到的先执行pipenv shell进入到虚拟环境后，再执行python程序；另一种方式，则是执行pyenv run，比如在虚拟环境中执行基于pytest框架编写的测试用例，只需要执行下面的命令即可：\npipenv run py.test ","date":"2024-04-12T00:00:00Z","image":"https://a-b-ab.github.io/p/python%E6%A8%A1%E5%9D%97%E5%92%8C%E5%8C%85%E7%AE%A1%E7%90%86/2e39b9e527ae73e37e51f2051e2a53bc_huf4aa06f40646842e5451a0603a8218dc_387515_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/python%E6%A8%A1%E5%9D%97%E5%92%8C%E5%8C%85%E7%AE%A1%E7%90%86/","title":"Python模块和包管理"},{"content":"环境管理 使用Anaconda和Pipenv共同管理Python项目环境\nAnaconda包含了许多常用的Python库,工具和环境,使得安装和管理这些工具变得更加容易.它包括了Python解释器本身以及许多流行的科学计算库,比如NumPy,SciPy,Pandas,Matplotlib等.此外,Anaconda还提供了一个名为Conda的包管理器,用于安装,更新和管理这些库,以及创建和管理不同的Python环境.\n对于Python新手来说,Anaconda是一个很好的选择,因为它提供了一个完整的Python环境,无需手动安装和配置每个库.你只需安装Anaconda,就可以开始编写Python代码并使用各种科学计算工具了.\nPipenv 是 Python 项目的包管理器和虚拟环境管理器.它旨在提供一种简单的方式来创建和管理项目的依赖项,并确保这些依赖项在不同环境中的一致性\n下面是 Pipenv 的一些主要功能:\n虚拟环境管理:Pipenv 可以为每个项目创建独立的虚拟环境,这意味着每个项目都有自己的 Python 解释器和依赖项,而不会干扰系统中的其他项目.\n依赖项管理:Pipenv 使用 Pipfile 和 Pipfile.lock 文件来管理项目的依赖项.Pipfile 是项目的声明性文件,用于指定所需的包和版本,而 Pipfile.lock 文件则记录了确切的依赖项版本,以确保在不同环境中的一致性.\n自动环境激活:当你进入项目目录时,Pipenv 会自动激活该项目的虚拟环境,这意味着你可以立即开始使用项目的依赖项而无需手动激活虚拟环境.\n命令行工具:Pipenv 提供了一组简单的命令行工具,用于安装,更新和删除依赖项,以及管理虚拟环境.\n胎教版:\n当你开始一个新的 Python 项目时,你需要安装各种各样的库来帮助你完成任务.Pipenv 就像一个保姆,帮助你照顾这些库,确保它们安装正确且不会互相冲突.\r想象一下你在一个大杂货店里购物,每个库就像是你要买的商品.Pipenv 就是你的购物清单和购物车.你列出你要买的东西(依赖库),然后 Pipenv 帮你把它们一个一个加到你的购物车里(项目环境).\r同时,Pipenv 还能确保你购物车里的东西都是正确的版本,就像是每个商品上都有一个标签标明它的准确型号和规格一样.这样,当你需要重建你的购物车时,你就可以确保每个东西都是一样的.\r最后,Pipenv 还能帮你管理购物车的整个过程.它会告诉你什么时候需要买新的东西(更新库),或者什么时候需要把一些东西扔掉(删除库).\r总的来说,Pipenv 就像是你的 Python 项目的好帮手,确保你所需的库都安装正确,没有任何混乱\r师兄画的图 编码标准 对于编码标准,推荐跟着社区的PEP8走\nPEP8 是 Python Enhancement Proposal 8 的缩写,它是 Python 社区中的一项规范,用于指导 Python 代码的编写风格和格式.PEP8 旨在提高代码的可读性,并促进代码在不同项目和团队之间的一致性.\n以下是 PEP8 涵盖的一些主要方面:\n缩进:使用 4 个空格作为缩进的标准,而不是制表符.\r行长度:每行代码应尽量控制在 79 个字符以内,但可以允许最多 99 个字符.\r命名约定:变量名应该采用小写字母,单词之间使用下划线分隔(例如,my_variable),而类名应采用驼峰命名法(例如,MyClass).\r空格:在逗号,冒号,分号等符号后应添加一个空格,但在圆括号内部不需要添加空格.\r导入:每个导入应该独立一行,避免使用通配符导入(例如,from module import *).\r注释:使用适当的注释来解释代码的作用和目的,注释应该清晰,简洁且易于理解.\r代码布局:遵循统一的代码布局规范,包括适当的缩进,空行和代码段分隔.\rpep8链接\n规范化 black:代码格式化 Black 是一个自动化代码格式化工具,它可以帮助你自动将 Python 代码按照一致的规范进行格式化,从而使得代码风格更加统一,减少手动调整格式的工作量.\npylint:代码检查 Pylint 是 Python 中一种静态代码分析工具,它的主要作用是检查 Python 代码中的语法错误,代码风格问题以及潜在的逻辑错误,并提供相应的建议和警告,以帮助开发者编写更加规范,可靠和易于维护的代码.\n语法错误检查:Pylint 可以检测出代码中的语法错误,并指出出错的位置和可能的原因.\r代码风格检查:Pylint 可以根据 PEP8 等 Python 代码规范,检查代码中的格式问题,如缩进,空格,命名规范等,并提供相应的建议.\r代码质量检查:Pylint 可以评估代码的质量,并提供一些度量指标,如代码的复杂度,重复代码等,帮助开发者识别出代码中的潜在问题.\r错误和警告提示:Pylint 可以检测出代码中的潜在逻辑错误,不安全的操作,未使用的变量等,并给出相应的警告或错误提示,帮助开发者尽早发现并修复问题.\risort:导包规划化 isort 的作用就是帮助开发者自动将导入语句按照指定的规范进行排序和格式化,以保持代码的清晰易读,并且符合约定俗成的编码规范.\n导入语句排序:isort 可以自动将导入语句按照特定的顺序排序,例如将标准库模块,第三方库模块和本地模块分组,并按字母顺序排列.\r分组和对齐:isort 可以将导入语句分组,并且对齐每个组内的导入语句,使得代码结构更加清晰易读.\r移除重复导入:isort 可以检测并移除重复的导入语句,避免在代码中出现不必要的重复导入.\r自定义配置:isort 提供了丰富的配置选项,可以根据项目的需要进行定制,包括导入顺序,分组规则,对齐方式等.\rflake8(mccabe):代码复杂度检查 flake8是一个Python工具,用于检查你的代码是否符合PEP 8样式指南 Flake8 是 Python 中常用的代码质量检查工具之一,而 mccabe 则是 Flake8 的一个插件,用于检测代码中的复杂度.\n使用方法: 终端激活虚拟环境 activate 环境名 进入所需项目 cd 路径 进行pipenv初始化 进行对应的工具处理\n文件作用 setup.cfg setup.cfg文件是一个配置文件,用于存储与项目相关的各种设置\nPipfile.lock 是由Pipenv生成的一个文件,用于锁定项目的依赖包的精确版本.这个文件确保了你的项目在不同的环境和时间点上都能使用相同版本的依赖,从而避免了因为依赖版本不一致导致的问题\nPipfile 是由Pipenv工具使用的一个文件,用于管理Python项目的依赖.它类似于requirements.txt文件,但提供了更强大的功能,例如锁定依赖的精确版本,区分开发和生产环境的依赖,以及自动管理虚拟环境.\n[[source]]:这个部分定义了包的源.在这个例子中,包的源被设置为PyPI.\n[packages]:这个部分列出了项目的主要依赖.在这个例子中,项目依赖了requests,lxml,black,isort,pylint,flake8这几个包.\n[dev-packages]:这个部分列出了项目的开发依赖.这些依赖只在开发环境中需要,例如测试工具,代码格式化工具等.在这个例子中,项目的开发依赖包括black,pylint,isort,pytest.\n[requires]这个部分定义了项目需要的Python版本.在这个例子中,项目需要Python 3.10.\n[scripts]:在Pipfile中用于定义一些自定义的命令.这些命令可以是任何你想要的shell命令,它们可以帮助你自动化一些常见的任务,例如运行测试,构建项目,或者启动开发服务器\n当你运行pipenv install命令时,Pipenv会查看Pipfile,然后安装列出的所有依赖.如果你运行pipenv install \u0026ndash;dev,Pipenv还会安装[dev-packages]中列出的依赖.\n[packages]和[dev-packages]两个部分都是用来列出项目的依赖的,但它们的用途有所不同.\n[packages]部分列出的是项目运行时需要的依赖.这些依赖是项目的核心部分,无论是在开发环境还是在生产环境,都需要这些依赖.例如,如果你的项目是一个Web应用,那么你可能需要像Flask或Django这样的Web框架,这些依赖应该被列在[packages]部分.\n[dev-packages]部分列出的是只在开发环境中需要的依赖.这些依赖在生产环境中通常是不需要的.例如,你可能需要像pytest这样的测试框架来运行你的测试,或者需要像flake8或black这样的工具来检查你的代码格式,这些依赖应该被列在[dev-packages]部分.\n当你运行pipenv install命令时,Pipenv会安装[packages]部分列出的所有依赖.如果你运行pipenv install \u0026ndash;dev命令,Pipenv会额外安装[dev-packages]部分列出的依赖.这样,你可以在开发环境中安装所有的依赖,而在生产环境中只安装必要的依赖,从而减少生产环境的复杂性和安全风险.*\nDockerfile 这个文件是一个Dockerfile,它是一种文本文档,用于自动化Docker镜像的构建过程.Dockerfile中的每一行都代表在镜像中执行的一条命令.\nDocker作用:\nDocker是一个开源的应用容器引擎,它允许开发者将应用及其依赖打包到一个可移植的容器中,然后发布到任何流行的Linux或Windows机器上,也可以实现虚拟化.容器是完全使用沙箱机制,相互之间不会有任何接口.\n以下是Docker的一些主要用途:\n提供一致的环境:Docker可以确保在不同的环境中(开发,测试,生产等)运行相同的软件和配置.\n简化配置:Docker使得配置应用环境变得更加简单.你只需要编写一个Dockerfile或者docker-compose.yml文件,就可以定义你的整个环境.\n隔离应用:每个Docker容器都在其自己的环境中运行,这意味着你可以在同一台机器上运行多个不同版本的同一软件,而不会有冲突.\n微服务架构:Docker非常适合微服务架构,因为它允许你为每个服务创建一个单独的容器.\n持续集成/持续部署(CI/CD):Docker可以与CI/CD工具(如Jenkins,GitLab CI等)集成,使得自动化构建,测试和部署变得更加简单.\n快速部署:Docker容器比传统的虚拟机启动得更快,这使得它非常适合需要快速扩展的应用.\n总的来说,Docker是一种使得软件开发,测试和部署变得更加简单和一致的工具.\n.pylintrc 这个文件是一个.pylintrc文件,它是Pylint工具的配置文件.Pylint是一个Python代码静态分析工具,它可以检查代码中的错误,查找不符合PEP 8编码规范的代码,以及查找代码中的其他问题\n总的来说,这个.pylintrc文件定义了Pylint如何检查你的Python代码.你可以根据你的需要修改这个文件,以定制Pylint的行为\n.gitinore 这个文件是一个.gitignore文件,它告诉Git哪些文件或目录不应该被版本控制系统跟踪 在这个.gitignore文件中,每一行都代表一个忽略规则,Git将忽略与这些规则匹配的文件和目录\npycache/和*.py[cod]规则告诉Git忽略Python编译的字节码文件和优化的字节码文件.\n*.so规则告诉Git忽略C扩展的共享对象文件.\nbuild/,dist/,*.egg-info/等规则告诉Git忽略Python打包和分发过程中生成的文件和目录.\n.manifest和.spec规则告诉Git忽略PyInstaller生成的文件.\nhtmlcov/,.tox/,.coverage等规则告诉Git忽略单元测试和覆盖率报告生成的文件和目录.\n总的来说,.gitignore文件帮助你管理你的Git仓库,确保只有需要跟踪的文件被包含在版本控制中.\n.gitattributes 这个文件是一个.gitattributes文件,它用于设置Git仓库的特定行为\n在这个.gitattributes文件中,Dockerfile linguist-language=Python这行告诉GitHub的Linguist工具将Dockerfile文件识别为Python语言.Linguist是GitHub用来在仓库中进行语言检测的工具,它决定了在GitHub仓库概览中显示的语言比例,以及在\u0026quot;搜索\u0026quot;功能中如何对代码进行语法高亮\n通常,Linguist会自动识别文件的语言,但有时你可能希望覆盖这个行为.在这个例子中,即使Dockerfile实际上是Docker的语言,但这行设置让Linguist将其识别为Python语言\n总的来说,.gitattributes文件允许你自定义Git在你的仓库中的行为\n.env 这个文件是一个.env文件,它用于设置环境变量.在这个文件中,PYTHONPATH=${PYTHONPATH}:${PWD}这行代码将当前工作目录(${PWD})添加到PYTHONPATH环境变量中.\nPYTHONPATH是一个环境变量,它告诉Python解释器在哪里查找模块.当你尝试导入一个模块时,Python解释器会在PYTHONPATH中列出的目录中查找这个模块.\n在这个例子中,${PWD}是一个环境变量,它表示当前工作目录.所以,这行代码的效果是将当前工作目录添加到PYTHONPATH中.这意味着你可以直接导入当前工作目录中的Python模块,而不需要将它们移动到Python的安装目录或其他位置.\n总的来说,.env文件允许你设置环境变量,这可以帮助你配置你的应用的运行环境.\n.dockerignore 这个文件是一个.dockerignore文件,它的作用类似于.gitignore文件,但是它是用于Docker的..dockerignore文件告诉Docker在构建镜像时应该忽略哪些文件和目录.\n在这个.dockerignore文件中:\n.idea/:这个规则告诉Docker忽略.idea/目录.这个目录通常包含由JetBrains IDE(如PyCharm)生成的项目设置和配置文件.\n.git/:这个规则告诉Docker忽略.git/目录.这个目录包含Git的版本控制信息.\n.vscode/:这个规则告诉Docker忽略.vscode/目录.这个目录通常包含由Visual Studio Code生成的项目设置和配置文件.\n总的来说,.dockerignore文件帮助你管理你的Docker镜像,确保只有需要的文件被包含在镜像中,从而减小镜像的大小并提高构建速度.\ntest文件夹 在Python项目中,test文件夹通常用于存放所有的测试代码.这些测试代码用于验证你的应用的功能和行为,确保它们按照预期工作.\n以下是test文件夹的一些主要用途:\n单元测试:这些是针对单个函数或类的测试,用于验证它们的行为.例如,你可能有一个测试用例来验证你的函数是否正确地处理边界条件.\n集成测试:这些是针对多个组件或模块的测试,用于验证它们是否能够正确地一起工作.\n回归测试:当你修改或添加新的代码时,你可以运行你的测试套件来确保你没有引入新的错误.\n性能测试:这些测试用于检查你的代码的性能,例如,检查一个函数是否在给定的时间内完成.\n在Python中,你可以使用unittest库来编写测试用例,并使用pytest或nose等工具来运行你的测试.\n总的来说,test文件夹是你的项目的重要组成部分,它帮助你保持你的代码的质量和可靠性.\nsrc文件夹 在Python项目中,src文件夹通常用于存放项目的源代码.这个目录通常包含一个__init__.py文件,这个文件告诉Python这个目录应该被视为一个包\n以下是src文件夹的一些主要用途:\n代码组织:src文件夹提供了一个地方,你可以在其中组织你的Python模块和包.你可以在src文件夹中创建子目录来进一步组织你的代码.\n导入模块:当你的Python文件在src文件夹中时,你可以使用相对导入来导入其他模块.例如,如果你有一个在src/my_package目录中的my_module.py文件,你可以在src/my_package目录中的其他Python文件中使用from . import my_module来导入它.\n测试:将你的源代码放在src文件夹中可以使得测试更加容易.你可以在一个单独的tests文件夹中编写测试,然后在测试中导入src文件夹中的模块.\n总的来说,src文件夹是你的项目的重要组成部分,它帮助你组织和管理你的源代码.\ndocs文件夹 在Python项目中,docs文件夹通常用于存放项目的文档.这些文档可能包括:\n项目说明:对项目的概述,包括它的目的,主要功能,以及如何使用它.\nAPI文档:对项目中的类,函数,方法和模块的详细描述.\n开发者指南:对如何为项目贡献代码的指南,包括代码风格指南,测试策略,以及代码提交流程.\n用户手册:对如何使用项目的详细指南,包括安装指南,教程,以及常见问题解答.\n这些文档可以使用Markdown,reStructuredText,或其他标记语言编写,然后使用工具如Sphinx生成HTML或PDF格式的文档.\n总的来说,docs文件夹是你的项目的重要组成部分,它帮助用户和开发者理解和使用你的项目.\n.vscode文件夹 .vscode文件夹是Visual Studio Code(VS Code)编辑器为特定项目创建的一个配置文件夹.这个文件夹通常包含两个文件:settings.json和launch.json.\nsettings.json:这个文件包含了VS Code的工作区设置.这些设置会覆盖用户级别和全局的VS Code设置.例如,你可以在这个文件中设置特定的Python解释器,或者设置代码格式化工具的参数.\nlaunch.json:这个文件定义了调试配置.例如,你可以在这个文件中设置启动和调试Python程序的参数.\n.vscode文件夹通常会被添加到.gitignore文件中,因为这个文件夹通常包含特定用户或特定环境的设置,这些设置可能不适用于其他用户或环境.\n.pytestcache文件夹 .pytest_cache文件夹是由pytest测试框架自动创建的.pytest在运行测试时使用这个文件夹来存储一些中间数据和测试结果,以便在后续的测试运行中重用.\n以下是.pytest_cache文件夹中可能包含的一些文件和目录:\nv/cache/lastfailed:这个文件包含了上次测试失败的测试用例的信息.pytest可以使用这个信息来首先运行失败的测试用例.\nv/cache/nodeids:这个文件包含了所有测试用例的节点ID.pytest使用节点ID来唯一标识每个测试用例.\nd/:这个目录包含了一些由pytest插件生成的数据.\n通常,你不需要手动修改.pytest_cache文件夹中的内容.这个文件夹通常会被添加到.gitignore文件中,因为它包含的是特定于本地环境的数据,不应该被添加到版本控制系统中\nCACHEDIR.TAG 这个文件名为CACHEDIR.TAG,它是一个缓存目录标签文件.这种文件通常用于标记一个目录被用作缓存,也就是说,该目录中的文件是可以被安全删除的,因为它们可以被重新生成.\n在这个特定的情况下,CACHEDIR.TAG文件是由pytest测试框架创建的,用于标记.pytest_cache目录为缓存目录.\n文件中的Signature: 8a477f597d28d172789f06886806bc55是一个固定的签名,用于标识这个文件是一个缓存目录标签.\n总的来说,CACHEDIR.TAG文件的存在告诉其他工具和用户,.pytest_cache目录中的文件是临时的,可以被安全删除.\n_pycache_文件 __pycache__是Python自动创建的一个目录,用于存储编译后的Python代码,也就是字节码文件.当你运行一个Python程序时,Python解释器会首先将源代码(.py文件)编译成字节码(.pyc文件),然后执行这个字节码.\n字节码文件的主要目的是加速程序的启动.当Python解释器再次运行同一个程序时,如果源代码没有改变,解释器可以直接加载字节码文件,而不需要再次编译源代码.\n__pycache__目录中的文件通常有如下的命名格式:module.version.pyc,其中module是源代码文件的名称,version是Python解释器的版本.\n通常,你不需要手动管理__pycache__目录或其中的文件.Python解释器会自动创建和更新这些文件.这个目录通常会被添加到.gitignore文件中,因为字节码文件是特定于Python解释器的,不应该被添加到版本控制系统中.\n","date":"2024-04-12T00:00:00Z","image":"https://a-b-ab.github.io/p/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/24366b5b414b2df862acef0a71d899d6d74df8ac_hueeba57d55881e25701a618879ca510a4_3614944_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/","title":"Python项目管理"},{"content":"Ajax Asynchronous javascript and xml AJAX 不是新的编程语言，而是一种使用现有标准的新方法。 AJAX 最大的优点是在不重新加载整个页面的情况下，可以与服务器交换数据并更新部分网页内容。 AJAX 不需要任何浏览器插件，但需要用户允许 JavaScript 在浏览器上执行。 XMLHttpRequest 只是实现 Ajax 的一种方式。\n应用：\n运用 XHTML+CSS 来表达资讯； 运用 JavaScript 操作 DOM（Document Object Model）来执行动态效果； 运用 XML 和 XSLT 操作资料; 运用 XMLHttpRequest 或新的 Fetch API 与网页服务器进行异步资料交换； 简介 AJAX 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。\nAjax是基于现有的internet标准 AJAX是基于现有的Internet标准，并且联合使用它们：\nXMLHttpRequest对象(异步的与服务器交换数据) JavaScript/DOM(信息显示/交互) CSS (给数据定义样式) XML (作为转换数据的格式) Google Suggest 使用 AJAX 创造出动态性极强的 web 界面：当您在谷歌的搜索框输入关键字时，JavaScript 会把这些字符发送到服务器，然后服务器会返回一个搜索建议的列表。\nXHR创建对象 AJAX-创建XMLHttpRequest 对象 XMLHttpRequest 是 AJAX 的基础。\nXMLHttpRequest 对象 所有现代浏览器均支持 XMLHttpRequest 对象,XMLHttpRequest 用于在后台与服务器交换数据。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。\nXHR请求 AJAX - 向服务器发送请求 XMLHttpRequest 对象用于和服务器交换数据。\n向服务器发送请求 如需将请求发送到服务器，我们使用 XMLHttpRequest 对象的 open() 和 send() 方法：\n对于 web 开发人员来说，发送异步请求是一个巨大的进步。很多在服务器执行的任务都相当费时。AJAX 出现之前，这可能会引起应用程序挂起或停止。\n通过 AJAX，JavaScript 无需等待服务器的响应，而是：\n在等待服务器响应时执行其他脚本 当响应就绪后对响应进行处理 XHR响应 AJAX - 服务器响应 如需获得来自服务器的响应，请使用 XMLHttpRequest 对象的 responseText 或 responseXML 属性。\nXHR readyState onreadystatechange 事件 当请求被发送到服务器时，我们需要执行一些基于响应的任务。 每当 readyState 改变时，就会触发 onreadystatechange 事件。 readyState 属性存有 XMLHttpRequest 的状态信息。\n使用回调函数 回调函数是一种以参数形式传递给另一个函数的函数 如果您的网站上存在多个 AJAX 任务，那么您应该为创建 XMLHttpRequest 对象编写一个标准的函数，并为每个 AJAX 任务调用该函数。 该函数调用应该包含 URL 以及发生 onreadystatechange 事件时执行的任务（每次调用可能不尽相同）\nAJAX数据库 AJAX 可用来与数据库进行动态通信。\nAJAX XML实例 ","date":"2024-04-10T00:00:00Z","image":"https://a-b-ab.github.io/p/ajax/1_huc5df6c4b548c84046f46eee0cba36680_1398675_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/ajax/","title":"Ajax"},{"content":"爬虫基础 “可见即可爬~”\n爬虫概念 如果互联网是一张大的蜘蛛网，那一台计算机上的数据便是蜘蛛网上的一个猎物，而爬虫程序就是一只小蜘蛛，沿着蜘蛛网抓取自己想要的猎物/数据\n网络爬虫也叫网络蜘蛛，特指一类自动批量下载网络资源的程序 网络爬虫是伪装成客户端与服务端进行数据交互的程序\n爬虫的应用 1.数据采集 2.搜索引擎 3.模拟操作 4.软件测试 5.网络安全\n爬虫的分类（略） 爬虫的一般开发流程： 1.最简单的单一页面数据的爬取：\nurl-\u0026gt;发送请求，获取响应-\u0026gt;提取数据-\u0026gt;保存数据\n2.多页面数据的爬取\n发送请求，获取响应——\u0026gt;提取url地址，继续请求\n爬虫开发的重难点 1.数据的获取（反爬） 2.采集的速度\nHTTP和HTTPS 大多数商业应用采用的架构： 1.c/s client server 2.b/s broser(浏览器) server 3.m/s mobile server\n以上统称为客户端与服务端\nHTTP协议（超文本传输协议） 爬取想要的数据前，一定要明确其使用的是什么协议\nHTTP是基于TCP/IP通信协议来传输数据的\nTCP/IP通信的三次握手与四次挥手 三次握手建立连接 client:嘿，服务端girl！我想和你建立连接 server：好呀，嘻嘻 client：真好，那我们开始（数据）交互吧！\n（进行数据交互ing）\n四次挥手断开连接 client：我已经和你交互（数据）完了，我们断开连接吧! server：你确定要断开连接吗？ server：那你断开连接吧 client：欧克，那我断开连接了\nHTTP请求流程： 我们日常用浏览器搜索东西，输入的是URL,浏览器会将其自动转换为HTTP协议\n一次http请求的基本流程，有客户端向服务端发起一次请求（request），而服务器在接收到以后返回给客户端一个响应（response）.所以一次完成的http请求包含请求和响应两部分\n浏览器发送http请求的过程：\n1.域名解析\r2.发起TCP的3次握手\r3.建立TCP连接后发起HTTP请求\r4.服务器响应http请求，浏览器得到html代码\r5.浏览器解析html代码，并请求html代码中的资源（js，css，图片等）\r6.浏览器对页面进行渲染呈现给用户\rtip： 在网页的右键检查network-\u0026gt;name-\u0026gt;request headers view parsed下的connection:keep-alive保持常连接，就不用频繁三次握手和四次挥手了\n浏览器获取的内容（elements的内容）包含：url地址对应的响应+js+css+picture 爬虫会获取：url地址对应的响应\nurl（浏览器搜索框里的内容） 发送http请求时，通过url对网络资源进行定位\nurl：统一资源定位符。用来标识某一处资源的地址，也叫网址\n组成：协议+域名（端口默认80）+路径+参数 http协议的端口号默认为80可以不写，http协议的端口号默认为443可以不写，（域名可以确定是哪一台电脑，而端口号是为了确定是哪台电脑的哪一个应用）\n域名通常是IP地址的映射\nhttp请求格式 客户端发送一个HTTP请求到服务器的请求消息包括一下部分：请求行,请求头，空行和请求数据\n请求方法 分类：\nOPTIONS\rPUT\rDELETE\rTRACE\rCONNECT\r常用方法是GET 和 POST\nGET:负责从服务器获取数据 POST:负责向服务器提交数据\n请求头 http请求正文 请求正文通常是使用POST请求中表单数据，而对于GET请求，请求体则为空\n在爬虫中，如果构造POST，需要正确的content-type，并了解各种请求库的各个参数设置时使用的是哪种content-type，不然可能会导致post提交后无法正常响应\nHTTP响应格式 由四个部分组成，分别是： 状态行（响应行） 消息报头 空行 响应正文\nhttp协议的特点 HTTP是无连接的 HTTP是媒体独立的 HTTP是无状态的\n保持http连接状态的技术是会话和Cookies\nhttps协议，安全版的HTTP http是基于tcp/ip协议的，而https是在http协议的基础之上，再加一层SSL/TLS协议，数据在传输过程中是加密的\nhttp是明文传输的而https是密文传输，所以较安全但性能低\n会话技术 会话在服务端，就是网站的服务器，用来保存用户的会话信息；cookies在客户端，也可以理解为浏览器端\nCookie 指某些网站为了辨别用户身份，进行session跟踪而存储在用户本地终端上的数据（通常经过加密）\nCookie可以理解为一个凭证 1.实际是由服务器发给客户端的特殊信息 2.这些信息以文本文件的方式存放在客户端 3.客户端每次向服务器发送请求的时候都会带上这些特殊信息 4.服务器在接收到Cookie以后，会验证cookie的信息，以此来辨别用户的身份\n爬虫为什么要使用cookie 好处： 能够访问登陆页面\r有一定的反爬作用\r坏处： 请求太频繁有可能被识别为爬虫\r一般使用多账号解决\rSession 一个浏览器窗口从打开到关闭这个期间\n在一个客户从打开浏览器到关闭浏览器这个期间，发起的所以请求都可以被识别为同一个用户，session是基于cookie的\n简单的爬虫程序 TODO:\n我的爬虫思路 TODO:\n","date":"2024-04-02T00:00:00Z","image":"https://a-b-ab.github.io/p/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/0fb747771069b0a5bcb1e8a58d048259_hu8655991107acde54143419b95283ed47_1882172_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/","title":"爬虫笔记"},{"content":"绪论 通常将优化一个模型的过程称为训练或学习 检验模型效果的过程称为测试 对不同参数下的模型表现基涅检验来选择模型参数是开发或验证 损失函数 损失函数（Loss Function）是机器学习和深度学习中用于评估模型预测值与实际值之间差异程度的函数。通过最小化损失函数，我们可以训练模型以更准确地预测结果。不同的学习任务和模型类型会使用不同的损失函数。以下是一些常见的损失函数：\n拟合和过拟合 拟合（Fitting）和过拟合（Overfitting）是机器学习和统计建模中常见的概念，它们描述了模型在训练数据上的表现与在新数据（即测试数据）上表现之间的关系。\n拟合（Fitting） 拟合是指模型在训练数据上学习的过程，目的是使模型能够准确地描述或预测训练数据中的关系或模式。在理想情况下，我们希望模型能够学习到数据的内在规律，而不仅仅是记住训练数据的具体细节。然而，在实际应用中，由于数据的复杂性、噪声以及模型的表达能力等因素，完全准确地拟合所有训练数据往往是不现实的。\n过拟合（Overfitting） 过拟合是指模型在训练数据上表现得过于复杂，以至于它学习了训练数据中的噪声和随机波动，而不是数据背后的真实关系。因此，当模型被应用到新的、未见过的数据时，它的表现会显著下降。过拟合通常发生在模型具有非常高的表达能力（例如，具有大量参数的非线性模型）而训练数据又相对较少时。\n如何识别过拟合 训练集和验证集/测试集的性能差异：如果模型在训练集上的性能（如准确率、损失值）远好于在验证集或测试集上的性能，那么很可能是出现了过拟合。 学习曲线：通过观察训练集和验证集上的损失随训练迭代次数变化的曲线，可以判断是否存在过拟合。如果验证集上的损失在训练过程中开始增加，而训练集上的损失仍在下降，那么很可能是过拟合。 防止过拟合的方法 增加数据量：更多的训练数据可以帮助模型学习到更加一般化的特征，减少过拟合的风险。 简化模型：减少模型的复杂度，例如减少神经网络的层数或神经元数量，可以降低过拟合的风险。 正则化：通过在损失函数中添加正则化项（如L1、L2正则化）来惩罚复杂的模型，从而防止过拟合。 早停法（Early Stopping）：在验证集性能开始下降时停止训练，以避免过拟合。 数据增强：通过生成额外的训练样本来增加数据量，例如通过旋转、缩放、裁剪等方式处理图像数据。 Dropout：在神经网络训练过程中随机丢弃一部分神经元，以减少神经元之间的共适应性，从而防止过拟合。 LDA主题模型简介 LDA（Latent Dirichlet Allocation）主题模型，也称为潜在狄利克雷分布模型，是一种文档主题生成模型，同时也是一种非监督机器学习技术。它基于三层贝叶斯概率模型，包含词、主题和文档三层结构。LDA模型的主要目的是识别文档中的主题，将文档-词汇矩阵转换成文档-主题矩阵（分布）和主题-词汇矩阵（分布）。\nLDA模型的基本思想 LDA模型认为，一篇文档是由多个主题混合而成的，而每个主题又是由多个词汇按照一定概率分布组成的。文档的生成过程可以看作是，首先以一定的概率选择某个主题，然后在这个主题下以一定的概率选择某个词，不断重复这个过程，直到生成整篇文档。LDA的使用则是这个过程的逆过程，即根据一篇得到的文档，去推断出这篇文档的主题以及这些主题所对应的词。\nLDA模型的数学表达 LDA模型通过概率图模型来表示上述的生成过程。具体来说，LDA假设文档到主题的分布服从多项式分布，而主题到词的分布也服从多项式分布。这两个分布的参数（即主题分布和词分布）是LDA模型需要学习的。\nLDA模型的算法流程 LDA模型的算法流程大致如下：\n初始化：随机给文档中的每个词分配一个主题编号。 统计：统计每个主题下每个词出现的次数，以及每个文档中每个主题下词出现的次数。 迭代更新：对于文档中的每个词，根据当前的主题分布和词分布，重新计算该词属于各个主题的概率，并根据这个概率更新该词的主题编号。这个过程会不断重复，直到主题分布和词分布收敛。 LDA模型的参数 LDA模型中有几个重要的参数需要设置，包括：\n主题数量：这是需要预先设定的参数，表示文档集合中将被识别的主题个数。主题数量的选择对模型的效果有很大影响，通常需要通过实验来确定。 迭代次数：表示算法迭代的次数，迭代次数越多，模型可能越稳定，但计算量也会增加。 超参数：包括α（文档-主题分布的先验参数）和β（主题-词分布的先验参数）。这些参数可以通过先验知识来设定，也可以通过模型训练来自动调整。 LDA模型的应用 LDA模型在文本挖掘、信息检索、自然语言处理等领域有广泛的应用。它可以用于文档的聚类、主题识别、特征提取等任务。通过LDA模型，我们可以从大量的文本数据中挖掘出潜在的主题信息，为后续的文本分析提供有力的支持。\nLDA模型的实现 LDA模型的实现通常可以使用一些开源的库和工具，如Gensim、scikit-learn等。这些库提供了LDA模型的实现和训练接口，用户只需要提供文档集合和相应的参数，就可以进行模型的训练和主题识别。\n总的来说，LDA主题模型是一种非常有效的文本挖掘工具，它可以帮助我们从大量的文本数据中挖掘出潜在的主题信息，为后续的文本分析提供有力的支持。\n词干化 词干化（Stemming）是自然语言处理（NLP）中的一种常见文本预处理技术，其目的是将单词缩减为其基本形式或词干，以减少词汇的复杂性并提高文本分析的准确性。以下是关于词干化的详细解释：\n定义 词干化是一种基于规则的文本处理技术，它尝试通过去除单词的后缀来将单词还原到它们的词干或根形式。这通常涉及到简单的字符串操作，如去除常见的后缀（如-ing、-ed、-s等）。词干化可以使不同形式的单词被视为相同的单词，从而简化文本分析。\n原理 词干化技术常常利用词缀规则来确定单词的词干。它不考虑单词的词法和语法，仅仅基于一系列预定义的规则来截断单词，从而得到其词干。这种方法的优点是简单快速，但可能不够精确，因为有时候去除后缀后得到的词干可能不是实际存在的单词。\n适用场景 词干化通常用于快速文本处理，例如信息检索或文档分类。在这些场景中，目标是快速地将不同形式的单词映射到它们的共同词干，以减少不同形式的单词的数量，提高处理效率。\n常用算法 Porter词干化算法：是最早和最常用的词干化算法之一。它通过一系列规则和模式匹配来截断单词的后缀，得到其词干。这个算法在许多自然语言处理任务中广泛使用，尤其是信息检索领域。 Snowball词干化算法（Porter2）：是Porter词干化算法的改进版本，提供了更准确的词干化，同时支持多种语言。它修复了Porter算法中的一些问题，使得词干化结果更加准确。 Lancaster词干化算法：是另一种基于规则的词干化算法，它比Porter算法更加激进，更倾向于将单词截断至更短的形式。它适用于某些任务，但可能会导致一些不常见的单词被切割过度。 示例 以单词“running”为例，经过词干化处理后，其词干为“run”。类似地，“ran”和“runs”等单词也会被还原为词干“run”。\n总结 词干化是自然语言处理中一种重要的文本预处理技术，它通过去除单词的后缀来得到其词干，从而简化文本分析并减少词汇的多样性。虽然词干化技术简单快速，但在某些情况下可能不够精确。因此，在选择是否使用词干化技术时，需要根据具体任务的需求和精确性要求来进行权衡。\n抽取词袋 抽取词袋（Bag-of-Words，简称BoW）是自然语言处理（NLP）中常用的一种文本特征提取方法，用于将文本数据转换为数值表示，从而便于机器学习算法的处理。以下是抽取词袋的基本步骤和要点：\n一、定义与原理 词袋模型的基本思想是将文本看作是由单词构成的“袋子”（即无序集合），然后统计每个单词在文本中出现的频次或使用其他权重方式来表示单词的重要性。这样，每个文本都可以用一个向量表示，其中向量的每个维度对应于一个单词，并记录了该单词在文本中的出现次数或权重。\n二、抽取步骤 文本预处理：\n分词：将文本分割成单词或词语。这通常依赖于特定的分词工具或算法，如正则表达式、机器学习模型等。 去除停用词：停用词是指那些在文本中频繁出现但对文本内容理解帮助不大的词汇，如“的”、“是”等。去除停用词可以减少词袋向量的维度，提高处理效率。 词干提取/词形还原：将单词还原为其基本形式（词干或词根），以减少词汇的多样性。这可以通过词干化算法（如Porter算法）或词形还原算法来实现。 构建词汇表：\n创建一个包含文本数据集中所有唯一词汇的词汇表。这个词汇表包括文本数据集中出现的所有单词，不重复，无顺序。 文本向量化：\n对于每个文本文档，将文档中的每个词汇映射到词汇表中的词汇。这通常涉及将文档中的每个词汇替换为其在词汇表中的索引。 统计每个词汇在文档中的出现次数（词频，TF），或者使用更高级的方法，如TF-IDF（Term Frequency-Inverse Document Frequency）来衡量词汇的重要性。TF-IDF考虑了词汇在文档中的出现频率以及在整个文本数据集中的分布情况，能够更准确地反映词汇对于文档的重要性。 每个文本文档都被表示为一个向量，其中向量的维度等于词汇表的大小。向量的每个元素对应于词汇表中的一个词汇，其值表示相应词汇在文档中的出现次数或其他相关信息（如TF或TF-IDF值）。 三、注意事项 忽略词汇顺序：词袋模型忽略了文档中词汇的语法和语义顺序，因此对于同一组词汇，无论它们出现的顺序如何，都会生成相同的文档向量。这在一定程度上限制了词袋模型在需要理解文本结构和语义关系任务中的应用。 维度灾难：当词汇表非常大时，词袋向量的维度也会非常高，这可能导致“维度灾难”问题，即随着维度的增加，计算复杂度和所需的存储空间急剧增加，同时模型的性能可能会下降。为了缓解这个问题，可以采用特征选择或降维技术来减少向量的维度。 扩展性：词袋模型可以很容易地扩展到大规模文本数据集上，但需要注意处理效率和存储成本的问题。 四、应用场景 词袋模型在文本分类、聚类、信息检索等任务中有广泛的应用。通过将文本数据转换为数值表示，词袋模型为这些任务提供了有效的输入特征，从而支持机器学习算法的训练和预测。\n五、总结 抽取词袋是自然语言处理中一种简单而有效的文本特征提取方法。通过文本预处理、构建词汇表和文本向量化等步骤，可以将文本数据转换为数值表示，为后续的机器学习算法提供输入特征。然而，词袋模型也存在一些局限性，如忽略词汇顺序和可能导致的维度灾难问题。在实际应用中，需要根据具体任务的需求和数据的特性来选择合适的文本表示方法。\n文本向量化 文本向量化是自然语言处理（NLP）中的一项关键技术，它旨在将文本数据（如单词、句子、文档等）转换为数值向量表示，以便计算机能够处理和分析。这种转换过程有助于捕捉文本中的语义信息，提高文本处理任务的效率和准确性。以下是对文本向量化的详细解析：\n一、定义与原理 文本向量化是将文本信息转换为向量表示的过程，这些向量能够表达文本的语义特征。通过向量化，文本数据可以被转化为计算机可处理的数值型数据，进而应用于各种NLP任务中，如文本分类、聚类、信息检索、情感分析等。\n二、主要方法 文本向量化的方法多种多样，以下是一些常见的方法：\n独热编码（One-Hot Encoding）\n独热编码是一种简单的文本向量化方法，它首先构建一个包含所有可能词汇的词典，然后为每个词汇分配一个唯一的索引。对于文本中的每个词汇，将其表示为一个与词典长度相同的向量，其中该词汇对应索引位置上的值为1，其余位置上的值为0。 优点：实现简单，易于理解。 缺点：当词典很大时，向量维度会非常高，导致“维度灾难”问题；且无法表示词汇之间的语义关系。 词袋模型（Bag-of-Words, BoW）\n词袋模型忽略了文本中词汇的顺序和语法结构，仅考虑词汇在文本中出现的频次。它首先将文本分割成词汇，然后统计每个词汇在文本中出现的次数，最后将这些频次作为向量的元素。 优点：实现简单，能够处理可变长度的文本。 缺点：同样存在维度灾难问题；且无法考虑词汇之间的语义关系和上下文信息。 TF-IDF（Term Frequency-Inverse Document Frequency）\nTF-IDF是一种改进的词袋模型，它结合了词频（TF）和逆文档频率（IDF）两个因素来评估词汇在文本中的重要性。TF表示词汇在文本中出现的频次，而IDF则表示词汇在文档集合中的普遍重要性。通过计算TF和IDF的乘积，可以得到词汇的TF-IDF值，从而更准确地表示词汇在文本中的权重。 优点：能够考虑词汇在文档集合中的普遍重要性，减少常见词汇的权重。 缺点：仍然无法考虑词汇之间的语义关系和上下文信息。 词嵌入（Word Embedding）\n词嵌入是一种将词汇映射到低维向量空间中的方法，这些向量能够捕捉词汇之间的语义关系。常见的词嵌入方法包括Word2Vec、GloVe、BERT等。这些方法通常基于大量的文本数据，通过训练神经网络模型来学习词汇的向量表示。 优点：能够捕捉词汇之间的语义关系，提高文本处理任务的准确性和效率。 缺点：需要大量的训练数据和计算资源；且模型的可解释性相对较差。 三、应用场景 文本向量化在NLP领域有着广泛的应用场景，包括但不限于：\n文本分类：将文本数据转换为向量表示后，可以使用分类算法对文本进行分类。 文本聚类：通过计算文本向量之间的相似度，可以将相似的文本聚集成簇。 信息检索：利用文本向量化技术，可以构建高效的搜索引擎，根据用户查询返回相关的文本结果。 情感分析：将文本数据转换为向量表示后，可以使用机器学习算法对文本的情感倾向进行分析。 推荐系统：通过计算用户兴趣和项目内容的向量表示，可以为用户推荐相关的项目或内容。 四、未来趋势 随着NLP技术的不断发展，文本向量化技术也在不断进步。未来，文本向量化技术可能会更加注重以下几个方面：\n语义理解能力：提高文本向量在捕捉语义信息方面的能力，以更好地支持复杂的NLP任务。 跨语言处理能力：开发能够处理多种语言的文本向量化技术，以满足全球化背景下的NLP需求。 高效性与可扩展性：优化文本向量化的算法和模型，以提高处理速度和可扩展性，支持大规模文本数据的处理和分析。 综上所述，文本向量化是NLP领域中的一项重要技术，它通过将文本数据转换为数值向量表示，为各种NLP任务提供了有力的支持。随着技术的不断发展，文本向量化技术将在更多领域发挥重要作用。\n决策树 决策树（Decision Tree）是一种在机器学习和决策分析领域广泛使用的技术，它通过树形结构来表示决策过程，并帮助解决分类、回归等问题。以下是对决策树的详细解析：\n一、定义与原理 决策树是一种通过树形图来表达决策过程中不同方案及其可能结果的图解法。在机器学习中，决策树是一个预测模型，它表示对象属性与对象值之间的一种映射关系。决策树由根节点、内部节点（决策节点）、分支和叶节点组成，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别或决策结果。\n二、基本组成部分 根节点：决策树最上边的节点，表示整个决策过程的起点。 内部节点（决策节点）：代表一个属性或特征的测试，用于根据属性值将数据集分割成不同的子集。 分支：连接节点之间的线，表示不同的决策路径。 叶节点：决策树的末端节点，表示一个类别或决策结果。 三、构建过程 决策树的构建过程通常包括以下几个步骤：\n数据准备：对原始数据进行清洗、变换和相关性分析，以确保数据的质量和适用性。 选择最佳属性：在构建决策树时，需要选择一个属性作为当前节点的测试属性。这通常通过计算信息增益、信息增益率或其他指标来实现。 分割数据集：根据选定的测试属性将数据集分割成不同的子集。 递归构建子树：对每个子集重复上述过程，直到满足停止条件（如子集属于同一类别、没有更多属性可供选择等）。 剪枝：为了避免过拟合，通常需要对生成的决策树进行剪枝处理，即去除一些不必要的分支。 四、经典算法 ID3算法：由J.R.Quinlan在1979-1986年间提出，使用信息增益来选择测试属性。 C4.5算法：对ID3算法的改进，使用信息增益率来选择测试属性，并支持对连续属性的离散化处理和不完整数据的处理。 CART算法：另一种常用的决策树算法，它既可以用于分类也可以用于回归，使用基尼系数作为选择测试属性的标准。 五、优点与缺点 优点 直观易懂：决策树以树形图的形式表示决策过程，易于理解和解释。 计算量小：相对于其他机器学习算法，决策树的计算量较小。 能够处理非线性关系：决策树能够处理复杂的非线性关系。 能够处理缺失值：一些决策树算法（如C4.5）能够处理数据中的缺失值。 缺点 容易过拟合：当决策树过于复杂时，可能会对数据中的噪声进行拟合，导致过拟合现象。 对连续变量处理不佳：传统的决策树算法对连续变量的处理相对较弱。 类别不平衡问题：当数据集中各类别样本数量差异较大时，决策树的性能可能会受到影响。 六、应用场景 决策树算法在多个领域都有广泛的应用，包括但不限于：\n分类问题：如文本分类、图像分类等。 回归问题：预测连续值的问题，如房价预测、股票价格预测等。 信用评估：根据客户的个人信息和贷款申请信息，判断客户的信用等级。 医学诊断：根据患者的症状和检测结果，判断患者可能患有的疾病。 推荐系统：根据用户的历史行为和喜好，预测用户可能感兴趣的物品或内容。 综上所述，决策树是一种功能强大且易于理解的机器学习算法，它通过树形结构来表示决策过程，并广泛应用于分类、回归等多个领域。\n剪枝 预剪枝和后剪枝是决策树算法中常用的两种剪枝技术，用于避免决策树模型的过拟合，提高模型的泛化能力。以下是对这两种剪枝技术的详细解析：\n一、预剪枝（Pre-pruning） 定义与原理： 预剪枝是在决策树生成过程中，对每个结点在划分前进行估计，如果当前结点的划分不能带来决策树模型泛化性能的提升（即验证集精度未提升），则不对当前结点进行划分，并且将当前结点标记为叶结点。\n核心思想： 在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。\n优点：\n降低过拟合风险：预剪枝通过提前停止树的生长，减少了不必要的分支，从而降低了过拟合的风险。 减少训练时间和测试时间：由于决策树的部分分支被提前剪除，因此训练时间和测试时间都会显著减少。 缺点：\n可能导致欠拟合：预剪枝是基于“贪心”策略的，它可能禁止了一些当前划分不能提升泛化性能但后续划分可能显著提升性能的分支的展开，从而带来欠拟合的风险。 参数选择困难：预剪枝需要设定一些停止划分的阈值（如熵减小的阈值），这些阈值的选择往往依赖于经验或交叉验证，具有一定的主观性。 二、后剪枝（Post-pruning） 定义与原理： 后剪枝是先从训练集生成一颗完整的决策树，然后自底向上地对树中的所有非叶节点进行逐一考察，如果将该结点对应的子树换为叶结点能够带来泛化性能的提升（即验证集精度提升），则把该子树替换为叶结点。\n核心思想： 在决策树完全生成后，通过比较剪枝前后的分类精度来决定是否进行剪枝。\n优点：\n泛化性能通常优于预剪枝：后剪枝决策树通常保留了更多的分支，能够更充分地利用数据中的信息，因此其泛化性能往往优于预剪枝决策树。 欠拟合风险小：后剪枝是在决策树完全生成后进行的，因此不会因为提前停止树的生长而导致欠拟合。 缺点：\n训练时间开销大：后剪枝需要生成完整的决策树后再进行剪枝操作，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大很多。 剪枝过程复杂：后剪枝需要自底向上地对树中的所有非叶节点进行逐一考察，并计算剪枝前后的分类精度，因此其剪枝过程相对复杂。 三、总结 预剪枝和后剪枝都是决策树算法中重要的剪枝技术，它们各有优缺点。在实际应用中，可以根据具体问题的特点和数据集的特性来选择合适的剪枝方法。例如，如果数据集较小或模型的训练时间受限，可以考虑使用预剪枝；如果数据集较大且对模型的泛化性能要求较高，可以考虑使用后剪枝。\n支持向量机 支持向量机（Support Vector Machine, SVM）是一种基于统计学习理论的监督学习模型，主要用于分类和回归分析。以下是对支持向量机的详细解析：\n一、基本概念 定义：支持向量机是一类按监督学习(supervised learning)方式对数据进行二元分类的广义线性分类器(generalized linear classifier)，其决策边界是对学习样本求解的最大边距超平面(maximum-margin hyperplane)。\n提出者：V.N. Vapnik, A.Y. Chervonenkis, C. Cortes等。\n提出时间：1964年。\n二、工作原理 超平面分割：SVM通过寻找一个超平面来对样本进行分割，分割的原则是间隔最大化。这个超平面由支持向量确定，支持向量是离超平面最近的样本点。 间隔最大化：SVM的目标就是最大化这个间隔值，这样可以使得分类器对于新的、未见过的样本有更好的泛化能力。 核方法：当遇到线性不可分的样例时，通常的做法是将样例特征映射到高维空间中去。尽管这样做可能会导致维度变得非常高，但通过使用核函数，可以在低维空间进行计算，而将实质上的分类效果表现在高维空间，从而避免了直接在高维空间中的复杂计算。 三、核函数 核函数是SVM中的关键部分，它决定了数据从低维空间映射到高维空间的方式。常见的核函数包括：\n线性核函数：简单，求解快，可解释性强。 高斯核函数（RBF核）：可以映射到无限维，决策边界更多样，只有一个参数，更容易选择，特征多时会选用。但可解释性差，容易过拟合，计算速度较慢。 多项式核函数：可解决非线性问题，参数较多，对大数量级特征不适用。 Sigmoid核函数：主要用于神经网络。 四、应用领域 支持向量机在许多领域都有广泛的应用，如：\n文本分类：如垃圾邮件过滤、情感分析、主题分类等。 图像识别：如手写数字识别、人脸识别、物体检测等。 生物信息学：如基因表达数据分析、蛋白质结构预测、药物设计等。 金融预测：如股票价格预测、信用评分、风险评估等。 五、优缺点 优点：\n具有非常完善的数学理论。 对于非线性问题具有较好的处理能力。 鲁棒性较好，对噪声数据具有较强的抗干扰能力。 缺点：\n对于大规模数据集，训练时间较长。 对参数和核函数的选择敏感，不同的参数和核函数可能导致模型性能差异较大。 六、总结 支持向量机是一种强大的机器学习算法，它通过寻找最大间隔超平面对数据进行分类，并通过核函数处理非线性问题。在实际应用中，需要根据具体问题选择合适的核函数和参数，以达到最佳的预测性能。\n核函数 核函数（Kernel Function）在支持向量机（SVM）中扮演着至关重要的角色。它们允许SVM算法有效地处理非线性分类问题，通过将输入空间（通常是低维的）映射到一个更高维的特征空间，在这个空间中，原本非线性可分的数据变得线性可分\n随机森林 随机森林（Random Forest）是一种集成学习方法，它通过构建多个决策树并将它们的结果进行投票或平均来得到最终的预测结果。以下是关于随机森林的详细解析：\n一、定义与原理 定义：随机森林是指利用多棵决策树对样本数据进行训练、分类并预测的一种方法。它不仅可以用于分类问题，还可以用于回归问题。在分类问题中，通过多棵树分类器的投票决定最终分类结果；在回归问题中，则由多棵树预测值的均值决定最终预测结果。\n原理：随机森林基于Bagging（Bootstrap Aggregating）策略，通过结合多个决策树的预测结果来提高整个模型的准确性和稳定性。在构建随机森林的过程中，每个决策树都是基于随机选取的样本和特征子集进行训练的，这使得每棵树都具有一定的差异性，从而增加了模型的多样性。\n二、构建过程 数据抽样：使用自助采样法（bootstrap sampling）从原始数据集中随机抽取多个样本，形成多个子数据集。这些子数据集之间可能存在重叠，但每个子数据集都用于训练一棵决策树。 特征选择：在每个节点上，随机选择一部分特征进行分裂。这些特征是从所有特征中随机选取的，而不是每次都使用全部特征。这种特征随机性有助于增加模型的多样性。 构建决策树：对于每个子数据集，使用选定的特征构建一棵决策树。决策树的构建过程遵循标准的决策树算法，如ID3、C4.5或CART等。 集成预测：将多棵决策树的预测结果进行投票或平均，以得到最终的预测结果。对于分类问题，通常采用投票方式；对于回归问题，则采用平均值方式。 三、特征重要性评估 随机森林模型可以评估各个特征在分类或回归中所起的作用，即特征重要性。特征重要性通常通过以下两种方式计算：\n平均不纯度减少：在构建随机森林的每棵树时，每个特征的分裂都会导致某种程度的不纯度减少（如基尼不纯度或信息增益）。一个特征的重要性可以被定义为它在所有树中减少的不纯度的平均值。 平均精度下降：另一种计算特征重要性的方法是通过随机排列特征值并测量这种排列对模型精度的影响。一个特征的重要性可以被定义为它被随机排列时模型精度下降的平均值。 四、优缺点 优点：\n高精度：通过集成多个决策树的预测结果，随机森林通常具有较高的预测准确性。 抗过拟合：由于引入了随机性（样本随机和特征随机），随机森林能够有效地降低模型的方差，从而抑制过拟合。 处理大量数据：随机森林能够有效地处理具有大量特征和数据的问题，且不需要进行复杂的特征选择。 处理缺失值：随机森林能够自然地处理数据中的缺失值，无需进行额外的缺失值处理。 易于使用和调优：随机森林的参数相对较少，且对参数的选择不敏感，因此在实际应用中比较容易使用和调优。 缺点：\n模型复杂度高：由于构建了许多棵决策树，随机森林的模型可能会相对复杂，需要更多的计算资源。 预测过程较慢：当森林中树木数量很多时，每次进行预测所需的时间会增加。但可以通过并行计算来优化。 可解释性不佳：虽然随机森林可以通过特征重要性来评估各个特征的影响，但整体上作为一个集成模型，其预测过程不如单一决策树那样直观易懂。 五、应用领域 随机森林因其强大的性能和广泛适应性，被广泛应用于多个领域，包括但不限于：\n医学诊断：通过分析患者的各种特征（如年龄、性别、症状指标等），帮助医生准确地诊断疾病。 图像分类：在特定领域的图像分类任务中表现出色，特别是当特征明显且数据量有限时。 房价预测：通过分析房屋的各种属性（如面积、地点、卧室数量等），帮助预测房屋的市场价格。 农业产量预测：根据气候、土壤以及种子类型等特征，预测农作物的年产量。 信用卡欺诈检测：分析用户的交易行为特征，识别出异常的交易模式，帮助银行检测和预防信用卡欺诈。 工业设备故障预测：通过监控设备运行参数和历史数据，检测出异常情况，并预测设备可能的故障。 总之，随机森林是一种强大而灵活的机器学习算法，适用于各种复杂的数据分析问题。\n集成学习 集成学习（Ensemble Learning）是一种强大的机器学习策略，它将多个弱学习器（也称为基学习器或基估计器）组合起来，以构建一个具有更强性能的机器学习模型。这种策略基于“三个臭皮匠，顶个诸葛亮”的思想，即多个个体学习器的结合可以产生超过单一学习器的整体性能。\n一、集成学习的分类 集成学习可以根据基学习器的类型是否相同，分为同质集成和异质集成两种方法：\n同质集成：使用相同类型的学习算法构建多个基学习器。例如，所有的基学习器都是决策树或都是神经网络。 异质集成：使用不同类型的学习算法构建基学习器。例如，可以同时使用支持向量机、逻辑回归和朴素贝叶斯等算法作为基学习器。 二、集成学习的基本原理 集成学习的基本原理基于两个关键假设：\n基学习器的准确性：基学习器的预测准确性应高于随机猜测。 基学习器的差异性：基学习器之间应具有一定的差异性，这样它们的预测结果才能互补，从而提高整体模型的性能。 三、集成学习的常用算法 集成学习有多种实现算法，其中一些最著名的算法包括：\nBagging（Bootstrap Aggregating）\n原理：通过自助采样（Bootstrap Sampling）方式，从原始数据集中有放回地采样得到多个子数据集，然后使用相同的学习算法在这些子数据集上构建多个基学习器，最后通过投票或平均的方式得到最终结果。 特点：能够有效降低模型的方差，提高模型的鲁棒性。 代表算法：随机森林（Random Forest）是Bagging的一个变体，通过随机特征选择和样本采样构建多颗决策树，并通过投票机制进行预测。 Boosting\n原理：通过迭代的方式构建基学习器。每一轮迭代中，Boosting算法会根据上一轮的学习结果调整样本的权重，使得模型更关注错误分类的样本。然后将这些基学习器进行线性组合，得到最终的强学习器。 特点：能够有效降低模型的偏差，提高模型的准确性。 代表算法：AdaBoost（Adaptive Boosting）和梯度提升（Gradient Boosting）是Boosting的两个重要算法。 Stacking\n原理：将多个基学习器的预测结果作为输入，再通过一个元学习器（Meta Learner）进行结合，得到最终的预测结果。 特点：能够充分利用基学习器之间的差异性，提高模型的泛化能力。 四、集成学习的应用 集成学习在多个领域都有广泛的应用，包括但不限于：\n金融风控：用于信用评估、欺诈检测等风控任务，提高风险识别能力。 医疗诊断：用于疾病诊断、药物预测等任务，提高诊断的准确性和可靠性。 图像识别：在计算机视觉领域，用于图像分类、目标检测等任务，提高图像识别的准确率。 自然语言处理：在自然语言处理领域，用于文本分类、情感分析等任务，提高文本处理的效果。 五、集成学习的优势和挑战 优势：\n提高准确性：通过多个基学习器的结合，可以提高整体模型的准确性。 提高鲁棒性：通过投票或平均等方式，可以减少模型的方差，提高鲁棒性。 充分利用信息：能够充分利用基学习器之间的差异性，提高模型的泛化能力。 挑战：\n计算复杂度高：需要构建多个基学习器并进行结合，因此计算复杂度较高。 数据不平衡问题：在某些情况下，数据可能存在不平衡的情况，这会影响集成学习的性能。 可解释性较差：由于集成了多个基学习器的预测结果，因此整体模型的可解释性较差。 综上所述，集成学习是一种强大的机器学习策略，它通过结合多个基学习器的预测结果来提高整体模型的性能。在实际应用中，可以根据具体问题的需求选择合适的集成学习算法和基学习器。\n神经网络基础 神经网络基础涉及多个方面，以下是对其的详细阐述：\n一、定义与起源 定义：神经网络是机器学习中的一种模型，是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。它最初是受生物神经系统的启发，为了模拟生物神经系统而出现的。 起源：神经网络模型最初是基于生物神经系统的结构和功能进行构建的，尤其是模拟神经元之间的连接和信号传递方式。 二、基本结构与组成 神经元：神经网络的基本组成单元是神经元（或称为节点）。每个神经元接收来自其他神经元的输入信号，通过加权求和、激活函数等步骤产生输出信号。 层次结构：典型的神经网络结构包括输入层、隐藏层和输出层。输入层负责接收外部输入数据；隐藏层是神经网络的核心部分，负责处理输入数据并提取特征；输出层则根据处理结果产生最终的输出。 连接与权重：神经元之间通过连接（也称为边）相互连接，每个连接都有一个权重值，表示该连接对信号传递的影响程度。 三、工作原理 信号传递：在神经网络中，输入信号首先进入输入层神经元，然后通过加权求和和激活函数处理传递到隐藏层神经元。隐藏层神经元对输入信号进行进一步处理并提取特征后，再将处理结果传递到输出层神经元。输出层神经元最终产生输出结果。 学习与优化：神经网络通过训练过程来学习输入与输出之间的映射关系。在训练过程中，神经网络会根据实际输出与期望输出之间的误差来调整连接权重和偏置项等参数，以减小误差并提高模型的准确性。 四、应用领域 图像识别：神经网络在图像识别领域具有广泛应用，如人脸识别、物体检测等。 自然语言处理：神经网络也被用于自然语言处理任务中，如文本分类、情感分析等。 推荐系统：在推荐系统中，神经网络可以根据用户的历史行为和其他信息来预测用户的兴趣偏好并推荐相关物品。 五、发展趋势 深度学习：随着计算能力的提升和大数据的兴起，深度学习成为神经网络发展的重要方向。深度学习通过构建更深层次的神经网络结构来提取更加抽象和复杂的特征表示，从而进一步提高模型的准确性和泛化能力。 模型压缩与优化：为了降低神经网络的计算复杂度和提高实时性能，研究者们开始关注模型压缩与优化技术。这些技术包括剪枝、量化、知识蒸馏等方法，可以在保证模型性能的前提下减少模型参数数量和计算量。 跨学科融合：神经网络的发展也促进了与其他学科的融合。例如，与脑科学的结合有助于更好地理解神经网络的工作机制和优化方法；与医学的结合则可以推动医疗诊断技术的创新和发展。 综上所述，神经网络作为一种重要的机器学习模型具有广泛的应用前景和发展潜力。随着技术的不断进步和应用领域的不断拓展，相信神经网络将在未来发挥更加重要的作用。\n理解神经网络是端到端的系统 神经网络，特别是深度神经网络，被视为一种端到端（End-to-End）的系统，这一理解主要基于以下几个方面：\n直接映射：端到端的神经网络直接学习从输入到输出的映射关系，而不需要人为地将问题分解成多个子问题或中间步骤。这意味着网络能够自动发现数据中的复杂特征和规律，而不需要人工设计特征提取器或规则。\n黑箱模型：在端到端的系统中，神经网络内部的具体工作机制（即如何从输入转换到输出）对于用户来说通常是不可见的，或者说是一个“黑箱”。用户只需要关心输入和输出，而不需要深入了解网络内部的复杂计算过程。这种特性使得神经网络在处理复杂问题时具有更高的灵活性和泛化能力。\n整体优化：由于神经网络是端到端的，因此可以对整个系统进行整体优化，而不是分别优化各个子模块。在训练过程中，网络会根据输出与期望结果之间的误差来调整其内部参数（如权重和偏置），以最小化这个误差。这种整体优化的方式有助于提高模型的性能和稳定性。\n减少人工干预：端到端的神经网络减少了人工干预的需要。在传统的机器学习方法中，通常需要人工设计特征提取器、选择分类器、调整参数等步骤。而在端到端的神经网络中，这些步骤都被自动化了，大大减轻了人工负担，并提高了模型的泛化能力。\n应用广泛：由于神经网络能够自动学习复杂的映射关系，并且具有高度的灵活性和泛化能力，因此被广泛应用于各种领域，如图像识别、语音识别、自然语言处理、推荐系统等。在这些应用中，神经网络都表现出了端到端系统的优势。\n综上所述，理解神经网络是一个端到端的系统，就是认识到它能够直接学习从输入到输出的映射关系，而不需要人工设计中间步骤或特征提取器；同时，它能够对整个系统进行整体优化，减少人工干预的需要，并广泛应用于各种领域。这种端到端的特性使得神经网络在处理复杂问题时具有更高的效率和准确性。\n感知机 感知机（Perceptron），也被称为感知器，是Frank Rosenblatt在1957年提出的一种人工神经网络模型。以下是对感知机的详细解读：\n一、定义与基础 定义：感知机是一种二分类的线性模型，其输入是实例的特征向量，输出是实例的类别，取值为+1和-1。它属于判别模型，是神经网络和支持向量机的基础。 基础概念：感知机的学习目标是求得一个能够将训练数据集正实例点和负实例点完全正确分开的分离超平面。这个超平面将特征空间划分为两部分，分别对应正类和负类。 二、工作原理 输入与输出：感知机的输入是特征向量，每个特征都对应一个权重，这些权重的和加上一个偏置项，经过符号函数（如sign函数）处理后，得到输出类别（+1或-1）。 学习过程：感知机的学习过程是误分类驱动的，通过不断调整权重和偏置项来减少误分类点的数量。具体来说，当某个实例点被误分类时，即其类别与通过当前模型计算得到的类别不符时，就调整权重和偏置项，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离。 三、优点与缺点 优点： 简单易懂：感知机模型结构简单，易于理解和实现。 训练速度快：由于感知机模型简单，因此训练速度相对较快。 适用于大规模数据集：感知机在处理大规模数据集时表现出色。 缺点： 只能解决线性可分问题：感知机只能处理线性可分的数据集，对于非线性问题表现不佳。 只能进行二分类：感知机只能进行二分类任务，对于多分类问题需要进行改进。 对噪声和异常点敏感：感知机对噪声和异常点比较敏感，需要进行特殊处理以提高模型的鲁棒性。 四、应用领域 感知机模型虽然简单，但在实际应用中仍然具有广泛的应用领域，包括但不限于：\n图像识别与分类：感知机可以用于图像的分类任务，如将图片分为不同的类别。 自然语言处理：在自然语言处理领域，感知机可以用于文本分类、情感分析等任务。 信号处理：在信号处理领域，感知机可以用于信号分类和噪声识别等任务。 数据挖掘与预测：在数据挖掘和预测领域，感知机可以用于分类、回归等任务。 五、发展历史与现状 历史：感知机由Frank Rosenblatt在1957年提出，是神经网络和支持向量机的基础。它的出现标志着人工神经网络研究的开始。 现状：随着人工智能和机器学习技术的不断发展，感知机作为最基础的人工神经网络模型之一，仍然具有一定的研究价值和应用前景。同时，更复杂的神经网络模型如深度神经网络（DNN）、卷积神经网络（CNN）等已经得到了广泛的应用和发展。 综上所述，感知机是一种简单而有效的二分类线性模型，在多个领域具有广泛的应用价值。然而，由于其自身的局限性，如只能处理线性可分问题和二分类任务等，因此在实际应用中需要根据具体问题的特点进行选择和改进。\n多层感知机 多层感知机（Multilayer Perceptron，简称MLP）是深度学习中的一种基础且广泛使用的神经网络模型。以下是多层感知机的简略介绍：\n一、定义与结构 定义：多层感知机是一种前馈神经网络，由多个神经元层组成，包括输入层、一个或多个隐藏层以及输出层。每一层的神经元都与前一层全连接，通过权重和激活函数实现非线性映射。 结构：多层感知机的基本结构包括输入层、隐藏层（可能有多层）和输出层。输入层接收外部数据，隐藏层对数据进行处理和特征提取，输出层则给出最终的预测结果。 二、工作原理 前向传播：输入数据通过输入层进入网络，经过隐藏层的加权求和与激活函数处理后，最终由输出层输出预测结果。 反向传播：在训练过程中，通过比较输出层的预测结果与实际标签的误差，利用反向传播算法调整网络中的权重和偏置项，以最小化误差。 三、优点与缺点 优点： 强大的表征能力：通过多个隐藏层的组合，可以学习到复杂的数据特征和表示。 灵活的非线性映射：激活函数的引入使得多层感知机能够处理非线性问题。 广泛的应用场景：适用于分类、回归、聚类等多种机器学习任务。 缺点： 训练时间长：多层感知机的训练需要大量的计算资源和时间。 易于过拟合：复杂的网络结构可能导致过拟合问题，需要通过正则化等技术进行缓解。 可解释性差：多层感知机的决策过程相对复杂，不如一些传统机器学习模型易于解释。 四、应用场景 多层感知机因其强大的表征能力和广泛的应用场景，在多个领域得到了广泛应用，包括但不限于：\n计算机视觉：图像分类、目标检测、图像分割等。 自然语言处理：文本分类、情感分析、机器翻译等。 推荐系统：个性化推荐、广告推荐等。 金融风控：信用评分、欺诈检测等。 医疗健康：疾病诊断、药物预测、基因分类等。 工业制造：质量控制、故障诊断、预测维护等。 五、实现工具与框架 在Python中，可以使用多种深度学习框架来实现多层感知机，如TensorFlow、PyTorch、Keras等。这些框架提供了丰富的API和工具，使得多层感知机的构建和训练变得更加便捷和高效。\n综上所述，多层感知机作为一种基础且强大的神经网络模型，在深度学习和机器学习领域具有广泛的应用前景。然而，在实际应用中需要注意其训练时间长、易于过拟合以及可解释性差等缺点，并结合具体问题和数据情况选择合适的模型和算法。\nBP神经网络 BP神经网络，全称为反向传播神经网络（Back Propagation Neural Network），是1986年由Rumelhart和McClelland为首的科学家提出的一种多层前馈神经网络模型。该网络通过误差逆向传播算法进行训练，是应用最广泛的神经网络模型之一。以下是对BP神经网络的详细解析：\n一、定义与结构 定义：BP神经网络是一种按照误差逆向传播算法训练的多层前馈神经网络。 结构：BP神经网络通常由输入层、隐藏层（可有多个）和输出层组成。每一层的神经元都与前一层全连接，通过加权和的方式传递信号，并经过激活函数进行非线性变换。 二、工作原理 BP神经网络的训练过程包括两个阶段：前向传播和反向传播。\n前向传播：\n输入信号从输入层开始，逐层向前传播，直到输出层。 在每一层，神经元的输入是前一层神经元输出的加权和，经过激活函数处理后得到该层神经元的输出。 反向传播：\n计算输出层的实际输出与期望输出之间的误差。 将误差信号反向传播回输入层，通过调整各层神经元之间的连接权重和偏置项，使误差逐步减小。 权重和偏置项的调整依据是梯度下降法，即沿着误差梯度下降的方向调整权重和偏置项，以最小化误差函数。 三、特点与优势 强大的非线性映射能力：BP神经网络能够逼近复杂的非线性函数关系，适用于解决各种复杂的非线性问题。 并行处理能力：BP神经网络的各个神经元之间是并行计算的，适合于结构化并行处理，能够快速处理大量数据。 良好的泛化能力：经过训练的BP神经网络能够对未见过的样本进行较好的预测和分类。 四、应用领域 BP神经网络因其强大的建模能力和广泛的应用场景，在多个领域得到了广泛应用，包括但不限于：\n函数逼近：用输入向量和相应的输出向量训练一个网络逼近一个函数。 模式识别：用一个待定的输出向量将它与输入向量联系起来，实现图像识别、语音识别等功能。 分类：把输入向量所定义的合适方式进行分类，如文本分类、图像分类等。 数据压缩：减少输出向量维数以便于传输或存储。 五、存在的问题与改进 尽管BP神经网络具有许多优点，但也存在一些问题和局限性：\n训练时间长：BP神经网络的训练过程需要大量的迭代计算，因此训练时间较长。 易陷入局部最优解：BP神经网络对初始权重和偏置项敏感，容易陷入局部最优解而非全局最优解。 网络结构选择困难：BP神经网络的网络结构选择需要经验和试错，网络的过拟合和欠拟合问题需要仔细调整。 针对这些问题，研究者们提出了许多改进措施，如引入动量项、学习率自适应调整、使用更复杂的激活函数等，以提高BP神经网络的训练效率和性能。\n六、总结 BP神经网络作为一种经典且广泛应用的神经网络模型，在多个领域发挥着重要作用。通过不断优化和改进，BP神经网络将继续在人工智能和机器学习领域发挥更大的潜力。\n径向基函数网络 径向基函数网络（Radial Basis Function Network，简称RBF网络）是一种使用径向基函数作为激活函数的人工神经网络。这种网络在多个领域都有广泛的应用，包括函数近似、时间序列预测、分类和系统控制等。以下是对径向基函数网络的详细解析：\n一、定义与结构 定义：径向基函数网络是一种三层前向网络，包括输入层、隐含层和输出层。其中，隐含层使用径向基函数作为激活函数，实现输入到输出的非线性映射。 结构： 输入层：由信号源节点组成，仅起到传输信号的作用，对输入信息不做任何变换。 隐含层：节点数视所描述问题的需要而定，隐单元的变换函数是径向基函数，通常是对中心点径向对称且衰减的非负非线性函数，如高斯函数。 输出层：对输入模式作出响应，是隐含层输出的线性组合。 二、工作原理 径向基函数：径向基函数是某种沿径向对称的标量函数，通常定义为空间中任一点x到某一中心c之间欧氏距离的单调函数，可记作k(||x-c||)。其作用往往是局部的，即当x远离c时函数取值很小。 前向传播：输入信号通过输入层进入网络，经过隐含层的径向基函数变换后，再传递到输出层。输出层将隐含层的输出加权求和得到最终的输出结果。 训练过程： 无监督学习：首先通过无监督学习确定输入层与隐含层间的参数（如基函数的中心和宽度）。 有监督学习：然后利用有监督学习确定隐含层与输出层间的权值。 三、特点与优势 逼近能力：径向基函数网络能够逼近任意非线性函数，具有强大的非线性映射能力。 学习速度：由于参数初始化具有一定的方法，并非随机初始化，且隐含层到输出层的变换是线性的，因此学习收敛速度快。 结构简单：相比其他神经网络模型，径向基函数网络的结构相对简单，训练过程也较为简洁。 可解释性强：由于网络结构清晰，各层功能明确，因此可解释性较强。 可在线学习：径向基函数网络支持在线学习，即在新数据到达时能够动态调整模型的权值。 四、应用领域 模式分类：如图像分类、语音识别等，通过将输入样本映射到高维空间来提高分类的准确性。 数据挖掘：用于聚类分析、回归分析等任务，帮助挖掘数据中的潜在规律和趋势。 时间序列预测：如股票价格预测、天气预测等，通过学习历史数据的模式和规律来预测未来的趋势和变化。 控制系统：如自适应控制、机器人控制等，通过学习环境的状态和反馈信号来实现智能化的控制策略。 五、总结 径向基函数网络作为一种常用的人工神经网络模型，通过径向基函数的非线性映射和局部响应特性，实现了输入到输出的高效转换。其在多个领域的广泛应用和独特优势使其成为人工智能和机器学习领域的重要工具之一。\nHopfield网络 Hopfield网络，也被称为Hopfield神经网络或浩斯菲尔德网络，是由物理学家约翰·霍普菲尔德（John Hopfield）在1982年发明的一种递归神经网络。以下是对Hopfield网络的详细解析：\n一、定义与结构 定义：Hopfield网络是一种结合存储系统和二元系统的神经网络，用于解决模式识别问题和提供一类组合优化问题的近似解。 结构：Hopfield网络由多个神经元组成，每个神经元可以取两个值（通常是0或1，或者-1和1），表示神经元的兴奋状态。神经元之间通过权重相连，且权重是对称的，即神经元i和神经元j之间的权重w(i,j)等于神经元j和神经元i之间的权重w(j,i)。 二、工作原理 能量函数：Hopfield网络的工作原理基于Lyapunov稳定性定理和LaSalle不变性定理，其内部状态可以通过能量函数来描述。网络的目标是使能量函数最小化，从而达到稳定状态。 联想记忆：Hopfield网络具有联想记忆功能，即能够通过部分或损坏的输入信息回忆起完整的记忆模式。这是通过网络的权重矩阵和神经元的激活状态共同作用实现的。 三、特点与优势 递归性：Hopfield网络是一种递归神经网络，其神经元之间的连接形成了一个闭环系统。 收敛性：网络保证了向局部极小的收敛，但也可能收敛到错误的局部极小值而非全局最小值。 记忆模型：Hopfield网络提供了模拟人类记忆的模型，通过神经元的连接和激活状态来存储和恢复记忆。 灵活性：网络可以存储多个记忆模式，并通过学习算法调整权重矩阵来适应新的记忆模式。 四、应用领域 模式识别：Hopfield网络可以用于存储和识别特定的模式，如人脸识别、指纹识别等。 优化问题：网络还可以用于解决优化问题，如最短路径问题、旅行商问题等。 数据压缩：通过压缩数据中的冗余信息，Hopfield网络可以用于数据压缩以提高存储和传输效率。 异常检测：在数据分析中，网络能够检测数据中的异常值或异常模式。 自组织映射：实现高维数据到低维空间的映射，便于数据的可视化和分析。 五、局限性与改进 记忆容量有限：Hopfield网络的记忆容量受到神经元数量和连接方式的限制。 可能收敛到错误的局部极小值：如前所述，网络在优化过程中可能陷入局部最优解而非全局最优解。 改进方向：为了克服这些局限性，研究人员对Hopfield网络进行了多种改进，如引入更复杂的神经元结构、改变连接方式以及结合其他优化算法等。 综上所述，Hopfield网络作为一种重要的神经网络模型，在模式识别、优化问题、数据压缩等领域具有广泛的应用前景。随着研究的不断深入和技术的不断发展，Hopfield网络的性能和应用范围将不断扩大和完善。\nBoltmann机 Boltzmann机（Boltzmann Machine，简称BM）是一种由二值随机神经元构成的两层对称连接神经网络，该定义于2018年由全国科学技术名词审定委员会公布。以下是对Boltzmann机的详细解析：\n一、定义与结构 定义：Boltzmann机是一种生成模型，它由二值随机神经元组成，并通过对称连接形成网络。这种网络通过优化玻尔兹曼能量函数来训练其权重。 结构：Boltzmann机通常包含可见层（visible layer）和隐藏层（hidden layer）。可见层包含输入数据的节点，而隐藏层包含用于学习的节点。两层之间通过权重进行连接，且这些连接是双向的，即每个节点都与其他层的节点相连。 二、工作原理 初始化：在训练开始时，隐藏层的状态被随机初始化。 迭代更新：随后，根据输入数据、连接权重和激活函数，网络会进行迭代更新。这包括计算输入层和隐藏层之间的激活值，以及更新连接权重以最小化能量函数。 生成与分类：在生成模型中，Boltzmann机会根据输入数据生成一组符合输入数据特征的隐藏状态；在判别模型中，它则根据输入数据判断其所属的类别。 三、数学模型 Boltzmann机的数学模型可以表示为概率分布，其中$P(x,h)$表示数据$x$在隐藏状态$h$下的概率。这个概率分布通过玻尔兹曼能量函数来定义，通常使用sigmoid函数作为激活函数来模拟神经元的激活行为。\n四、应用与前景 应用：Boltzmann机在多个领域都有潜在的应用，包括图像处理、自然语言处理、机器学习等。然而，由于其训练过程的复杂性和计算资源的消耗，实际应用中可能面临一些挑战。 深度玻尔兹曼机：为了克服这些挑战，研究者们提出了深度玻尔兹曼机（Deep Boltzmann Machine，DBM），它包含多个隐藏层，能够学习更复杂的表示和抽象。DBM在图像识别、自然语言处理等领域展现出了良好的性能。 五、未来发展趋势与挑战 发展趋势：随着计算能力的提升和算法的优化，Boltzmann机及其变体有望在更多领域得到应用。特别是在自动驾驶、智能机器人等前沿技术中，Boltzmann机可能发挥重要作用。 挑战：然而，Boltzmann机的训练过程仍然是一个挑战。由于其连接权重是双向的且需要优化能量函数，这导致训练过程可能非常耗时且容易陷入局部最优解。因此，未来的研究需要关注如何改进训练算法以提高效率和性能。 综上所述，Boltzmann机作为一种重要的神经网络模型，在多个领域都具有潜在的应用价值。然而，其训练过程的复杂性和计算资源的消耗仍然是实际应用中需要克服的难题。随着技术的不断进步和发展，我们有理由相信Boltzmann机将在更多领域展现出其独特的优势和价值。\n自组织映射网络 自组织映射网络（Self-Organizing Map，简称SOM或SOFM）是一种基于无监督学习方法的神经网络，由芬兰神经网络专家Kohonen于1981年提出。该网络通过模拟人脑中神经细胞的自组织特性，对输入数据进行学习和映射，生成一个低维的拓扑表示。以下是关于自组织映射网络的详细解析：\n一、定义与结构 定义：自组织映射网络是一种竞争学习网络，通过神经元之间的竞争实现大脑神经系统中的“近兴奋远抑制”功能，并具有把高维输入映射到低维的能力（拓扑保形特性）。 结构：自组织映射网络通常包含输入层和输出层（竞争层）。输入层负责接收原始数据，输出层则通过竞争学习机制对输入数据进行分类和映射。输出层的神经元被放置在一维、二维甚至多维的网格节点中，最常见的是二维拓扑结构。 二、工作原理 竞争学习：当输入数据进入网络时，输出层的神经元会进行竞争，以争夺对输入数据的响应权。竞争过程通常通过计算输入数据与每个神经元之间的距离（如欧氏距离）来实现，距离最小的神经元获胜并更新其权值。 合作过程：获胜神经元周围的神经元也会受到一定程度的刺激，这被称为侧向相互作用或合作过程。这种合作机制有助于保持输出层神经元的拓扑结构，并使得相似的输入数据在输出层上能够形成连续的映射区域。 三、主要特性 拓扑保形特性：自组织映射网络能够保持输入数据在降维过程中的拓扑结构不变，即相似的输入数据在输出层上仍然保持相近的位置关系。 自组织性：网络能够自动地根据输入数据的特征进行学习和调整，而不需要外部的监督信息。 鲁棒性和泛化性能：网络对噪声和异常值具有较好的处理能力，能够提取输入数据的主要特征并进行有效的分类和映射。 四、应用领域 自组织映射网络已广泛应用于多个领域，包括但不限于：\n样本分类与排序：通过自组织映射网络可以对样本数据进行有效的分类和排序，提高数据处理的效率和准确性。 样本检测：在图像处理、信号处理等领域中，自组织映射网络可以用于检测异常样本或目标对象。 模式识别：在生物信息学、医学图像处理等领域中，自组织映射网络可以用于识别特定的模式或结构。 系统分析与优化：在工程、金融、军事等领域中，自组织映射网络可以用于系统分析和优化决策过程。 五、未来发展 随着人工智能技术的不断发展和进步，自组织映射网络将在更多领域发挥其独特的优势。未来的研究方向可能包括以下几个方面：\n算法优化：通过改进学习算法和竞争规则，提高自组织映射网络的训练效率和性能。 多模态学习：结合图像、文本、语音等多种模态的数据进行学习和映射，提高网络的综合处理能力。 跨领域应用：将自组织映射网络应用于更多的实际场景中，如自动驾驶、智能机器人等领域。 总之，自组织映射网络作为一种重要的神经网络模型，在多个领域都展现出了广泛的应用前景和巨大的发展潜力。\n深度神经网络 卷积神经网络(空间共享参数)与循环神经网络(时间共享参数) 基本思想 局部连接 参数共享 卷积操作 卷积操作是一种数学运算，广泛应用于信号处理、图像处理和深度学习等领域。以下是对卷积操作的详细解释：\n一、定义与原理 卷积操作是通过将一个函数（或称为卷积核、滤波器）在另一个函数（通常是输入信号或图像）上进行滑动，并在每个位置上计算两个函数的乘积之和，从而得到一个新的函数（或称为输出信号、特征图）。在图像处理中，卷积核通常是一个小的二维矩阵，用于提取图像中的局部特征，如边缘、纹理等。\n二、数学表达 卷积操作的数学表达可以表示为：\n[ (f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau) , d\\tau ]\n其中，$f(t)$ 是输入函数，$g(t)$ 是卷积核，$t$ 和 $\\tau$ 是变量，$*$ 表示卷积操作。在离散情况下（如图像处理），积分变为求和，公式变为：\n[ (I * K){i,j} = \\sum{m,n} I_{i-m,j-n} \\cdot K_{m,n} ]\n其中，$I$ 是输入图像（二维矩阵），$K$ 是卷积核，$(I * K){i,j}$ 表示输出特征图中第 $i, j$ 个元素的值，$I{i-m,j-n}$ 表示输入图像中第 $i-m, j-n$ 个元素的值，$K_{m,n}$ 表示卷积核中第 $m, n$ 个元素的值。\n三、操作过程 在图像处理中，卷积操作的具体过程如下：\n输入图像：将输入的图像表示为一个二维矩阵，矩阵的每个元素表示图像中的一个像素点。 定义卷积核：定义一个小的二维矩阵作为卷积核，卷积核的大小和其中的权重值根据需要提取的特征来确定。 滑动卷积核：将卷积核在输入图像上按照设定的步长进行滑动，每次滑动到一个新位置时，将卷积核与输入图像中对应位置的元素进行逐元素乘积运算。 求和：将乘积运算的结果进行求和，得到输出特征图中对应位置的元素值。 遍历图像：重复步骤3和4，直到卷积核遍历完整个输入图像，最终得到完整的输出特征图。 四、应用领域 卷积操作在多个领域都有广泛的应用，包括但不限于：\n图像处理：用于边缘检测、图像滤波、特征提取等任务。 音频处理：用于音频信号的滤波和降噪，如实现音频信号的均衡器、混响效果或噪声消除。 自然语言处理：在卷积神经网络（CNN）中，通过将文本转化为向量表示，并利用卷积核进行卷积操作，实现文本分类、情感分析和语义理解等任务。 深度学习：卷积层是卷积神经网络（CNN）的核心组件，通过应用多个卷积核对输入数据进行特征提取，实现对图像、音频或文本等复杂数据的高级特征学习和表示。 五、特点与优势 卷积操作具有以下几个特点和优势：\n局部连接：卷积核只与输入图像中的局部区域相连接，减少了网络的参数数量和计算量。 参数共享：同一个卷积核在输入图像的不同位置共享相同的参数，进一步减少了网络的参数量，提高了模型的泛化能力。 平移不变性：卷积操作对输入信号的平移变换具有不变性，即输入信号发生平移时，输出信号也相应平移，但内容保持不变。 综上所述，卷积操作是一种强大的数学工具，在多个领域都有广泛的应用和重要的作用。\n池化层 池化层（Pooling Layer）是深度学习神经网络中常用的一种层级结构，尤其在卷积神经网络（CNN）中扮演着重要角色。以下是关于池化层的详细解释：\n一、定义与功能 池化层主要用于减小输入数据的空间尺寸（即宽度和高度），从而降低模型的计算复杂度，减少过拟合，并在一定程度上提取输入数据的重要特征。通过池化操作，CNN能够在保持模型表达能力的同时，有效降低计算成本和过拟合风险。\n二、主要类型 池化层根据所采用的池化函数不同，主要分为以下几种类型：\n最大池化（Max Pooling）：\n定义：在每个池化窗口（通常是一个小的二维区域，如2x2或3x3）中选择最大值作为输出。 优点：保留了信号的最大强度信息，有助于增强模型对局部细节的敏感性。 局限：可能会丢失一些次要但仍然重要的信息。 平均池化（Average Pooling）：\n定义：在每个池化窗口中计算所有值的平均值作为输出。 优点：降低了输出的方差，增加了预测的稳定性。 局限：可能导致细节模糊。 其他池化方法：如随机池化（Stochastic Pooling）、中值池化（Median Pooling）等，这些方法提供了额外的信息层次，用于更复杂的特征提取，但计算相对复杂，可能增加训练时间。\n三、作用与优势 降维：通过减少特征图的空间尺寸，可以减少模型的参数数量和计算量，从而加速模型的训练和推理过程。 特征不变性：池化操作能够提取特征的局部不变性，即使输入数据发生轻微的平移或变形，池化层仍然能够识别出相同的特征。 提高泛化能力：通过减少模型的复杂度，池化层有助于防止过拟合，提高模型的泛化能力。 增强鲁棒性：池化层增强了模型对输入数据变化的鲁棒性，使其能够更好地应对实际应用中的噪声和干扰。 四、应用领域 池化层广泛应用于各种深度学习框架中，尤其在计算机视觉任务中发挥核心作用，如图像分类、目标检测、语义分割等领域。它们还被用于自然语言处理和音频分析等其他领域的相关任务。\n五、实现方式 在深度学习框架中，如PyTorch和TensorFlow，都提供了实现池化层的函数或层。例如，在PyTorch中，可以使用nn.MaxPool2d和nn.AvgPool2d等函数来实现最大池化和平均池化操作。\n综上所述，池化层是深度学习神经网络中不可或缺的一部分，它通过减小输入数据的空间尺寸、提取重要特征、降低计算复杂度和提高模型泛化能力等方式，为深度学习模型的性能提升做出了重要贡献。\n循环单元 循环单元（Recurrent Units）是循环神经网络（Recurrent Neural Network, RNN）中的核心组成部分，它们负责在序列数据中传递信息，并具有记忆功能，能够捕捉序列数据中的长期依赖关系。以下是对循环单元的详细解析：\n一、基本概念 循环单元是RNN中的基本构件，它通过不断更新隐藏状态来实现信息的传递和记忆。在每个时间步，循环单元接收当前的输入和前一时间步的隐藏状态作为输入，然后输出一个新的隐藏状态，该隐藏状态随后被传递给下一个时间步的循环单元，或者用于生成输出。\n二、设计与功能 循环单元的设计使其能够处理序列数据中的时间依赖性。它们通过以下方式实现其功能：\n状态更新：在每个时间步，循环单元根据当前输入和前一时间步的隐藏状态计算新的隐藏状态。这个计算过程通常包括线性变换（如矩阵乘法）和非线性激活（如tanh或sigmoid函数）。 记忆功能：通过保留前一时间步的隐藏状态，循环单元能够“记住”序列中的历史信息，并在处理当前输入时考虑这些信息。这种记忆功能使得RNN能够捕捉序列数据中的长期依赖关系。 信息传递：循环单元之间的连接形成了一个循环，使得信息可以在序列中沿时间轴传递。这种信息传递机制是RNN处理序列数据的关键所在。 三、常见类型 在实际应用中，循环单元有多种变体，其中最常见的是长短期记忆网络（Long Short-Term Memory, LSTM）和门控循环单元（Gated Recurrent Unit, GRU）。\nLSTM：LSTM通过引入三个“门”结构（遗忘门、输入门和输出门）来控制信息的传递和遗忘。这些门结构使得LSTM能够更好地处理长期依赖问题，并在许多任务上表现出色。 GRU：GRU是LSTM的一种简化版本，它只有两个门结构（更新门和重置门）。相比于LSTM，GRU的参数数量更少，计算速度更快，同时在许多任务上的表现也相当不错。 四、应用领域 循环单元及其变体广泛应用于自然语言处理（如文本生成、机器翻译、情感分析等）、语音识别、时间序列预测等领域。它们能够处理序列数据中的时间依赖性，并提取出有用的特征和信息，为这些任务提供有力的支持。\n五、总结 循环单元是RNN中的核心组成部分，它们通过不断更新隐藏状态来实现信息的传递和记忆。在实际应用中，循环单元有多种变体，其中LSTM和GRU是最常见的两种。这些变体通过引入不同的门结构来控制信息的传递和遗忘，从而更好地处理序列数据中的长期依赖问题。\n搭建训练神经网络的项目（待做） 数据预处理模块 数据准备模块 工具函数 神经网络中各组件 神经网络模型 参数优化模块 训练过程定义 启动脚本 聚类算法 K-means K-means（K-均值）聚类算法是一种经典的无监督学习算法，用于将数据集中的样本点划分为K个簇（Cluster），使得每个簇内的样本点尽可能相似，而不同簇之间的样本点尽可能不同。以下是关于K-means算法的详细介绍：\n一、算法原理 K-means算法的核心思想是通过迭代的方式，将数据集中的样本点分配到K个簇中，使得每个簇内的样本点到该簇的质心（即簇内所有样本点的均值）的距离之和最小。算法的具体步骤如下：\n初始化：从数据集中随机选择K个样本点作为初始的簇中心（质心）。 分配簇：对于数据集中的每一个样本点，计算其与各个簇中心的距离，并将其分配到距离最近的簇中心所在的簇中。 更新质心：对于每个簇，重新计算该簇内所有样本点的均值，作为新的簇中心（质心）。 迭代：重复步骤2和步骤3，直到簇中心不再发生变化或达到预设的迭代次数为止。 二、算法特点 无监督学习：K-means算法不需要事先知道数据集的标签信息，能够自动将数据划分为多个簇。 基于划分的聚类：K-means算法通过划分的方式将数据集划分为K个簇，每个簇内的样本点具有较高的相似度。 迭代优化：K-means算法采用迭代的方式不断优化簇的划分和簇中心的位置，以最小化簇内样本点到簇中心的距离之和。 需要预先指定K值：K-means算法需要事先指定要划分的簇的个数K，这个值的选择对聚类结果有很大影响。 三、算法应用 K-means算法在各个领域都有广泛的应用，包括但不限于：\n市场分析：通过聚类分析，企业可以将客户划分为不同的群体，以便制定更加精准的营销策略。 图像处理：在图像处理中，K-means算法可以用于图像分割、颜色量化等任务。 生物信息学：在生物信息学中，K-means算法可以用于基因表达数据的分析，揭示基因之间的相互作用关系。 文本挖掘：在文本挖掘中，K-means算法可以用于文档聚类，将相似的文档划分到同一个簇中。 四、算法优化 为了提高K-means算法的性能和聚类效果，可以采取以下优化措施：\n选择合适的K值：可以通过手肘法（Elbow Method）、轮廓系数法（Silhouette Coefficient）等方法来确定合适的K值。 选择初始质心：初始质心的选择对聚类结果有很大影响，可以采用K-means++等算法来优化初始质心的选择。 使用距离度量：在计算样本点到簇中心的距离时，可以选择不同的距离度量方式，如欧氏距离、曼哈顿距离等。 并行化计算：对于大规模数据集，可以采用并行化计算来提高K-means算法的执行效率。 五、注意事项 对异常值敏感：K-means算法对异常值（离群点）比较敏感，可能会导致聚类结果不准确。 可能陷入局部最优解：K-means算法采用贪心策略进行迭代优化，可能会陷入局部最优解而无法达到全局最优。 需要预先指定K值：K值的选择对聚类结果有很大影响，需要根据实际情况进行选择。 总之，K-means算法是一种简单而有效的聚类算法，在各个领域都有广泛的应用。然而，在使用时需要注意其特点和限制条件，并采取适当的优化措施来提高聚类效果。\n寻优算法之遗传算法 遗传算法（Genetic Algorithm, GA）是一种模拟自然界生物进化过程的优化搜索方法，它基于达尔文进化论中的自然选择和遗传学原理。遗传算法通过模拟生物进化过程中的选择、交叉（杂交）和变异等操作，对问题的解进行迭代更新，从而搜索最优解或近似最优解。以下是对遗传算法的详细介绍：\n一、起源与发展 起源：遗传算法最早由美国计算机科学家John H. Holland于20世纪60年代提出，并于1975年详细阐述了遗传算法的基本理论和方法。 发展：自20世纪80年代以来，遗传算法进入兴盛发展时期，被广泛应用于自动控制、生产计划、图像处理、机器人等研究领域。 二、基本概念 种群：遗传算法从一个代表问题可能潜在解集的种群开始，种群由多个个体组成，每个个体表示一个解。 染色体：在遗传算法中，每个个体由代表基因集合的染色体构成，染色体可以是二进制串、实数向量或其他形式的编码。 适应度函数：用于评估种群中个体的优劣程度，根据问题的目标函数来确定。适应度得分高的个体更有可能被选中进行繁殖。 三、基本操作 选择（Selection）：根据适应度函数，从当前种群中选择适应度高的个体作为父代，用于繁殖下一代。常用的选择策略有轮盘赌选择、锦标赛选择等。 交叉（Crossover）：随机选择两个父代个体，并交换它们染色体的一部分，以生成新的后代个体。交叉操作有助于保持种群的多样性。 变异（Mutation）：以一定的概率随机改变后代个体染色体中的某些基因，以引入新的遗传信息。变异操作有助于避免算法陷入局部最优解。 四、算法流程 初始化种群：随机生成一定数量的个体作为初始种群。 评估适应度：计算种群中每个个体的适应度值。 选择操作：根据适应度值选择父代个体。 交叉操作：对选中的父代个体进行交叉，生成新的后代个体。 变异操作：对后代个体的染色体进行变异。 更新种群：用新生成的后代个体替换种群中的部分个体，形成新的种群。 终止条件：重复执行步骤2至步骤6，直到满足预设的迭代次数或达到其他停止条件。 五、优缺点 优点：\n自适应性：能够处理复杂的非线性、非凸优化问题。 全局搜索能力：具有较好的全局搜索能力，可以避免陷入局部最优解。 鲁棒性：对问题的依赖性较小，易于实现和应用。 缺点：\n收敛速度可能较慢：需要较多的迭代次数才能找到最优解。 对参数敏感：如种群大小、交叉概率、变异概率等参数的选择对算法性能有较大影响。 早熟收敛：在某些情况下，算法可能过早地收敛到局部最优解。 六、应用领域 遗传算法已被广泛应用于多个领域，包括但不限于：\n函数优化：求解各种复杂形式的优化问题。 组合优化：解决背包问题、装载问题、选址问题等组合优化问题。 机器学习：优化机器学习模型的参数，提高模型的性能。 控制系统：优化控制系统的设计，如控制器的参数调节。 信号处理：优化信号处理问题，如图像压缩、音频处理等。 生物信息学：解决生物信息学中的问题，如基因编码、蛋白质结构预测等。 总之，遗传算法作为一种模拟自然进化过程的优化搜索方法，在多个领域都具有广泛的应用前景。\n算法拓展 精英主义思想 每次产生新种群时，把父代种群中的部分最优解直接复制到子代群体里或按概率选择方法保留一部分个体\n灾变 如果找不到最优解，或陷入局部最优，杀死一定比例的最优个体，给其他远离最优的个体一个机会\n","date":"2024-03-30T00:00:00Z","image":"https://a-b-ab.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/097ac305590341a9a724bf7be454397f9b1719b3_hu16356dd1fac7f7c338edea820ef7cfab_13104053_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","title":"机器学习基础笔记"},{"content":"Window与Linux双系统安装与卸载Linux 如果你事先在window上安装的ubuntu,但是因为种种原因需要重装，那么你需要先卸载装在你电脑上的ubuntu系统才能安装新的Ubuntu\n重装unbuntu记得先保存好ubuntu想保存的文件\nUbuntu卸载 删除ubuntu所在卷 进入windows系统，右键此电脑-管理-磁盘管理-删除ubuntu所在的卷，使这些区域成为“可用空间”\n删除unbuntu EFI分区 Win + R 输入cmd打开终端，输入 diskpart进入磁盘工具\n输入 list disk 查看磁盘，输入 select disk 1 （我的Ubuntu EFI分区在磁盘1，根据自己的情况选择）\n输入 list partition ，输入 select partition * （*为Ubuntu EFI分区号）\n删除Ubuntu系统启动项 Win + R 输入cmd打开终端，输入 diskpart 进入磁盘工具\n输入 list disk 查看磁盘，输入 select disk 1\n输入 list partition ，输入 select partition * （*为Windows EFI分区，一般为260M）\n输入 assign letter=J（分配盘符）\n管理员模式打开记事本，记事本选择文件-打开-选中磁盘J\n打开 EFI 文件夹，删除Ubuntu文件夹\n返回 Distpart 界面，输入 remove letter=J\n删除时一定要看仔细，不如会导致很严重的后果\n磁盘分区（如果你是第一次装Ubuntu） 打开计算机管理 找到磁盘管理 选择你要压缩的盘，你压缩掉的空间将作为新系统的空间 右键打开你要压缩的盘符，点击压缩卷 安装ubuntu系统 强烈建议安装最新lts版本，我写这篇博客时是22.04版 我就是因为第一次装的是18.04版所以才重装的，QAQ\n下载Linux镜像 1.下载 Ubuntu 镜像，这里可以去官方下载，但是官方在国外，默认外网链接可能网速有点小慢。（科技玩家例外） 更好的选择是国内的资源镜像网站，比如说清华大学开源软件镜像站\n清华大学开源软件镜像站\n准备u盘刻盘工具 我用的是UltralSO\n下载后打开，找到你下载的镜像文件，找到后，点击右上角的启动-\u0026gt;写入硬盘映像\n写入时记得插入u盘，这个u盘将作为启动盘 写入时，这个u盘的数据全会被格式化，谨慎操作\n然后点击写入就可以了\n开机引导界面 将写好的映像文件的U盘插入你需要的电脑，在开始时按f12，本人用的是联想，不同电脑可能不同，正常都是f12\n在引导界面可以看到几个选项，选择你的u盘作为启动项，反正不是window和network就是了\n启动后你会进入系统选择界面，使用方向键选择 ubuntu 后回车就进入了 Ubuntu 的安装引导界面。在侧边栏中选择系统语言，English、Chinese都可，看自己喜好，然后点击 Install Ubuntu 进入安装，选择安装方式，选择正常安装就行，会默认安装火狐浏览器等软件。或者选择最小安装的话可以在安装完成后自行安装需要的软件，两种方式影响不大。下面的安装第三方软件选项也可以选上，也可以不选，后面再根据需要手动安装。我这里就只选择了正常安装，然后点击继续，在安装类型选择时，建议自己手动分区，说一下分区情况吧，我找了大部分教程都是分为四个区：\n分区方案 /boot : 1G（最好） 主分区。系统的boot启动引导项安装位置\n/ : 随意（尽量大） 主分区。根目录，所有目录的根节点，其下包含很多子目录，如/usr /tmp等\n/home : 自定义（尽量大，一般最后分） 逻辑分区。一般放置自己的数据\nswap : 16G 逻辑分区。交换空间，一般是物理内存的1~2倍就行了\n具体操作，首先找到 free space 空间，如下，选中该空间，点击左下角的加号+，进行内存分配\n安装后-\u0026gt;选择地区（上海或香港都可以，我选的是上海）-\u0026gt;设置账户密码-\u0026gt;重启-\u0026gt;输入密码，ok\n我和同学在重启时都出现，黑屏跳一堆数据的情况，我们当时都很慌，但是我们最后强制关机重启后，发现没有大问题，起码我们没发现问题\n以后你每次开机时都可以选择windo和Linux之一的系统进行启动\nTip 安装后可能会出现很多问题，比如，Linux熄屏黑屏启动不了，这个是好像是显卡冲突问题，内置键盘输入不了等等\u0026hellip;\n可以去网上找对应解决方法\n","date":"2024-03-25T00:00:00Z","image":"https://a-b-ab.github.io/p/linux-win%E5%8F%8C%E7%B3%BB%E7%BB%9F/f5fe3c72432cf9bace81d3d424f1a6a5_hu787b0e61590c825174742ce250fd0c32_3527535_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/linux-win%E5%8F%8C%E7%B3%BB%E7%BB%9F/","title":"Linux+Win双系统"},{"content":"MySQL基本操作命令汇总 基本操作 对数据库以及表的一些基本操作\n关于数据库 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 创建数据库 create database h_test; // 查看数据库 show databases; //查看数据库信息 show create database h_test; //修改数据库的编码，可使用上一条语句查看是否修改成功 alter database h_test default character set gbk collate gbk_bin; //删除数据库 drop database h_test; //综上，可以直接创建数据库且设置编码方式 create database h_test default character set utf8 collate utf_general_ci; 关于数据表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 //首先选定操作的数据库 use h_test; //create table student( id int(11), name varchar(20), age int(11) ); //查看数据表 show tables; //查看数据表信息，后面加上参数/G可使结果更加美观 show create table student; //查看表的字段信息 desc student; //修改表名 alter table student rename [to] h_student; //修改字段名 alter table h_student change name stu_name varchar(20); //修改字段的数据类型 alter table h_sutdent modify id int(20); //添加字段 alter table h_student add grade float; //删除字段 alter table h_student drop grade; //修改字段的位置 alter table h_student modify stu_name varchar(20) first; alter table h_student modify id int(11) after age; //删除数据表 drop table h_student;S 表的约束 PRIMARY KEY 主键约束，用于唯一标识对应的记录 FOREIGN KEY 外键约束 NOT NULL 非空约束 UNIQUE 唯一性约束 DEFAULT 默认值约束，用于设置字段的默认值\n索引 作用：提高表中数据的查询速度\n普通索引 唯一性索引 全文索引 单列索引 多列索引 空间索引 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 //创建索引 //一.创建表的时候创建索引 create table 表名( 字段名 数据类型[完整性约束条件], ... 字段名 数据类型, [UNIQUE|FULLTEXT|SPATIAL] INDEX|KEY ); //1-1.创建普通索引 create table test1( id INT, name VARCHAR(20), age INT, INDEX (id) ); //可以插入一条数据,查看索引是否被使用 explain select * from test1 where id=1 \\G; //1-2.创建唯一性索引 create table test2( id INT, name VARCHAR(20), age INT, UNIQUE INDEX unique_id(id asc) ); //1-3.创建全文索引 create table test3( id INT, name VARCHAR(20), age INT, FULLTEXT INDEX fulltext_name(name) )ENGINE=MyISAM; //1-4.创建单列索引 create table test4( id INT, name VARCHAR(20), age INT, INDEX single_name(name(20)) ); //1-5.创建多列索引 create table test5( id INT, name VARCHAR(20), age INT, INDEX multi(id,name(20)) ); //1-6.创建空间索引 create table test6( id INT, space GEOMETRY NOT NULL, SPATIAL INDEX sp(space) )ENGINE=MyISAM; --------------------------------------------------- //二.使用create index语句在已经存在的表上创建索引 //首先新建一个表,这个表没有索引 create table student( id int, age int, name varchar(20), intro varchar(40), g GEOMETRY NOT NULL )ENGINE=MyISAM; //2-1.创建普通索引 create index index_id on student(id); //2-2.创建唯一性索引 create unique index uniqueidx on student(id); //2-3.创建单列索引 create index singleidx on student(age); //2-4.创建多列索引 create index mulitidx on student(name(20),intro(40)); //2-5.创建全文索引 create fulltext index fulltextidx on student(name); //2-6.创建空间索引 create spatial index spatidx on student(g); //下图是第二种方法创建索引演示后的所有索引 //三.使用alter table语句在已经存在的表上创建索引 //删除student表，重新创建 drop table student; create table student( id int, age int, name varchar(20), intro varchar(40), space GEOMETRY NOT NULL )ENGINE=MyISAM; //3-1.创建普通索引 alter table student add index index_id(id); //3-2.创建唯一性索引 alter table student add unique uniqueidx(id); //3-3.创建单列索引 alter table student add index singleidx (age); //3-4.创建多列索引 alter table student add index multidx(name(20),intro(40)); //3-5.创建全文索引 alter table student add fulltext index fulltextidx(name); //3-6.创建空间索引 alter table student add spatial index spatidx(space); //删除索引，有下面两种方式 //1.使用alter table删除索引fulltextidx alter table student drop index fulltextidx; //2.使用drop index删除索引spatidx drop index spatidx on student; //下图可看到删除成功 添加数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 //重新建立表student drop table student; create table student( id int, name varchar(20) not null, grade float ); //插入一条数据，也可以少某个字段的同时也少对应的数据 insert into student(id,name,grade) values(1,\u0026#39;CROW\u0026#39;,70); //也可以不指定字段名，但要注意顺序 insert into student values(2,\u0026#39;CROW\u0026#39;,80); //也可以这样添加数据 insert into student set id=3,name=\u0026#34;CROW\u0026#34;,grade=90; //同时添加多条数据 insert into student values (4,\u0026#39;CROW\u0026#39;,80), (5,\u0026#39;CROW\u0026#39;,80), (6,\u0026#39;CROW\u0026#39;,80); 更新数据 1 2 3 4 //更新id=1的数据 update student set name=\u0026#34;abab\u0026#34;,grade=60 where id=1; //批量更新，如果没有where子句，会更新表中所有对应数据 update student set grade=100 where id\u0026lt;4; 删除数据 1 2 3 4 5 6 //删除id=6的数据 delete from student where id=6; //批量删除数据 delete from student where id\u0026gt;3; //删除所有数据,DDL(数据定义语言)语句 truncate table student也可以删除表内所有数据 delete from student; 单表查询和多表操作 单表查询：如何从数据库中获取你需要的数据 多表查询：实际开发中，需要进行2张表以上进行操作\n单表查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 //建立表student create table student( id int not null auto_increment, name varchar(20) not null, grade float, primary key(id) ); //插入数据 insert into student (name,grade) values (\u0026#34;Crow1\u0026#34;,40), (\u0026#34;Crow1\u0026#34;,50), (\u0026#34;Crow2\u0026#34;,50), (\u0026#34;Crow3\u0026#34;,60), (\u0026#34;Crow4\u0026#34;,70), (\u0026#34;Crow5\u0026#34;,80), (\u0026#34;Crow6\u0026#34;,null); //查询全部 select * from student; //查询某个字段 select name from student; //条件查询,查询id=2学生的信息 select * from student where id=2; //in关键字查询,也可以使用not in select * from student where id IN(1,2,3); //between and关键字查询 select * from student where id between 2 and 5; //空值(NULL)查询，使用IS NULL来判断 select * from student where grade is null; //distinct关键字查询 select distinct name from student; //like关键字查询,查询以h开头，e结尾的数据 select * from student where name like \u0026#34;h%e\u0026#34;; //and关键字多条件查询,or关键字的使用也是类似 select * from student where id\u0026gt;5 and grade\u0026gt;60; 高级查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 //聚合函数 //count()函数,sum()函数,avg()函数,max()函数,min()函数 select count(*) from student; select sum(grade) from student; select avg(grade) from student; select max(grade) from student; select min(grade) from student; //对查询结果进行排序 select * from student order by grade; //分组查询 //1.单独使用group by分组 select * from student group by grade; //2.和聚合函数一起使用 select count(*),grade from student group by grade; //3.和having关键字一起使用 select sum(grade),name from student group by grade having sum(grade) \u0026gt;100; //使用limit限制查询结果的数量 select * from student limit 5; select * from student limit 2,2; select * from student order by grade desc limit 2,2; //函数,mysql提供了许多函数 select concat(id,\u0026#39;:\u0026#39;,name,\u0026#39;:\u0026#39;,grade) from student; //为表取别名 select * from student as stu where stu.name=\u0026#34;CROW\u0026#34;; //为字段取别名,as关键字也可以不写 select name as stu_name,grade stu_grade from student; 多表操作 1.了解外键 2.了解关联关系 3.了解各种连接查询多表的数据 4.了解子查询，会使用各种关键字以及比较运算符查询多表中的数据\n外键 外键是指引用另一个表中的一列或者多列，被引用的列应该具有主键约束或者唯一性约束，用于建立和加强两个数据表之间的连接。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //创建表class,student create table class( id int not null primary key, classname varchar(20) not null )ENGINE=InnoDB; create table student( stu_id int not null primary key, stu_name varchar(20) not null, cid int not null -- 表示班级id，它就是class表的外键 )ENGINE=InnoDB; //添加外键约束 alter table student add constraint FK_ID foreign key(cid) references class(id); //删除外键约束 alter table student drop foreign key FK_ID; 操作关联表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 //数据表有三种关联关系，多对一、多对多、一对一 //学生(student)和班级(class)是多对一关系，添加数据 //首选添加外键约束 alter table student add constraint FK_ID foreign key(cid) references class(id); //添加数据,这两个表便有了关联若插入中文在终端显示空白，可设置set names \u0026#39;gbk\u0026#39;; insert into class values(1,\u0026#34;软件一班\u0026#34;),(2,\u0026#34;软件二班\u0026#34;); insert into student values(1,\u0026#34;CROW\u0026#34;,1),(2,\u0026#34;Crow1\u0026#34;,2),(3,\u0026#34;Crow2\u0026#34;,1),(4,\u0026#34;Crow3\u0026#34;,2); //交叉连接 select * from student cross join class; //内连接，该功能也可以使用where语句实现 select student.stu_name,class.classname from student join class on class.id=student.cid; //外连接 //首先在student,class表中插入数据 insert into class values(3,\u0026#34;软件三班\u0026#34;); //左连接，右连接 select s.stu_id,s.stu_name,c.classname from student s left join class c on c.id=s.cid; select s.stu_id,s.stu_name,c.classname from student s right join class c on c.id=s.cid; //复合条件连接查询就是添加过滤条件 //子查询 //in关键字子查询跟上面的in关键字查询类似 select * from student where cid in(select id from class where id=2); //exists关键字查询,相当于测试，不产生数据，只返回true或者false，只有返回true，外层才会执行，具体看下图 select * from student where exists(select id from class where id=12); -- 外层不会执行 select * from student where exists(select id from class where id=1); -- 外层会执行 //any关键字查询 select * from student where cid\u0026gt;any(select id from class); //all关键字查询 select * from student where cid=all(select id from class); 事务于存储过程 事务的概念，会开启、提交和回滚事务 事务的四种隔离级别 创建存储过程 调用、查看、修改和删除存储过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 start transaction; -- 开启事务 commit; -- 提交事务 rollback; -- 取消事务(回滚) //创建表account，插入数据 create table account( id int primary key auto_increment, name varchar(40), money float ); insert into account(name,money) values(\u0026#39;a\u0026#39;,1000),(\u0026#39;b\u0026#39;,2000),(\u0026#39;c\u0026#39;,3000); //利用事务实现转账功能，首先开启事务，然后执行语句，提交事务 start transaction; update account set money=money-100 where name=\u0026#39;a\u0026#39;; update account set money=money+100 where name=\u0026#39;b\u0026#39;; commit; //事务的提交，通过这个命令查看mysql提交方式 select @@autocommit; -- 若为1，表示自动提交，为0，就要手动提交 //若事务的提交方式为手动提交 set @@autocommit = 0; -- 设置为手动提交 start transaction; update account set money=money+100 where name=\u0026#39;a\u0026#39;; update account set money=money-100 where name=\u0026#39;b\u0026#39;; //现在执行select * from account 可以看到转账成功，若此时退出数据库重新登录，会看到各账户余额没有改变，所以一定要用commit语句提交事务，否则会失败 //事务的回滚，别忘记设置为手动提交的模式 start transaction; update account set money=money-100 where name=\u0026#39;a\u0026#39;; update account set money=money+100 where name=\u0026#39;b\u0026#39;; //若此时a不想转账给b，可以使用事务的回滚 rollback; //事务的隔离级别 read uncommitted; read committed; repeatable read; serializable; 存储过程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //创建查看student表的存储过程 //创建student表 create table student( id int not null primary key auto_increment, name varchar(4), grade float )ENGINE=InnoDB default character set utf8; delimiter // -- 将mysql的结束符设置为// create procedure Proc() begin select * from student; end // delimiter ; -- 将mysql的结束符设置为; call Proc(); -- 这样就可以调用该存储过程 //变量的使用,mysql中变量不用事前申明，在用的时候直接用“@变量名”使用就可以 set @number=100; -- 或set @num:=1; //定义条件和处理程序 //光标的使用 //1.声明光标 DECLARE * cursor_name* CURSOR FOR select_statement 2. 光标OPEN语句 OPEN cursor_name 3. 光标FETCH语句 FETCH cursor_name INTO var_name [, var_name] ... 4. 光标CLOSE语句 CLOSE cursor_name //流程控制的使用 不做介绍 调用存储过程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //定义存储过程 delimiter // create procedure proc1(in name varchar(4),out num int) begin select count(*) into num from student where name=name; end// delimiter ; //调用存储过程 call proc1(\u0026#34;tom\u0026#34;,@num) -- 查找名为tom学生人数 //查看结果 select @num; //查看存储过程 show procedure status like \u0026#39;p%\u0026#39; \\G -- 获得以p开头的存储过程信息 //修改存储过程 alter {procedure|function} sp_name[characteristic...] //删除存储过程 drop procedure proc1; 视图 如何创建视图 查看、修改、更新、删除视图\n视图的基本操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 //在单表上创建视图,重新创建student表，插入数据 create table student( id int not null primary key auto_increment, name varchar(10) not null, math float, chinese float ); insert into student(name,math,chinese) values (\u0026#39;Crow1\u0026#39;,66,77), (\u0026#39;Crow2\u0026#39;,66,77), (\u0026#39;Crow3\u0026#39;,66,77); //开始创建视图 create view stu_view as select math,chinese,math+chinese from student; -- 下图可看出创建成功 //也可以创建自定义字段名称的视图 create view stu_view2(math,chin,sum) as select math,chinese,math+chinese from student; //在多表上创建视图，创建表stu_info，插入数据 create table stu_info( id int not null primary key auto_increment, class varchar(10) not null, addr varchar(100) ); insert into stu_info(class,addr) values (\u0026#39;1\u0026#39;,\u0026#39;anhui\u0026#39;), (\u0026#39;2\u0026#39;,\u0026#39;fujian\u0026#39;), (\u0026#39;3\u0026#39;,\u0026#39;guangdong\u0026#39;); //创建视图stu_class create view stu_class(id,name,class) as select student.id,student.name,stu_info.class from student,stu_info where student.id=stu_info.id; //查看视图 desc stu_class; show table status like \u0026#39;stu_class\u0026#39;\\G show create view stu_class\\G //修改视图 create or replace view stu_view as select * from student; alter view stu_view as select chinese from student; //更新视图 update stu_view set chinese=100; insert into student values(null,\u0026#39;haha\u0026#39;,100,100); delete from stu_view2 where math=100; //删除视图 drop view if exists stu_view2; ","date":"2024-03-22T00:00:00Z","image":"https://a-b-ab.github.io/p/mysql%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4/8c8b581ab1cf8ea5e3aa00cfb897ea4e_hu3179d4694729e87dcb13406dae7e5a71_5532016_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/mysql%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4/","title":"MySQL简单命令"},{"content":"前言 在搭建个人博客时，遇到了各种阴间问题，为此我删了起码七个库，都是泪，归根到底就是两个问题，一是主题配置，二是远程连接\nhugo（静态网站生成器） Hugo是一个快速、灵活且功能强大的静态网站生成器。它使用Go语言开发，旨在帮助用户轻松创建和管理网站\nhugo中文文档\nhugo下载地址\n生成站点 使用Hugo快速生成站点，比如希望生成到 /path/to/site 路径：\nwin+r输入cmd调用命令行\nhugo new site /path/to/site\n这样就在 /path/to/site 目录里生成了初始站点，进去目录：\ncd /path/to/site\n站点目录结构：\narchetypes/:存放内容模板的目录 content/:：存放所有内容文件（如Markdown文件）的目录。 layouts/:存放布局文件的目录。 static/:存放静态资源（如图片、CSS、JavaScript等）的目录 config.toml(yaml):站点的配置文件。 在这个目录下，最主要的是对hugo.toml，themes/的配置。创建的所有.md文件都放置在content/文件下，所有文章的图片都放在static/下面\n创建文章 创建一个 about 页面\nhugo new about.md\nabout.md 自动生成到了 content/about.md ，打开 about.md 看下 \u0026mdash; title: Chinese Test description: 这是一个副标题 date: 2020-09-09 slug: test-chinese image: 文章封面图片 categories: - Test - 测试 \u0026mdash;\ntitle: 页面或文章的标题，这里是 \u0026ldquo;Chinese Test\u0026rdquo;。 description: 页面或文章的描述，这里是 \u0026ldquo;这是一个副标题\u0026rdquo;。 date: 文章的发布日期，这里是 \u0026ldquo;2020-09-09\u0026rdquo;。 slug: 文章的URL别名，这里是 \u0026ldquo;test-chinese\u0026rdquo;。 image: 文章的封面图片，这里是 \u0026ldquo;helena-hertz-wWZzXlDpMog-unsplash.jpg\u0026rdquo;。 categories: 文章的分类，这里有两个分类：\u0026ldquo;Test\u0026rdquo; 和 \u0026ldquo;测试\u0026rdquo;。 draft:草稿，建议直接设为false 正文内容\n创建第一篇文章，放到 post 目录，方便之后生成聚合页面\nhugo new post/first.md\n安装皮肤 一个好看的主题是必要的，但是完全自己去写就太麻烦了，hugo中有很多精美的主题，比较方便的操作是在Hugo项目目录里面使用Git命令来克隆themes：你需要提供主题的 Git 仓库 URL。通常，主题的仓库 URL 可以在 Hugo Themes 官方网站或主题的文档中找到。\ngit clone https://github.com/author/theme-name.git themes/theme-name\n或者直接下载主题的压缩包，将其解压到themes/文件夹下，这样的话，就需要你对hugo.toml文件进行一点过的修改。\n每个主题文件里面都有对应的配置教程和演示网站，比如stack在exampleSite文件夹里面\n配置hugo.toml 好多教程里面直接就说会生成config.yaml文件，但事实上新版的都是生成的hugo.toml，这不免让第一次配置的人感到迷惑，这两种的使用都是可以的，只是在语法结构上会有区别(类似于c++和python)，如果你想要完全自己手搓，按照你喜欢的语言就好，如果像我一样，只想点点鼠标，那就主要观察你下载的themes里面它使用的是什么，跟着用就行。\n配置文件可以理解为对这个项目的总的配置，比如修改网站的标题等\n使用主题前最好看一下主题相应的配置教程，不同的主题是不一样，主题就是别人写好的网站的模板，而你就是去套用别人的模板。\ntheme = \u0026ldquo;my-theme\u0026rdquo; my-theme是你下载的theme的主题名，要和你解压的文件名一致\n配置还有很多，可以结合网上资源或自己专业知识定制自己的网站，类似：\n1 2 3 4 5 6 7 sidebar: emoji: 🍔 subtitle: 所行皆坦途,所愿皆如期 avatar: enabled: true local: true src: img/avatar.jpg 生成网站 需要注意的是，虽然我们做了这么多，但是在这个结构下(在 my-site这个结构下)，我们网站的页面实际上是还没有生成的，要想生成静态网站页面，必须运行如下命令\nhugo\n命令运行后，在上文提到my-site这个结构下会产生一个public/文件夹，里面保存生成的静态页面，后面将其在GitHub上面布置，实际上就是将public/中的内容远程推送到Github仓库中后进行展示。\nhugo server\n可以在本地预览你生成的网站，点击链接就可以在你本地的电脑看到自己搭建的网页的，但是这只是完成了一半，你需要将其托管到github page上，别人才能通过网址找到你的网站，github page是最简单且免费的方法，当然便宜的东西是有缺陷的，如果以后自己想深入，就需要买个域名和服务器了\n推送到GitHub 首先在GitHub上创建一个仓库，名字最好和你自己的名字一样，其实也无所谓，但是好多人都这么做，但我不是\n在pubilc文件夹中创建仓库 记得cd 进入对应的文件夹\n其实远程连接在我的git博客上有详细的教程，这里我大概说一下\ngit init 初始化仓库 git add. 增加所有修改的意思 git commit -m \u0026ldquo;备注\u0026rdquo; 提交到远程仓库“备注随便写，你写我是傻逼都行”\n将两个仓库链接起来 这一步是最恶心我的，不知道为什么，不能用http链接，只能用ssh链接 大概意思就是你需要在你的电脑生成一个ssh链接，然后将其放到GitHub的配置上\n这步是参考别人的\n打开git bash命令窗口 生成ssh key ssh-keygen -t rsa -b 4096 -C \u0026ldquo;your_email@example.com\u0026rdquo; your_email@example.com为github上你注册的email地址。\n然后直接三个enter不管他\n上面默认生成在用户主目录的.ssh目录下，可以自己输入自定义位置\n把ssh key添加到github 复制文件c/Users/Administrator/.ssh/id_rsa.pub内容，把key添加到：github \u0026gt; settings \u0026gt; SSH and GPG keys \u0026gt; New SSH key \u0026gt; 粘贴保存。\n测试SSH连接 ssh -T git@github.com\n如果成功的话你就可以通过SSH方式来clone及提交代码了\ngit branch -M main // 创建一个分支\ngit remote add origin https://github.com/your-username/your-repo.git //使用 git remote add 命令将远程仓库添加到仓库配置中\ngit remote set-url origin git@github.com:your-username/your-repo.git //使用 SSH 连接后，可以使用 SSH URL 推送到 GitHub 仓库\ngit push -u origin main //推送文件\n后续提交 hugo //在站点中运行\ncd public/\ngit add . //提交文件\ngit commit -m \u0026ldquo;备注\u0026rdquo; //推送到远程：在ssh已连接的情况下\ngit push -u origin main\n","date":"2024-03-20T00:00:00Z","image":"https://a-b-ab.github.io/p/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/1636c128aa9d846a9aa395599c438a041d4f56b41dff52-PmgBaF_hu1c7f7f6d3e56642cafe1f4e912a06462_1965906_120x120_fill_q75_box_smart1.jpg","permalink":"https://a-b-ab.github.io/p/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","title":"个人博客搭建"}]